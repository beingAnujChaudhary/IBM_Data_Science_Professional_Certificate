{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Okay, I will break down the \"Python Cheat Sheet for Data Analysis\" PDF, explain each section with Python code examples runnable in Google Colab, and then provide 50 practice questions.\n",
        "\n",
        "---\n",
        "### Python Cheat Sheet for Data Analysis: Explained\n",
        "\n",
        "This cheat sheet covers the essential steps and Python commands for a typical data analysis workflow, from loading data to model development and evaluation.\n",
        "\n",
        "---\n",
        "### 1. Data Loading\n",
        "This section focuses on importing data into a Pandas DataFrame, which is the primary data structure used for data analysis in Python.\n",
        "\n",
        "\n",
        "```markdown\n",
        "#### Reading a CSV dataset\n",
        "Pandas provides the `read_csv()` function to load data from a Comma-Separated Values (CSV) file into a DataFrame.\n",
        "- You can specify if the file has a header or assign column names.\n",
        "```\n",
        "\n",
        "**Code Cell 1: Reading CSV and Basic Inspection**"
      ],
      "metadata": {
        "id": "Wuyg3Zn4FESk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np # numpy is often needed for numerical operations and handling NaN\n",
        "\n",
        "# Create a dummy CSV file for demonstration\n",
        "csv_data_with_header = \"\"\"col1,col2,col3\n",
        "1,a,?\n",
        "2,b,5\n",
        "3,?,7\n",
        "4,d,9\"\"\"\n",
        "with open('sample_with_header.csv', 'w') as f:\n",
        "    f.write(csv_data_with_header)\n",
        "\n",
        "csv_data_no_header = \"\"\"10,x,11\n",
        "20,y,15\n",
        "30,z,17\n",
        "40,w,19\"\"\"\n",
        "with open('sample_no_header.csv', 'w') as f:\n",
        "    f.write(csv_data_no_header)\n",
        "\n",
        "# Load CSV using the first row as header (default behavior if header=0 or not specified)\n",
        "df_with_header = pd.read_csv('sample_with_header.csv')\n",
        "print(\"DataFrame with header:\")\n",
        "print(df_with_header)\n",
        "\n",
        "# Load CSV without a header, pandas will assign default integer headers (0, 1, 2...)\n",
        "df_no_header_auto = pd.read_csv('sample_no_header.csv', header=None)\n",
        "print(\"\\nDataFrame without header (auto-assigned column names):\")\n",
        "print(df_no_header_auto)\n",
        "\n",
        "# Assign custom header names if the CSV doesn't have them\n",
        "headers = [\"ID\", \"Category\", \"Value\"]\n",
        "df_custom_headers = pd.read_csv('sample_no_header.csv', names=headers) # 'names' is preferred over 'header=None' then df.columns\n",
        "print(\"\\nDataFrame with custom assigned headers:\")\n",
        "print(df_custom_headers)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "XxA8Aj4dFESq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```markdown\n",
        "#### Inspecting the DataFrame\n",
        "Once loaded, you can inspect the DataFrame:\n",
        "- `df.head(n)`: Shows the first `n` rows (default is 5).\n",
        "- `df.tail(n)`: Shows the last `n` rows (default is 5).\n",
        "- `df.columns`: Shows the column names. Can also be used to assign new column names.\n",
        "- `df.dtypes`: Shows the data type of each column.\n",
        "- `df.describe()`: Provides descriptive statistics (count, mean, std, min, max, quartiles) for numerical columns. `include=\"all\"` shows stats for object/categorical columns too.\n",
        "- `df.info()`: Gives a concise summary of the DataFrame, including data types, non-null values, and memory usage.\n",
        "```\n",
        "\n",
        "**Code Cell 2: Inspecting DataFrame**"
      ],
      "metadata": {
        "id": "qz1WzKNkFESs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using df_with_header from the previous cell for inspection\n",
        "print(\"\\nFirst 3 rows of df_with_header:\")\n",
        "print(df_with_header.head(3))\n",
        "\n",
        "print(\"\\nLast 2 rows of df_with_header:\")\n",
        "print(df_with_header.tail(2))\n",
        "\n",
        "print(\"\\nColumn names of df_with_header:\")\n",
        "print(df_with_header.columns)\n",
        "\n",
        "# Example of assigning new header names (though df_with_header already has them)\n",
        "# df_with_header.columns = ['NewUrl1', 'NewUrl2', 'NewUrl3']\n",
        "# print(\"\\nAfter renaming columns:\")\n",
        "# print(df_with_header.columns)\n",
        "\n",
        "print(\"\\nData types of df_with_header:\")\n",
        "print(df_with_header.dtypes)\n",
        "\n",
        "print(\"\\nStatistical description of df_with_header (numerical columns by default):\")\n",
        "# Convert 'col3' to numeric, as '?' might make it object type.\n",
        "# First, replace '?' with NaN\n",
        "df_with_header_cleaned = df_with_header.replace(\"?\", np.nan)\n",
        "df_with_header_cleaned['col3'] = pd.to_numeric(df_with_header_cleaned['col3'])\n",
        "print(df_with_header_cleaned.describe())\n",
        "\n",
        "print(\"\\nStatistical description including all attributes (object types too):\")\n",
        "print(df_with_header_cleaned.describe(include=\"all\"))\n",
        "\n",
        "print(\"\\nSummary info of df_with_header_cleaned:\")\n",
        "df_with_header_cleaned.info()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "voeEv858FESs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```markdown\n",
        "#### Handling Specific Values and Saving\n",
        "- `df.replace(\"?\", np.nan)`: Replaces specific placeholders (like \"?\") with NumPy's `NaN` (Not a Number), which is Pandas' standard way of representing missing values.\n",
        "- `df.to_csv(<output_path>)`: Saves the DataFrame to a CSV file. `index=False` is often used to avoid writing the DataFrame index as a column in the CSV.\n",
        "```\n",
        "\n",
        "**Code Cell 3: Replacing Values and Saving**"
      ],
      "metadata": {
        "id": "9-abMweAFESt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_with_header_cleaned already has '?' replaced with NaN from the previous cell.\n",
        "print(\"\\nDataFrame after replacing '?' with NaN (df_with_header_cleaned):\")\n",
        "print(df_with_header_cleaned)\n",
        "\n",
        "# Save the cleaned DataFrame to a new CSV file\n",
        "output_csv_path = 'cleaned_data.csv'\n",
        "df_with_header_cleaned.to_csv(output_csv_path, index=False) # index=False avoids writing df index as a column\n",
        "print(f\"\\nCleaned DataFrame saved to {output_csv_path}\")\n",
        "\n",
        "# You can verify by reading it back\n",
        "df_reloaded = pd.read_csv(output_csv_path)\n",
        "print(\"\\nReloaded DataFrame:\")\n",
        "print(df_reloaded)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "vE-qZHsOFESt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 2. Data Wrangling\n",
        "This involves preprocessing the data to handle issues like missing values, incorrect data types, and transforming data into a more suitable format for analysis or modeling.\n",
        "\n",
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Handling Missing Data\n",
        "Missing data can be handled in several ways:\n",
        "- **Replace with most frequent value:** Useful for categorical columns.\n",
        "- **Replace with mean/median:** Useful for numerical columns. Median is often preferred if the data has outliers.\n",
        "`inplace=True` modifies the DataFrame directly.\n",
        "```\n",
        "\n",
        "**Code Cell 4: Handling Missing Data**"
      ],
      "metadata": {
        "id": "TvSH5N_ZFESt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample DataFrame with missing values\n",
        "data_missing = {'colA': ['X', 'Y', 'X', 'Z', np.nan, 'Y', 'X'],\n",
        "                'colB': [10, 20, np.nan, 30, 40, 20, np.nan],\n",
        "                'colC': [100, 110, 120, 100, np.nan, 110, 100]}\n",
        "df_missing = pd.DataFrame(data_missing)\n",
        "print(\"Original DataFrame with missing values:\")\n",
        "print(df_missing)\n",
        "\n",
        "# Replace missing data in 'colA' (categorical) with the most frequent entry\n",
        "most_frequent_colA = df_missing['colA'].value_counts().idxmax()\n",
        "df_missing['colA'].replace(np.nan, most_frequent_colA, inplace=True) # Modifies df_missing directly\n",
        "print(\"\\nDataFrame after replacing NaN in 'colA' with most frequent:\")\n",
        "print(df_missing)\n",
        "\n",
        "# Replace missing data in 'colB' (numerical) with the mean\n",
        "# Ensure colB is numeric before calculating mean (it should be if NaNs are np.nan)\n",
        "average_value_colB = df_missing['colB'].astype(float).mean(axis=0) # axis=0 for column mean\n",
        "df_missing['colB'].replace(np.nan, average_value_colB, inplace=True)\n",
        "print(\"\\nDataFrame after replacing NaN in 'colB' with mean:\")\n",
        "print(df_missing)\n",
        "\n",
        "# Alternative using fillna() which is often preferred\n",
        "# df_missing['colC'].fillna(df_missing['colC'].median(), inplace=True) # Example with median for colC\n",
        "# print(\"\\nDataFrame after replacing NaN in 'colC' with median using fillna():\")\n",
        "# print(df_missing)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "guvacfpUFESu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Fixing Data Types\n",
        "Ensure columns have the correct data types for analysis (e.g., numbers stored as strings should be converted to numeric types).\n",
        "```\n",
        "\n",
        "**Code Cell 5: Fixing Data Types**"
      ],
      "metadata": {
        "id": "9kAxPt9jFESu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: df_with_header_cleaned['col3'] was already converted in a previous cell.\n",
        "# Let's assume we have another column that should be an integer but is an object.\n",
        "df_types = pd.DataFrame({'ID': [1, 2, 3], 'ValueStr': ['100', '250', '80']})\n",
        "print(\"\\nOriginal DataFrame with 'ValueStr' as object:\")\n",
        "print(df_types)\n",
        "print(df_types.dtypes)\n",
        "\n",
        "df_types['ValueStr'] = df_types['ValueStr'].astype(int)\n",
        "print(\"\\nDataFrame after converting 'ValueStr' to int:\")\n",
        "print(df_types)\n",
        "print(df_types.dtypes)\n",
        "\n",
        "# Multiple columns at once\n",
        "# df_types[['col_num1', 'col_num2']] = df_types[['col_num1', 'col_num2']].astype(float)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "se1-6DvxFESv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Data Normalization (Simple Max Scaling)\n",
        "Normalization scales numerical data to a common range, like [0, 1]. One simple method is dividing by the maximum value in the column.\n",
        "```\n",
        "\n",
        "**Code Cell 6: Data Normalization (Max Scaling)**"
      ],
      "metadata": {
        "id": "DICdR_qgFESv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_normalize = pd.DataFrame({'Score': [10, 20, 5, 15, 20]})\n",
        "print(\"\\nOriginal DataFrame for normalization:\")\n",
        "print(df_normalize)\n",
        "\n",
        "df_normalize['Score_Normalized'] = df_normalize['Score'] / df_normalize['Score'].max()\n",
        "print(\"\\nDataFrame after max normalization:\")\n",
        "print(df_normalize)\n",
        "# Note: Other common normalization methods include Min-Max scaling and Z-score standardization."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "4d4CrjFWFESv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Binning\n",
        "Binning groups continuous numerical data into discrete \"bins\" or categories.\n",
        "```\n",
        "\n",
        "**Code Cell 7: Binning**"
      ],
      "metadata": {
        "id": "4VDphjxkFESv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_binning = pd.DataFrame({'Age': [22, 25, 31, 45, 52, 23, 38, 60, 29]})\n",
        "print(\"\\nOriginal DataFrame for binning:\")\n",
        "print(df_binning)\n",
        "\n",
        "# Define bin edges\n",
        "# np.linspace creates evenly spaced numbers over a specified interval.\n",
        "# Here, 3 bins mean 4 edges.\n",
        "bins = np.linspace(min(df_binning['Age']), max(df_binning['Age']), 4) # 4 edges for 3 bins\n",
        "print(\"\\nBin edges:\", bins)\n",
        "\n",
        "group_names = ['Young', 'Middle-aged', 'Senior']\n",
        "\n",
        "df_binning['Age_Group'] = pd.cut(df_binning['Age'], bins, labels=group_names, include_lowest=True)\n",
        "print(\"\\nDataFrame after binning 'Age':\")\n",
        "print(df_binning)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "gaImSc4MFESv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Changing Column Names\n",
        "Use `df.rename()` to change column names.\n",
        "```\n",
        "\n",
        "**Code Cell 8: Changing Column Names**"
      ],
      "metadata": {
        "id": "fYh9UE1dFESw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_rename = pd.DataFrame({'old_col_1': [1,2], 'old_col_2': [3,4]})\n",
        "print(\"\\nOriginal DataFrame for renaming:\")\n",
        "print(df_rename)\n",
        "\n",
        "df_rename.rename(columns={'old_col_1': 'new_column_A', 'old_col_2': 'new_column_B'}, inplace=True)\n",
        "print(\"\\nDataFrame after renaming columns:\")\n",
        "print(df_rename)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "GsDosbuNFESw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Indicator Variables (One-Hot Encoding)\n",
        "Converts categorical variables into a set of binary (0 or 1) columns, one for each category. This is essential for many machine learning algorithms.\n",
        "```\n",
        "\n",
        "**Code Cell 9: Indicator Variables (One-Hot Encoding)**"
      ],
      "metadata": {
        "id": "4_Cluih3FESw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_indicator = pd.DataFrame({'Color': ['Red', 'Blue', 'Green', 'Red', 'Blue']})\n",
        "print(\"\\nOriginal DataFrame for indicator variables:\")\n",
        "print(df_indicator)\n",
        "\n",
        "# Create dummy variables for the 'Color' column\n",
        "dummy_variables_color = pd.get_dummies(df_indicator['Color'], prefix='Color') # prefix is optional but good practice\n",
        "print(\"\\nDummy variables created:\")\n",
        "print(dummy_variables_color)\n",
        "\n",
        "# Concatenate the new dummy variable columns to the original DataFrame\n",
        "df_indicator = pd.concat([df_indicator, dummy_variables_color], axis=1)\n",
        "# Optionally, drop the original categorical column\n",
        "# df_indicator.drop('Color', axis=1, inplace=True)\n",
        "print(\"\\nDataFrame after adding dummy variables:\")\n",
        "print(df_indicator)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "LmKHi40yFESw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 3. Exploratory Data Analysis (EDA)\n",
        "This stage involves examining the data to find patterns, anomalies, test hypotheses, and check assumptions with the help of summary statistics and graphical representations.\n",
        "\n",
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Correlation\n",
        "Correlation measures the statistical relationship between two variables.\n",
        "- `df.corr()`: Computes pairwise correlation of all numerical columns.\n",
        "- `df[['col1', 'col2']].corr()`: Computes correlation between specified columns.\n",
        "Correlation coefficients range from -1 to +1.\n",
        "```\n",
        "\n",
        "**Code Cell 10: Correlation**"
      ],
      "metadata": {
        "id": "zGScarJFFESw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample DataFrame for EDA\n",
        "data_eda = {\n",
        "    'EngineSize': [100, 120, 110, 150, 90, 130],\n",
        "    'Horsepower': [70, 90, 80, 110, 60, 100],\n",
        "    'Price': [10000, 15000, 12000, 20000, 9000, 17000],\n",
        "    'Category': ['A', 'B', 'A', 'C', 'A', 'B']\n",
        "}\n",
        "df_eda = pd.DataFrame(data_eda)\n",
        "print(\"Sample DataFrame for EDA:\")\n",
        "print(df_eda)\n",
        "\n",
        "print(\"\\nComplete DataFrame correlation (numerical columns):\")\n",
        "print(df_eda.corr(numeric_only=True)) # numeric_only=True to avoid warnings with mixed types\n",
        "\n",
        "print(\"\\nCorrelation between 'EngineSize' and 'Price':\")\n",
        "print(df_eda[['EngineSize', 'Price']].corr())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "PaYgdWg9FESw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Visualization\n",
        "Visual plots are key to understanding data.\n",
        "- **Scatter Plot (`plt.scatter`)**: Shows the relationship between two numerical variables.\n",
        "- **Regression Plot (`sns.regplot`)**: A scatter plot with a linear regression line fitted to the data. Useful for visualizing linear relationships.\n",
        "- **Box Plot (`sns.boxplot`)**: Displays the distribution of a numerical variable, showing median, quartiles, and potential outliers. Often used to compare distributions across categories.\n",
        "Matplotlib (`plt`) and Seaborn (`sns`) are common plotting libraries.\n",
        "```\n",
        "\n",
        "**Code Cell 11: Scatter Plot, Regression Plot, Box Plot**"
      ],
      "metadata": {
        "id": "kKKkkMgQFESw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scatter plot: Horsepower vs Price\n",
        "plt.figure(figsize=(6,4)) # Create a new figure\n",
        "plt.scatter(df_eda['Horsepower'], df_eda['Price'])\n",
        "plt.xlabel(\"Horsepower\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Scatter Plot: Horsepower vs. Price\")\n",
        "plt.show() # Display the plot\n",
        "\n",
        "# Regression plot: EngineSize vs Price\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.regplot(x='EngineSize', y='Price', data=df_eda)\n",
        "plt.title(\"Regression Plot: EngineSize vs. Price\")\n",
        "plt.show()\n",
        "\n",
        "# Box plot: Price distribution by Category\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.boxplot(x='Category', y='Price', data=df_eda)\n",
        "plt.title(\"Box Plot: Price by Category\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "s41MJnmaFESx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Grouping Data\n",
        "`df.groupby()` allows you to group data based on some criteria and then apply an aggregate function (like mean, sum, count).\n",
        "- Can group by a single attribute or multiple attributes.\n",
        "`as_index=False` keeps the grouping keys as columns rather than setting them as the index.\n",
        "```\n",
        "\n",
        "**Code Cell 12: GroupBy Statements**"
      ],
      "metadata": {
        "id": "QRAksd_8FESx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by a single attribute ('Category') and calculate the mean of other numerical columns\n",
        "df_group_single = df_eda[['Category', 'Price', 'Horsepower']] # Select relevant columns\n",
        "grouped_by_category_mean = df_group_single.groupby(['Category'], as_index=False).mean()\n",
        "print(\"\\nMean Price and Horsepower grouped by Category:\")\n",
        "print(grouped_by_category_mean)\n",
        "\n",
        "# Create a more diverse category for multiple grouping example\n",
        "df_eda['SubCategory'] = ['S1', 'S1', 'S2', 'S1', 'S2', 'S2']\n",
        "\n",
        "# Group by multiple attributes ('Category', 'SubCategory')\n",
        "df_group_multiple = df_eda[['Category', 'SubCategory', 'Price']]\n",
        "grouped_by_multi_mean = df_group_multiple.groupby(['Category', 'SubCategory'], as_index=False).mean()\n",
        "print(\"\\nMean Price grouped by Category and SubCategory:\")\n",
        "print(grouped_by_multi_mean)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "vYvN_h_LFESx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Pivot Tables\n",
        "Pivot tables reshape data, allowing you to summarize and aggregate it with one variable along the rows, another along the columns, and values in the cells.\n",
        "```\n",
        "\n",
        "**Code Cell 13: Pivot Tables**"
      ],
      "metadata": {
        "id": "P0IIl6hUFESx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the grouped_by_multi_mean DataFrame to create a pivot table\n",
        "# Let's make sure we have a suitable structure for pivot\n",
        "# For example, let's use df_eda and average price by Category and SubCategory\n",
        "# The df_group_multiple used above already has 'Category', 'SubCategory', 'Price'\n",
        "# Let's say we want 'Category' as index, 'SubCategory' as columns, and 'Price' (mean) as values.\n",
        "\n",
        "# We first need to group and get the mean if we didn't have grouped_by_multi_mean\n",
        "grouped_for_pivot = df_eda.groupby(['Category', 'SubCategory'], as_index=False)['Price'].mean()\n",
        "print(\"\\nData prepared for pivot table (average price):\")\n",
        "print(grouped_for_pivot)\n",
        "\n",
        "# Create the pivot table\n",
        "pivot_table_result = grouped_for_pivot.pivot(index='Category', columns='SubCategory', values='Price')\n",
        "print(\"\\nPivot Table (Category as index, SubCategory as columns, Avg Price as values):\")\n",
        "print(pivot_table_result)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "zlWGu7FCFESx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Heatmap / Pseudocolor Plot\n",
        "A heatmap visually represents matrix-like data where values are depicted by color intensity. `plt.pcolor()` can be used with pivot table data.\n",
        "```\n",
        "\n",
        "**Code Cell 14: Pseudocolor Plot (Heatmap of Pivot Table)**"
      ],
      "metadata": {
        "id": "UEGQwE-1FESx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the pivot_table_result from the previous cell\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.pcolor(pivot_table_result, cmap='RdBu') # RdBu is a Red-Blue colormap\n",
        "plt.colorbar(label='Average Price') # Add a color bar to indicate values\n",
        "plt.yticks(np.arange(0.5, len(pivot_table_result.index), 1), pivot_table_result.index) # Set Y-axis ticks and labels\n",
        "plt.xticks(np.arange(0.5, len(pivot_table_result.columns), 1), pivot_table_result.columns) # Set X-axis ticks and labels\n",
        "plt.xlabel(\"SubCategory\")\n",
        "plt.ylabel(\"Category\")\n",
        "plt.title(\"Heatmap of Average Price by Category and SubCategory\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "F5mKYyixFESx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Pearson Coefficient and p-value\n",
        "The Pearson correlation coefficient measures the linear correlation between two continuous variables. The p-value helps determine the statistical significance of the correlation.\n",
        "- `scipy.stats.pearsonr(col1, col2)` returns the Pearson coefficient and the p-value.\n",
        "```\n",
        "\n",
        "**Code Cell 15: Pearson Coefficient and p-value**"
      ],
      "metadata": {
        "id": "SUJi1y7jFESy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Calculate Pearson correlation between 'Horsepower' and 'Price'\n",
        "pearson_coef, p_value = stats.pearsonr(df_eda['Horsepower'], df_eda['Price'])\n",
        "\n",
        "print(f\"\\nPearson Correlation Coefficient (Horsepower vs Price): {pearson_coef:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"The correlation is statistically significant (p < 0.05).\")\n",
        "else:\n",
        "    print(\"The correlation is not statistically significant (p >= 0.05).\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "w9zLNFPDFESy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 4. Model Development\n",
        "This section deals with creating predictive models. The cheat sheet focuses on Linear and Polynomial Regression.\n",
        "\n",
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Linear Regression\n",
        "Linear regression models the relationship between a dependent variable (target) and one or more independent variables (features) by fitting a linear equation.\n",
        "- **Simple Linear Regression:** One independent variable.\n",
        "- **Multiple Linear Regression:** Multiple independent variables.\n",
        "The `sklearn.linear_model.LinearRegression` class is used.\n",
        "```\n",
        "\n",
        "**Code Cell 16: Linear Regression - Object Creation and Training**"
      ],
      "metadata": {
        "id": "UKudcxRlFESy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create a Linear Regression model object\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Prepare data for training (using df_eda)\n",
        "# Single attribute (Simple Linear Regression): Predicting Price from Horsepower\n",
        "X_simple = df_eda[['Horsepower']] # Features need to be 2D (DataFrame)\n",
        "Y_target = df_eda['Price']       # Target is 1D (Series)\n",
        "\n",
        "# Train the Simple Linear Regression model\n",
        "lr.fit(X_simple, Y_target)\n",
        "print(\"Simple Linear Regression model trained.\")\n",
        "\n",
        "# Multiple attributes (Multiple Linear Regression): Predicting Price from Horsepower and EngineSize\n",
        "X_multiple = df_eda[['Horsepower', 'EngineSize']]\n",
        "# Y_target is the same\n",
        "\n",
        "# Create a new lr object for multiple regression or re-train the existing one\n",
        "lr_multiple = LinearRegression()\n",
        "lr_multiple.fit(X_multiple, Y_target)\n",
        "print(\"\\nMultiple Linear Regression model trained.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "FvGHBdRdFESy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Generating Predictions and Model Parameters\n",
        "- `lr.predict(X)`: Generates predictions for new input `X`.\n",
        "- `lr.coef_`: Returns the slope coefficient(s) (m). For multiple regression, it's an array.\n",
        "- `lr.intercept_`: Returns the intercept (c).\n",
        "The linear model is defined by $Y = mX + c$ (simple) or $Y = m_1X_1 + m_2X_2 + ... + c$ (multiple).\n",
        "```\n",
        "\n",
        "**Code Cell 17: Predictions and Model Parameters (Linear Regression)**"
      ],
      "metadata": {
        "id": "wEDzYI8CFESy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions using the simple linear regression model\n",
        "Y_hat_simple = lr.predict(X_simple)\n",
        "print(\"\\nPredictions from Simple Linear Regression (first 3):\")\n",
        "print(Y_hat_simple[:3])\n",
        "\n",
        "# Identify coefficient and intercept for the simple linear model\n",
        "coeff_simple = lr.coef_\n",
        "intercept_simple = lr.intercept_\n",
        "print(f\"Simple LR - Coefficient (slope for Horsepower): {coeff_simple[0]:.2f}\") # lr.coef_ is an array\n",
        "print(f\"Simple LR - Intercept: {intercept_simple:.2f}\")\n",
        "\n",
        "# Predictions using the multiple linear regression model\n",
        "Y_hat_multiple = lr_multiple.predict(X_multiple)\n",
        "print(\"\\nPredictions from Multiple Linear Regression (first 3):\")\n",
        "print(Y_hat_multiple[:3])\n",
        "\n",
        "# Identify coefficients and intercept for the multiple linear model\n",
        "coeffs_multiple = lr_multiple.coef_\n",
        "intercept_multiple = lr_multiple.intercept_\n",
        "print(f\"Multiple LR - Coefficients (for Horsepower, EngineSize): {coeffs_multiple}\")\n",
        "print(f\"Multiple LR - Intercept: {intercept_multiple:.2f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Bxkz2HpBFESy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Residual Plot\n",
        "A residual plot shows the residuals (differences between actual and predicted values) on the y-axis and an independent variable (or predicted values) on the x-axis.\n",
        "It helps to check if the linear model assumptions are met (e.g., residuals are randomly scattered around zero). Patterns in residuals suggest the model might not be a good fit.\n",
        "Seaborn's `sns.residplot()` is used.\n",
        "```\n",
        "\n",
        "**Code Cell 18: Residual Plot**"
      ],
      "metadata": {
        "id": "fYJj3lYTFESy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Residual plot for the Simple Linear Regression (Price vs Horsepower)\n",
        "plt.figure(figsize=(7,5))\n",
        "# sns.residplot(x=df_eda['Horsepower'], y=df_eda['Price'], model=lr) # Can pass the fitted model directly\n",
        "# Or, if using specific X and Y that were used for fitting:\n",
        "sns.residplot(x=X_simple['Horsepower'], y=Y_target) # x needs to be 1D for residplot's x usually\n",
        "plt.title(\"Residual Plot for Simple Linear Regression (Horsepower vs Price)\")\n",
        "plt.xlabel(\"Horsepower\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.show()\n",
        "\n",
        "# For multiple regression, you typically plot residuals against predicted values or against each feature\n",
        "# Residuals = Y_target - Y_hat_multiple\n",
        "# plt.scatter(Y_hat_multiple, Y_target - Y_hat_multiple)\n",
        "# plt.axhline(0, color='red', linestyle='--')\n",
        "# plt.xlabel(\"Predicted Values\")\n",
        "# plt.ylabel(\"Residuals\")\n",
        "# plt.title(\"Residual Plot for Multiple Linear Regression (Residuals vs Predicted)\")\n",
        "# plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Jgyz_rTuFESz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Distribution Plot (KDE Plot)\n",
        "`sns.distplot()` (now often replaced by `sns.kdeplot()` or `sns.histplot()`) visualizes the distribution of a single variable.\n",
        "Using `hist=False` with `distplot` or just `kdeplot` shows the Kernel Density Estimate, a smoothed representation of the distribution.\n",
        "This is useful for comparing the distribution of predicted values vs. actual values.\n",
        "```\n",
        "\n",
        "**Code Cell 19: Distribution Plot (KDE Plot)**"
      ],
      "metadata": {
        "id": "KtfqNRN7FESz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting distribution of actual 'Price' and predicted 'Price' from simple LR\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.kdeplot(Y_target, label='Actual Price', fill=True, alpha=0.5)\n",
        "sns.kdeplot(Y_hat_simple, label='Predicted Price (Simple LR)', fill=True, alpha=0.5)\n",
        "plt.title(\"Distribution of Actual vs. Predicted Prices (Simple LR)\")\n",
        "plt.xlabel(\"Price\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Note: sns.distplot is deprecated. sns.kdeplot or sns.histplot(kde=True) are alternatives.\n",
        "# Example using kdeplot (as above) is the modern way."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "QiIzdFrrFES0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Polynomial Regression (Single Variable)\n",
        "Models a non-linear relationship using a polynomial equation of degree `n`.\n",
        "NumPy's `np.polyfit(x, y, n)` fits a polynomial and returns its coefficients.\n",
        "`np.poly1d(coefficients)` creates a polynomial function object from these coefficients, which can then be used for prediction.\n",
        "```\n",
        "\n",
        "**Code Cell 20: Polynomial Regression (Single Variable with NumPy)**"
      ],
      "metadata": {
        "id": "SzJpfuc-FES0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a slice of the data for a clearer polynomial fit example\n",
        "x_poly_data = df_eda['EngineSize'].values # NumPy array\n",
        "y_poly_data = df_eda['Price'].values      # NumPy array\n",
        "\n",
        "degree = 2 # Degree of the polynomial\n",
        "\n",
        "# Fit the polynomial model\n",
        "# f contains the coefficients of the polynomial\n",
        "coefficients_poly = np.polyfit(x_poly_data, y_poly_data, degree)\n",
        "print(f\"\\nPolynomial coefficients (degree {degree}): {coefficients_poly}\")\n",
        "\n",
        "# Create the polynomial model function\n",
        "p_model = np.poly1d(coefficients_poly)\n",
        "print(f\"Polynomial model equation:\\n{p_model}\")\n",
        "\n",
        "# Generate predicted output using the polynomial model\n",
        "Y_hat_poly = p_model(x_poly_data)\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.scatter(x_poly_data, y_poly_data, label='Actual Data')\n",
        "# Sort x values for a smooth line plot\n",
        "sorted_indices = np.argsort(x_poly_data)\n",
        "plt.plot(x_poly_data[sorted_indices], Y_hat_poly[sorted_indices], color='red', label=f'Polynomial Fit (degree {degree})')\n",
        "plt.xlabel(\"Engine Size\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Single Variable Polynomial Regression\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "YAiY7bSOFES5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Multivariate Polynomial Regression (Scikit-learn)\n",
        "Extends polynomial regression to multiple independent variables.\n",
        "`sklearn.preprocessing.PolynomialFeatures(degree=n)` generates a new feature matrix consisting of all polynomial combinations of the features up to the specified degree. This transformed feature matrix can then be used with a linear regression model.\n",
        "```\n",
        "\n",
        "**Code Cell 21: Multivariate Polynomial Regression (Scikit-learn)**"
      ],
      "metadata": {
        "id": "8G_YsFahFES5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Features for multivariate polynomial regression\n",
        "Z_multi_poly = df_eda[['Horsepower', 'EngineSize']]\n",
        "# Y_target is the same df_eda['Price']\n",
        "\n",
        "# Create PolynomialFeatures object\n",
        "poly_features_transformer = PolynomialFeatures(degree=2, include_bias=False) # include_bias=False is common as LinearRegression handles it\n",
        "\n",
        "# Transform the original features into polynomial features\n",
        "Z_multi_poly_transformed = poly_features_transformer.fit_transform(Z_multi_poly)\n",
        "print(f\"\\nOriginal features shape: {Z_multi_poly.shape}\")\n",
        "print(f\"Transformed polynomial features shape (degree 2): {Z_multi_poly_transformed.shape}\")\n",
        "print(\"Feature names after transformation (if you use get_feature_names_out):\")\n",
        "print(poly_features_transformer.get_feature_names_out(['Horsepower', 'EngineSize']))\n",
        "\n",
        "# Now, fit a linear regression model using these transformed features\n",
        "lr_poly_multi = LinearRegression()\n",
        "lr_poly_multi.fit(Z_multi_poly_transformed, Y_target)\n",
        "print(\"\\nMultivariate Polynomial Regression model trained.\")\n",
        "\n",
        "# Predict using the transformed features\n",
        "# For new data, it must also be transformed using poly_features_transformer.transform()\n",
        "Y_hat_poly_multi = lr_poly_multi.predict(Z_multi_poly_transformed)\n",
        "print(\"Predictions from Multivariate Polynomial Regression (first 3):\")\n",
        "print(Y_hat_poly_multi[:3])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "hFS2H9QrFES5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Pipeline\n",
        "`sklearn.pipeline.Pipeline` allows chaining multiple processing steps (e.g., scaling, polynomial feature creation, model fitting) into a single estimator.\n",
        "This simplifies workflows, prevents data leakage from test set to training set during preprocessing, and makes grid search over parameters of different steps easier.\n",
        "Each step is a tuple of `('name', estimator_object)`.\n",
        "```\n",
        "\n",
        "**Code Cell 22: Pipeline**"
      ],
      "metadata": {
        "id": "eT2_iyusFES6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler # For feature scaling\n",
        "\n",
        "# Define the steps in the pipeline\n",
        "# 1. Scale data (StandardScaler)\n",
        "# 2. Create polynomial features (PolynomialFeatures)\n",
        "# 3. Fit a linear regression model (LinearRegression)\n",
        "Input_pipeline_steps = [\n",
        "    ('scale', StandardScaler()),\n",
        "    ('polynomial', PolynomialFeatures(degree=2, include_bias=False)),\n",
        "    ('model', LinearRegression())\n",
        "]\n",
        "\n",
        "# Create the pipeline object\n",
        "pipe = Pipeline(Input_pipeline_steps)\n",
        "\n",
        "# Data for the pipeline (using Z_multi_poly and Y_target from previous examples)\n",
        "# Ensure Z is float (StandardScaler might require it)\n",
        "Z_pipe_data = Z_multi_poly.astype(float)\n",
        "# Y_target is already appropriate\n",
        "\n",
        "# Fit the entire pipeline to the data\n",
        "pipe.fit(Z_pipe_data, Y_target)\n",
        "print(\"\\nPipeline trained.\")\n",
        "\n",
        "# Make predictions using the pipeline\n",
        "# The pipeline automatically applies all transformations before prediction\n",
        "Y_pipe_predictions = pipe.predict(Z_pipe_data)\n",
        "print(\"Predictions from Pipeline (first 3):\")\n",
        "print(Y_pipe_predictions[:3])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "e6hWihQ5FES6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### R² value (Coefficient of Determination)\n",
        "R² measures how well the regression model fits the observed data. It represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n",
        "- Ranges from 0 to 1 (or can be negative for very poor models). Higher is generally better.\n",
        "- For Scikit-learn linear models, `model.score(X, Y)` directly returns R².\n",
        "- For NumPy polynomial models, `sklearn.metrics.r2_score(y_true, y_predicted)` can be used.\n",
        "```\n",
        "\n",
        "**Code Cell 23: R² Value**"
      ],
      "metadata": {
        "id": "jxb63JGWFES6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# R² for Simple Linear Regression (using lr trained earlier)\n",
        "r2_score_simple_lr = lr.score(X_simple, Y_target)\n",
        "print(f\"\\nR² for Simple Linear Regression (Horsepower vs Price): {r2_score_simple_lr:.4f}\")\n",
        "\n",
        "# R² for Multiple Linear Regression (using lr_multiple)\n",
        "r2_score_multiple_lr = lr_multiple.score(X_multiple, Y_target)\n",
        "print(f\"R² for Multiple Linear Regression: {r2_score_multiple_lr:.4f}\")\n",
        "\n",
        "# R² for Single Variable Polynomial Regression (using NumPy model p_model and its predictions Y_hat_poly)\n",
        "# y_poly_data is the true y, Y_hat_poly is the predicted y\n",
        "r2_score_numpy_poly = r2_score(y_poly_data, Y_hat_poly)\n",
        "print(f\"R² for NumPy Polynomial Regression (degree 2): {r2_score_numpy_poly:.4f}\")\n",
        "\n",
        "# R² for Multivariate Polynomial Regression (using lr_poly_multi and Z_multi_poly_transformed)\n",
        "r2_score_sklearn_poly_multi = lr_poly_multi.score(Z_multi_poly_transformed, Y_target)\n",
        "print(f\"R² for Scikit-learn Multivariate Polynomial Regression: {r2_score_sklearn_poly_multi:.4f}\")\n",
        "\n",
        "# R² for Pipeline model\n",
        "r2_score_pipeline = pipe.score(Z_pipe_data, Y_target)\n",
        "print(f\"R² for Pipeline model: {r2_score_pipeline:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "AAjWdlOXFES6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### MSE value (Mean Squared Error)\n",
        "MSE measures the average of the squares of the errors (the difference between actual and estimated values).\n",
        "Lower MSE indicates a better fit.\n",
        "`sklearn.metrics.mean_squared_error(y_true, y_predicted)` is used.\n",
        "```\n",
        "\n",
        "**Code Cell 24: MSE Value**"
      ],
      "metadata": {
        "id": "VK8S8Y_OFES6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# MSE for Simple Linear Regression\n",
        "mse_simple_lr = mean_squared_error(Y_target, Y_hat_simple)\n",
        "print(f\"\\nMSE for Simple Linear Regression: {mse_simple_lr:.2f}\")\n",
        "\n",
        "# MSE for Multiple Linear Regression\n",
        "mse_multiple_lr = mean_squared_error(Y_target, Y_hat_multiple)\n",
        "print(f\"MSE for Multiple Linear Regression: {mse_multiple_lr:.2f}\")\n",
        "\n",
        "# MSE for NumPy Polynomial Regression\n",
        "mse_numpy_poly = mean_squared_error(y_poly_data, Y_hat_poly)\n",
        "print(f\"MSE for NumPy Polynomial Regression: {mse_numpy_poly:.2f}\")\n",
        "\n",
        "# MSE for Scikit-learn Multivariate Polynomial Regression\n",
        "mse_sklearn_poly_multi = mean_squared_error(Y_target, Y_hat_poly_multi)\n",
        "print(f\"MSE for Scikit-learn Multivariate Polynomial Regression: {mse_sklearn_poly_multi:.2f}\")\n",
        "\n",
        "# MSE for Pipeline model\n",
        "mse_pipeline = mean_squared_error(Y_target, Y_pipe_predictions)\n",
        "print(f\"MSE for Pipeline model: {mse_pipeline:.2f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ZV7EEON9FES7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 5. Model Evaluation and Refinement\n",
        "This involves assessing model performance more robustly and tuning models.\n",
        "\n",
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Splitting Data for Training and Testing\n",
        "It's crucial to evaluate a model on data it hasn't seen during training.\n",
        "`sklearn.model_selection.train_test_split` splits data into random train and test subsets.\n",
        "- `test_size`: Proportion of the dataset to include in the test split.\n",
        "- `random_state`: Ensures reproducibility of the split.\n",
        "```\n",
        "\n",
        "**Code Cell 25: Train/Test Split**"
      ],
      "metadata": {
        "id": "f9llHOqRFES7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Using df_eda as an example dataset\n",
        "# Let's assume 'Price' is the target and all other numeric columns are features\n",
        "X_data_full = df_eda[['EngineSize', 'Horsepower']] # Features\n",
        "Y_data_full = df_eda['Price']                     # Target\n",
        "\n",
        "# Split data: 10% for testing, 90% for training, random_state for reproducibility\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_data_full, Y_data_full, test_size=0.20, random_state=42) # Test size 0.2 for more data\n",
        "\n",
        "print(f\"\\nShape of x_train: {x_train.shape}, y_train: {y_train.shape}\")\n",
        "print(f\"Shape of x_test: {x_test.shape}, y_test: {y_test.shape}\")\n",
        "\n",
        "# Now you would train your model on x_train, y_train\n",
        "# And evaluate it on x_test, y_test\n",
        "lr_eval = LinearRegression()\n",
        "lr_eval.fit(x_train, y_train)\n",
        "r2_test = lr_eval.score(x_test, y_test)\n",
        "print(f\"R² score on the test set: {r2_test:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ou73UZlAFES7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Cross-Validation Score\n",
        "Cross-validation provides a more robust measure of model performance by training and testing the model on different subsets (folds) of the data.\n",
        "`sklearn.model_selection.cross_val_score` performs K-fold cross-validation.\n",
        "- `cv=n`: Number of folds.\n",
        "It returns an array of scores (e.g., R²) for each fold. The mean and standard deviation of these scores give an overall performance estimate.\n",
        "```\n",
        "\n",
        "**Code Cell 26: Cross-Validation Score**"
      ],
      "metadata": {
        "id": "mxg71j-HFES7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "lre_cv = LinearRegression() # Model instance\n",
        "\n",
        "# Perform 3-fold cross-validation on X_data_full, Y_data_full\n",
        "# The scoring parameter can be specified, default for LinearRegression is R²\n",
        "# Using X_data_full[['EngineSize']] as an example with one feature for simplicity in the cheat sheet's style\n",
        "rcross_scores = cross_val_score(lre_cv, X_data_full[['EngineSize']], Y_data_full, cv=3) # cv=n, number of folds\n",
        "\n",
        "print(f\"\\nCross-validation R² scores for each fold: {rcross_scores}\")\n",
        "print(f\"Mean R² from cross-validation: {rcross_scores.mean():.4f}\")\n",
        "print(f\"Standard deviation of R² from cross-validation: {rcross_scores.std():.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "VTr6WVZqFES7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Cross-Validation Prediction\n",
        "`sklearn.model_selection.cross_val_predict` generates predictions for each data point by training the model on the other folds.\n",
        "This is useful for visualizing model performance or for creating out-of-sample predictions for further analysis.\n",
        "```\n",
        "\n",
        "**Code Cell 27: Cross-Validation Prediction**"
      ],
      "metadata": {
        "id": "4Nxqslw7FES7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "lre_cv_pred = LinearRegression()\n",
        "\n",
        "# Get cross-validated predictions\n",
        "# Using X_data_full[['EngineSize']] as an example with one feature\n",
        "yhat_cv = cross_val_predict(lre_cv_pred, X_data_full[['EngineSize']], Y_data_full, cv=3)\n",
        "\n",
        "print(\"\\nFirst 5 cross-validated predictions:\")\n",
        "print(yhat_cv[:5])\n",
        "\n",
        "# These predictions can be compared to Y_data_full to assess out-of-sample performance\n",
        "r2_cv_pred = r2_score(Y_data_full, yhat_cv)\n",
        "print(f\"R² score using cross-validated predictions: {r2_cv_pred:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "mPJwW19IFES8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Ridge Regression\n",
        "Ridge Regression is a linear regression model that includes L2 regularization.\n",
        "Regularization adds a penalty term to the loss function to shrink coefficient magnitudes, helping to prevent overfitting, especially when dealing with multicollinearity or many features.\n",
        "- `alpha`: The regularization strength. Higher alpha means stronger regularization (more shrinkage).\n",
        "It's often used with polynomial features to control their complexity.\n",
        "```\n",
        "\n",
        "**Code Cell 28: Ridge Regression**"
      ],
      "metadata": {
        "id": "XeW8h2QCFES8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Assume x_train, x_test, y_train, y_test are from the train_test_split earlier\n",
        "# And we want to use polynomial features with Ridge regression\n",
        "\n",
        "# Create polynomial features\n",
        "poly_ridge_transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
        "x_train_poly_ridge = poly_ridge_transformer.fit_transform(x_train)\n",
        "x_test_poly_ridge = poly_ridge_transformer.transform(x_test) # Use transform on test data, not fit_transform\n",
        "\n",
        "# Create and train Ridge Regression model\n",
        "# Alpha is the regularization strength; larger values specify stronger regularization.\n",
        "ridge_model = Ridge(alpha=1.0) # Common default for alpha\n",
        "ridge_model.fit(x_train_poly_ridge, y_train)\n",
        "print(\"\\nRidge Regression model trained with polynomial features.\")\n",
        "\n",
        "# Make predictions\n",
        "yhat_ridge = ridge_model.predict(x_test_poly_ridge)\n",
        "print(\"First 3 predictions from Ridge Regression:\")\n",
        "print(yhat_ridge[:3])\n",
        "\n",
        "# Evaluate the Ridge model\n",
        "r2_ridge = ridge_model.score(x_test_poly_ridge, y_test)\n",
        "print(f\"R² score for Ridge Regression on test set: {r2_ridge:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "H-0CZoTvFES8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell:**\n",
        "```markdown\n",
        "#### Grid Search\n",
        "`sklearn.model_selection.GridSearchCV` systematically searches for the best combination of hyperparameter values for a model.\n",
        "It tries all specified combinations using cross-validation and selects the one that performs best on average.\n",
        "- `parameters`: A dictionary or list of dictionaries where keys are parameter names and values are lists of settings to try.\n",
        "- `cv`: Number of cross-validation folds.\n",
        "`.best_estimator_` gives the model instance with the best found parameters.\n",
        "```\n",
        "\n",
        "**Code Cell 29: Grid Search for Ridge Regression Alpha**"
      ],
      "metadata": {
        "id": "RjUk8RA3FES8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for Ridge's alpha\n",
        "# These are the alpha values that GridSearchCV will test\n",
        "parameters_grid = [{'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}]\n",
        "\n",
        "# Create a Ridge regression object\n",
        "RR_grid = Ridge()\n",
        "\n",
        "# Create GridSearchCV object\n",
        "# It will search for the best alpha from parameters_grid using 4-fold CV\n",
        "Grid1 = GridSearchCV(RR_grid, parameters_grid, cv=4, scoring='r2') # Can specify scoring metric\n",
        "\n",
        "# Fit GridSearchCV to the polynomial training data (using data from Ridge example)\n",
        "# For GridSearch, it's common to use the full training set (x_train_poly_ridge) or even the entire dataset (X_data_full_poly)\n",
        "# if the final model selection includes retraining on all available data after finding best params.\n",
        "# Here, let's use the polynomial transformed training data.\n",
        "Grid1.fit(x_train_poly_ridge, y_train) # Using the polynomial features from Ridge example\n",
        "\n",
        "print(\"\\nGridSearchCV fitting completed.\")\n",
        "\n",
        "# Get the best estimator (model with best parameters) found by GridSearchCV\n",
        "BestRR = Grid1.best_estimator_\n",
        "print(f\"Best alpha found by GridSearchCV: {BestRR.alpha}\")\n",
        "\n",
        "# Evaluate the best model on the (polynomial transformed) test set\n",
        "r2_score_best_ridge = BestRR.score(x_test_poly_ridge, y_test)\n",
        "print(f\"R² score of the best Ridge model on test set: {r2_score_best_ridge:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "QNy76ObXFES8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Practice Questions (50)\n",
        "\n",
        "**Data Loading**\n",
        "\n",
        "1.  What Pandas function is used to read a CSV file?\n",
        "2.  How do you read a CSV file that does not contain a header row, assigning default integer column names?\n",
        "3.  How can you specify custom column names when reading a CSV file that has no header?\n",
        "4.  What does `df.head(7)` do?\n",
        "5.  What does `df.tail()` return by default?\n",
        "6.  How can you get a list of all column names in a DataFrame `df`?\n",
        "7.  What method would you use to change all column names of `df` to a new list `new_headers`?\n",
        "8.  How do you replace all occurrences of the string \"-\" with `np.nan` in a DataFrame `df`?\n",
        "9.  What information does `df.dtypes` provide?\n",
        "10. What is the difference between `df.describe()` and `df.describe(include=\"all\")`?\n",
        "11. What does `df.info()` show?\n",
        "12. How do you save a DataFrame `df` to a CSV file named 'output.csv' without writing the index?\n",
        "\n",
        "**Data Wrangling**\n",
        "\n",
        "13. How can you replace missing values (`np.nan`) in a column 'Age' with its median?\n",
        "14. How do you find the most frequent value in a categorical column 'Category'?\n",
        "15. How do you convert a column 'PriceString' (containing numbers as strings) to a float data type?\n",
        "16. What is the purpose of data normalization? Explain one simple normalization technique.\n",
        "17. How can you use `np.linspace` and `pd.cut` to bin a numerical column 'Score' into 5 equal-width bins?\n",
        "18. How would you rename a column 'OldName' to 'NewName' in DataFrame `df` permanently?\n",
        "19. What are indicator variables (one-hot encoding), and why are they useful?\n",
        "20. What Pandas function creates one-hot encoded columns from a categorical column?\n",
        "\n",
        "**Exploratory Data Analysis (EDA)**\n",
        "\n",
        "21. How do you compute the pairwise correlation of all numerical columns in a DataFrame `df`?\n",
        "22. How would you get the correlation only between columns 'A' and 'B' from `df`?\n",
        "23. What type of plot is best for visualizing the relationship between two numerical variables to see if they cluster or follow a trend?\n",
        "24. What does `sns.regplot(x='feature', y='target', data=df)` show?\n",
        "25. What information does a box plot (`sns.boxplot`) convey about a variable's distribution?\n",
        "26. How can you group a DataFrame `df` by a column 'Region' and then calculate the average 'Sales' for each region?\n",
        "27. What is a pivot table, and what is it used for in data analysis?\n",
        "28. How can you visually represent a pivot table's data using colors (like a heatmap)?\n",
        "29. What two values does `scipy.stats.pearsonr()` return?\n",
        "30. If `pearsonr()` returns a p-value of 0.001 for two variables, what does this suggest about their linear correlation?\n",
        "\n",
        "**Model Development**\n",
        "\n",
        "31. What Scikit-learn class is used for creating a linear regression model?\n",
        "32. How do you train a linear regression model `lr` using features `X` and target `Y`?\n",
        "33. After training, how do you get the predicted values `Y_hat` for input features `X_test` using model `lr`?\n",
        "34. What attributes of a fitted `LinearRegression` object store the slope coefficient(s) and the intercept?\n",
        "35. What is a residual plot, and what does a random scatter of points around zero in a residual plot indicate?\n",
        "36. Which Seaborn function is primarily recommended now for plotting a Kernel Density Estimate of a variable (replacing `distplot` for this specific purpose)?\n",
        "37. How can you perform single-variable polynomial regression of degree 3 for `x` and `y` using NumPy functions?\n",
        "38. What is the role of `sklearn.preprocessing.PolynomialFeatures`?\n",
        "39. What is a Scikit-learn `Pipeline` and what is one of its key benefits?\n",
        "40. Name three common steps that might be included in a machine learning `Pipeline`.\n",
        "\n",
        "**Model Evaluation and Refinement**\n",
        "\n",
        "41. What is the purpose of splitting data into training and testing sets? Which Scikit-learn function is used for this?\n",
        "42. What does `test_size=0.3` signify in `train_test_split`?\n",
        "43. Explain K-fold cross-validation. What Scikit-learn function performs this and returns scores for each fold?\n",
        "44. If `cross_val_score` returns `[0.8, 0.85, 0.75]`, what do these values represent, and how would you get an overall performance measure?\n",
        "45. What does `cross_val_predict` return, and how is it different from `cross_val_score`?\n",
        "46. What is Ridge Regression, and what problem does its `alpha` parameter help address?\n",
        "47. How does a larger `alpha` value affect a Ridge Regression model?\n",
        "48. What is `GridSearchCV` used for in model tuning?\n",
        "49. What does the `.best_estimator_` attribute of a fitted `GridSearchCV` object contain?\n",
        "50. What does R² (Coefficient of Determination) measure? What does an MSE (Mean Squared Error) of 0 indicate?"
      ],
      "metadata": {
        "id": "78VdV75NFES8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"md-recitation\">\n",
        "  Sources\n",
        "  <ol>\n",
        "  <li><a href=\"https://github.com/Karthik311099/DSA-Assignments\">https://github.com/Karthik311099/DSA-Assignments</a></li>\n",
        "  <li><a href=\"https://www.qconcursos.com/questoes-de-concursos/questoes/bdf148ad-c7\">https://www.qconcursos.com/questoes-de-concursos/questoes/bdf148ad-c7</a></li>\n",
        "  <li><a href=\"https://github.com/r-rodri/ImportedCars\">https://github.com/r-rodri/ImportedCars</a></li>\n",
        "  <li><a href=\"https://github.com/haininhhoang94/funix-data-science\">https://github.com/haininhhoang94/funix-data-science</a></li>\n",
        "  </ol>\n",
        "</div>"
      ],
      "metadata": {
        "id": "ZbqRfueiFES8"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}