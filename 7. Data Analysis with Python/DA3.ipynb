{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Okay, here is the continuation of your comprehensive notes, covering Module 6.\n",
        "\n",
        "---\n",
        "\n",
        "**Module 6: Essential Data Preprocessing for Modeling**\n",
        "\n",
        "Before data can be effectively used to train machine learning models, it often requires significant preprocessing. Raw data is rarely in the perfect shape for algorithms. This module covers crucial steps like defining features and targets, scaling numerical features to a common range, encoding categorical data into a numerical format, and binning continuous variables into discrete categories. These steps are vital for improving model performance and ensuring algorithms work correctly.\n",
        "\n",
        "**6.1 Defining Features, Target Variables, and Predictors**\n",
        "\n",
        "In the context of supervised machine learning, it's essential to clearly distinguish between the input data used for making predictions and the output data we are trying to predict.\n",
        "\n",
        "* **Features (X):**\n",
        "    * **Also known as:** Independent Variables, Predictors, Input Variables, Attributes.\n",
        "    * **Definition:** These are the input variables that a machine learning model uses to learn patterns and make predictions. Features represent the characteristics, properties, or measurements of the data instances being analyzed. *[66, 68]*\n",
        "    * **Example:** In a model to predict house prices, features could be 'size_in_square_feet', 'number_of_bedrooms', 'age_of_house', 'location_rating'.\n",
        "    * **Types:** Features can be numerical (e.g., age, income, temperature) or categorical (e.g., gender, product category, city). *[67]*\n",
        "\n",
        "* **Target Variable (y):**\n",
        "    * **Also known as:** Dependent Variable, Label, Outcome Variable, Response Variable.\n",
        "    * **Definition:** This is the specific outcome or value that the machine learning model aims to predict or classify. *[70]*\n",
        "    * **Example:** In a house price prediction model, the 'price' of the house would be the target variable. In an email spam detection model, the target variable would be 'is_spam' (e.g., 0 for not spam, 1 for spam).\n",
        "\n",
        "* **Separating Features (X) and Target (y) in a DataFrame:**\n",
        "    * A common practice is to split your dataset into two separate Pandas objects:\n",
        "        * `X`: A DataFrame containing all the feature columns.\n",
        "        * `y`: A Pandas Series (or sometimes a DataFrame with a single column) containing the target variable."
      ],
      "metadata": {
        "id": "2RxBKg8InQas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    # Sample data for a hypothetical modeling scenario (predicting 'ExamScore')\n",
        "    data_for_model_raw = {\n",
        "        'StudentID': range(1, 7),\n",
        "        'HoursStudied': [2, 5, 1, 6, 4, 3],\n",
        "        'PreviousGrade': [70, 85, 60, 90, 75, 80],\n",
        "        'Attendance': ['Good', 'Good', 'Poor', 'Good', 'Poor', 'Good'], # Categorical feature\n",
        "        'ExamScore': [65, 88, 55, 92, 70, 78] # Target variable\n",
        "    }\n",
        "    df_model_data = pd.DataFrame(data_for_model_raw)\n",
        "    print(\"Original DataFrame for modeling:\\n\", df_model_data)\n",
        "\n",
        "    # Define the target variable name\n",
        "    target_column_name = 'ExamScore'\n",
        "\n",
        "    if target_column_name in df_model_data.columns:\n",
        "        # X: Features (all columns EXCEPT the target variable)\n",
        "        X = df_model_data.drop(target_column_name, axis=1) # axis=1 indicates dropping a column\n",
        "\n",
        "        # y: Target variable (only the target column)\n",
        "        y = df_model_data[target_column_name]\n",
        "\n",
        "        print(\"\\nFeatures (X) - first 2 rows:\\n\", X.head(2))\n",
        "        # Output:\n",
        "        #    StudentID  HoursStudied  PreviousGrade Attendance\n",
        "        # 0          1             2             70       Good\n",
        "        # 1          2             5             85       Good\n",
        "\n",
        "        print(\"\\nTarget (y) - first 2 values:\\n\", y.head(2))\n",
        "        # Output:\n",
        "        # 0    65\n",
        "        # 1    88\n",
        "        # Name: ExamScore, dtype: int64\n",
        "    else:\n",
        "        print(f\"\\n'{target_column_name}' column not found in df_model_data.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "uMGv-2JdnQau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Crucial Note on Data Leakage:** It is fundamentally important that the target variable (`y`) is NOT included in the feature set (`X`) that the model learns from. Including it would mean the model is \"cheating\" by having access to the answer during training, leading to unrealistically good performance on training data but very poor performance on new, unseen data. The model's goal is to learn the relationship *between* `X` and `y` to predict `y` for new `X`.\n",
        "\n",
        "**6.2 Feature Scaling and Normalization**\n",
        "\n",
        "Many machine learning algorithms perform better, converge faster, or avoid numerical instability when numerical input features are on a similar scale. If features have vastly different ranges (e.g., one feature from 0-1, another from 0-100,000), some algorithms might be biased towards features with larger magnitudes.\n",
        "\n",
        "* **Why Scale/Normalize?**\n",
        "    * **Distance-based Algorithms:** Algorithms like K-Nearest Neighbors (KNN), Support Vector Machines (SVM), K-Means Clustering, and Principal Component Analysis (PCA) use distance metrics (e.g., Euclidean distance). Features with larger value ranges can disproportionately influence these metrics, leading to suboptimal results. *[72]*\n",
        "    * **Gradient Descent-based Algorithms:** Algorithms like Linear Regression, Logistic Regression, and Neural Networks use gradient descent (or its variants) for optimization. Feature scaling helps gradient descent converge faster and more smoothly by ensuring that the cost function's contours are more spherical, preventing oscillations. *[72]*\n",
        "    * **Numerical Stability:** Can prevent numerical overflow or underflow issues if feature values are extremely large or small. *[74]*\n",
        "    * **Equal Contribution (for some models):** Ensures that all features contribute more equally to the model training process, rather than models being dominated by features with larger magnitudes, especially in regularized models (like Ridge or Lasso regression) where penalties are applied to coefficients. *[73]*\n",
        "* **When is Scaling Less Critical?**\n",
        "    * **Tree-based Models:** Algorithms like Decision Trees, Random Forests, and Gradient Boosting Trees are generally not sensitive to the scale of features. They make splits based on individual feature thresholds and do not rely on distance metrics in the same way. *[72]* However, scaling won't hurt them.\n",
        "* **Important: Fit on Training Data Only!**\n",
        "    * Scalers (like `MinMaxScaler`, `StandardScaler`) learn parameters (min/max values for Min-Max, mean/std for Standard) from the data.\n",
        "    * These parameters **must be learned ONLY from the training dataset**.\n",
        "    * Then, the *same learned parameters* (the fitted scaler) must be used to transform the training set, the validation set, and the test set (and any future new data).\n",
        "    * Fitting the scaler on the entire dataset before splitting, or fitting it separately on the training and test sets, introduces **data leakage**. This means information from the test/validation set influences the training process, leading to overly optimistic performance estimates that won't generalize to truly unseen data. *[77, 74]*"
      ],
      "metadata": {
        "id": "yja-k6F9nQav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Illustrative data split (assuming X and y from the previous section)\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # For demonstration, let's select only numerical features from X for scaling\n",
        "    # 'StudentID' might be an identifier, not a feature for scaling in many cases.\n",
        "    # 'Attendance' is categorical and needs encoding first (covered later).\n",
        "    X_numerical_features = X[['HoursStudied', 'PreviousGrade']]\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_numerical_features, y, test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    print(\"X_train shape:\", X_train.shape)\n",
        "    print(\"X_test shape:\", X_test.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Tll-W_SLnQaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Normalization (Min-Max Scaling):**\n",
        "    * **Concept:** Rescales features to a fixed range, typically between 0 and 1 (or sometimes -1 to 1 if data contains negative values and is scaled appropriately).\n",
        "    * **Formula:** `X_normalized = (X - X_min) / (X_max - X_min)` *[75]*\n",
        "    * **Characteristics:**\n",
        "        * Preserves the shape of the original distribution (doesn't change relative relationships). *[76]*\n",
        "        * **Sensitive to outliers:** Extreme minimum or maximum values (outliers) can compress the majority of the data into a very small sub-range, as they will define `X_min` and `X_max`. *[73]*\n",
        "    * **Scikit-learn:** `MinMaxScaler` from `sklearn.preprocessing`. *[77]*\n",
        "        * `fit(X_train)`: Computes `X_min` and `X_max` from the training data.\n",
        "        * `transform(X_data)`: Applies the scaling transformation using the learned `X_min` and `X_max`.\n",
        "        * `fit_transform(X_train)`: Combines fitting and transforming in one step on the training data.\n",
        "        * `inverse_transform(X_scaled)`: Reverts scaled data back to its original scale. *[77]*"
      ],
      "metadata": {
        "id": "EYD8FMX0nQaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "\n",
        "    # 1. Fit the scaler on the TRAINING data ONLY\n",
        "    min_max_scaler.fit(X_train)\n",
        "\n",
        "    # 2. Transform both training and testing data using the FITTED scaler\n",
        "    X_train_minmax_scaled = min_max_scaler.transform(X_train)\n",
        "    X_test_minmax_scaled = min_max_scaler.transform(X_test) # Use the same scaler fitted on train data\n",
        "\n",
        "    # Convert back to DataFrame for easier viewing (optional)\n",
        "    X_train_minmax_scaled_df = pd.DataFrame(X_train_minmax_scaled, columns=X_train.columns, index=X_train.index)\n",
        "    X_test_minmax_scaled_df = pd.DataFrame(X_test_minmax_scaled, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "    print(\"\\nOriginal X_train (first 2 rows):\\n\", X_train.head(2))\n",
        "    print(\"\\nMin-Max Scaled X_train (first 2 rows):\\n\", X_train_minmax_scaled_df.head(2))\n",
        "    print(\"\\nMin-Max Scaled X_test (first row):\\n\", X_test_minmax_scaled_df.head(1))\n",
        "    print(f\"Min values learned by scaler: {min_max_scaler.data_min_}\")\n",
        "    print(f\"Max values learned by scaler: {min_max_scaler.data_max_}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "CzxEIFb6nQax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Standardization (Z-score Normalization):**\n",
        "    * **Concept:** Transforms data to have a mean of 0 and a standard deviation of 1. The resulting values are Z-scores, representing how many standard deviations an original value is from the mean.\n",
        "    * **Formula:** `X_standardized = (X - μ) / σ` (where `μ` is the mean and `σ` is the standard deviation of the feature). *[75]*\n",
        "    * **Characteristics:**\n",
        "        * Less sensitive to outliers compared to Min-Max scaling, although outliers still influence the calculation of `μ` and `σ`. *[73]*\n",
        "        * Does not bind values to a specific range (they can be positive or negative and extend beyond +/- 1).\n",
        "        * Often preferred when the algorithm assumes data is centered around zero and has a standard normal-like distribution (e.g., some forms of PCA, linear models with L1/L2 regularization often benefit). *[76]*\n",
        "    * **Scikit-learn:** `StandardScaler` from `sklearn.preprocessing`. *[75]*\n",
        "        * `fit(X_train)`: Computes mean (`μ`) and standard deviation (`σ`) from the training data.\n",
        "        * `transform(X_data)`: Applies the standardization.\n",
        "        * `fit_transform(X_train)`."
      ],
      "metadata": {
        "id": "n_TrPbIInQax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "    standard_scaler = StandardScaler()\n",
        "\n",
        "    # 1. Fit the scaler on the TRAINING data ONLY\n",
        "    standard_scaler.fit(X_train)\n",
        "\n",
        "    # 2. Transform both training and testing data\n",
        "    X_train_standard_scaled = standard_scaler.transform(X_train)\n",
        "    X_test_standard_scaled = standard_scaler.transform(X_test)\n",
        "\n",
        "    # Convert back to DataFrame (optional)\n",
        "    X_train_standard_scaled_df = pd.DataFrame(X_train_standard_scaled, columns=X_train.columns, index=X_train.index)\n",
        "    X_test_standard_scaled_df = pd.DataFrame(X_test_standard_scaled, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "    print(\"\\nOriginal X_train (first 2 rows):\\n\", X_train.head(2)) # Shown again for context\n",
        "    print(\"\\nStandardized X_train (first 2 rows):\\n\", X_train_standard_scaled_df.head(2))\n",
        "    print(\"\\nStandardized X_test (first row):\\n\", X_test_standard_scaled_df.head(1))\n",
        "    print(f\"Mean values learned by scaler: {standard_scaler.mean_}\")\n",
        "    print(f\"Scale (std dev) values learned by scaler: {standard_scaler.scale_}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "NX7A_qMpnQay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Robust Scaling:**\n",
        "    * **Concept:** Uses statistics that are robust to outliers, specifically the median and the Interquartile Range (IQR). It subtracts the median and divides by the IQR.\n",
        "    * **Formula (approximate):** `X_robust = (X - Median) / IQR` (where IQR = Q3 - Q1)\n",
        "    * **Characteristics:**\n",
        "        * Significantly less sensitive to outliers than Min-Max Scaling or Standardization. *[78]*\n",
        "        * Does not bind values to a specific range.\n",
        "        * Good choice when your dataset contains a notable number of outliers that you don't want to remove but want to reduce their influence on scaling.\n",
        "    * **Scikit-learn:** `RobustScaler` from `sklearn.preprocessing`. *[78]*"
      ],
      "metadata": {
        "id": "PJwBdhjtnQay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "    robust_scaler = RobustScaler()\n",
        "\n",
        "    # Fit and transform in one step for training data\n",
        "    X_train_robust_scaled = robust_scaler.fit_transform(X_train)\n",
        "    # Transform test data using the scaler fitted on train data\n",
        "    X_test_robust_scaled = robust_scaler.transform(X_test)\n",
        "\n",
        "    X_train_robust_scaled_df = pd.DataFrame(X_train_robust_scaled, columns=X_train.columns, index=X_train.index)\n",
        "\n",
        "    print(\"\\nRobust Scaled X_train (first 2 rows):\\n\", X_train_robust_scaled_df.head(2))\n",
        "    print(f\"Center (median) values learned by scaler: {robust_scaler.center_}\")\n",
        "    print(f\"Scale (IQR) values learned by scaler: {robust_scaler.scale_}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "lrvtSI8tnQay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Table: Comparison of Feature Scaling Techniques** *[72, 73, 75, 76, 77, 78, 79, 80]*\n",
        "    | Technique             | Formula (Conceptual)              | Output Range         | Sensitivity to Outliers | Common Use Cases                                                       | Scikit-learn Class |\n",
        "    | :-------------------- | :-------------------------------- | :------------------- | :---------------------- | :--------------------------------------------------------------------- | :----------------- |\n",
        "    | **Min-Max Scaling** | `(X - X_min) / (X_max - X_min)`   | Typically `[0, 1]`   | High                    | Algorithms requiring bounded input (e.g., some neural nets), image processing. When features have known bounds. | `MinMaxScaler`     |\n",
        "    | **Standardization** | `(X - Mean) / StdDev`             | No specific bounds   | Moderate                | PCA, linear/logistic regression with regularization, algorithms assuming Gaussian-like distribution. General purpose. | `StandardScaler`   |\n",
        "    | **Robust Scaling** | `(X - Median) / IQR`              | No specific bounds   | Low                     | Datasets with significant outliers where you want to mitigate their scaling impact. | `RobustScaler`     |\n",
        "\n",
        "    * `[Diagram: Three small distribution plots. Original data (skewed with an outlier). Then show how Min-Max scaling might compress most data. Then show how Standardization centers it. Then show how Robust Scaling handles the outlier better in terms of spread of non-outlier data.]`\n",
        "\n",
        "**6.3 Encoding Categorical Data**\n",
        "\n",
        "Machine learning algorithms typically require numerical input. Categorical data (text labels, categories) must be converted into a numerical format before being fed into most models. *[81]*\n",
        "\n",
        "* **One-Hot Encoding:**\n",
        "    * **Concept:** Transforms each categorical feature with `k` unique categories into `k` (or `k-1`) new binary (0 or 1) features, often called \"dummy variables.\" Each new column corresponds to one category. For a given observation, the column representing its original category will have a value of 1, and all other new dummy columns for that original feature will be 0. *[81]*\n",
        "    * **Purpose:**\n",
        "        * Avoids imposing an artificial ordinal relationship between categories (e.g., \"Red\" is not inherently \"greater\" or \"less\" than \"Blue\"). *[81]*\n",
        "        * Suitable for nominal categorical variables (where categories have no natural order).\n",
        "    * **Pandas: `pd.get_dummies()`** *[81]*\n",
        "        * `data`: The DataFrame or Series to encode.\n",
        "        * `columns`: List of column names to encode. If `None`, attempts to encode all columns with `object` or `category` dtype.\n",
        "        * `prefix`: String or list of strings to append to new column names (e.g., if original column is 'Color', `prefix='Color'` gives 'Color_Red', 'Color_Blue').\n",
        "        * `drop_first=True/False` (default `False`): If `True`, removes the first category's dummy column (creates `k-1` dummy variables instead of `k`). This is important for some models like linear regression to avoid multicollinearity (perfect correlation between predictors). *[81]*\n",
        "        * `dummy_na=True/False` (default `False`): If `True`, creates a separate dummy column for `NaN` values if they exist in the categorical column. If `False`, `NaN`s result in all zeros for the dummy variables of that feature. *[81]*"
      ],
      "metadata": {
        "id": "T0gEFfEmnQaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's use the 'Attendance' column from df_model_data (which is in X)\n",
        "    # Create a small DataFrame for clear demonstration\n",
        "    df_categorical_example = X[['Attendance']].copy() # X was created in 6.1\n",
        "    # Fill NaN for demonstration if any\n",
        "    # df_categorical_example['Attendance'].fillna('Unknown', inplace=True)\n",
        "    print(\"\\nOriginal categorical data for encoding:\\n\", df_categorical_example)\n",
        "\n",
        "    # One-hot encode 'Attendance' using pd.get_dummies()\n",
        "    df_one_hot_encoded = pd.get_dummies(df_categorical_example, columns=['Attendance'], prefix='Attend')\n",
        "    print(\"\\nDataFrame after one-hot encoding 'Attendance' (pd.get_dummies()):\\n\", df_one_hot_encoded)\n",
        "    # Output:\n",
        "    #    Attend_Good  Attend_Poor\n",
        "    # 0         True        False  (Assuming 'Good' was the first category if drop_first was used in a different scenario)\n",
        "    # 1         True        False\n",
        "    # ...\n",
        "    # Note: output column names depend on unique values in 'Attendance'\n",
        "\n",
        "    # One-hot encode with drop_first=True\n",
        "    df_one_hot_encoded_drop_first = pd.get_dummies(df_categorical_example, columns=['Attendance'], prefix='Attend', drop_first=True)\n",
        "    print(\"\\nDataFrame after one-hot encoding (drop_first=True):\\n\", df_one_hot_encoded_drop_first)\n",
        "    # Output: If categories are 'Good', 'Poor', one of them (e.g., 'Attend_Good') will be dropped.\n",
        "    # The dropped category is represented when all other dummy variables for that feature are 0.\n",
        "\n",
        "    # Handling NaNs with dummy_na=True\n",
        "    df_categorical_example_with_nan = pd.DataFrame({'Color': ['Red', 'Blue', np.nan, 'Green', 'Red']})\n",
        "    df_one_hot_nan = pd.get_dummies(df_categorical_example_with_nan, columns=['Color'], prefix='C', dummy_na=True)\n",
        "    print(\"\\nOne-hot encoding with dummy_na=True:\\n\", df_one_hot_nan)\n",
        "    # Output will have C_Red, C_Blue, C_Green, AND C_nan columns"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "L_CRMfUVnQaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Scikit-learn: `OneHotEncoder` from `sklearn.preprocessing`** *[83]*\n",
        "        * Generally preferred within Scikit-learn pipelines, especially for consistent handling of training and test sets (e.g., ensuring same columns are created, handling unseen categories in test data using `handle_unknown='ignore'`).\n",
        "        * `fit(X_train_cat)`: Learns the categories from the training data.\n",
        "        * `transform(X_data_cat)`: Applies the encoding.\n",
        "        * `sparse_output=False` (default in newer versions, was `sparse=True`): Returns a dense NumPy array. If `True`, returns a sparse matrix (memory efficient for high cardinality).\n",
        "        * `handle_unknown='ignore'`: If a new category appears in the test set (that wasn't in training), it will result in all zeros for the encoded columns of that feature. If `'error'` (default), it will raise an error."
      ],
      "metadata": {
        "id": "vy1__bBOnQaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "    # Let's use X_train and X_test (assuming they exist and contain categorical columns)\n",
        "    # For this example, create dummy train/test sets with a categorical feature\n",
        "    X_train_cat_demo = pd.DataFrame({'CategoryFeature': ['A', 'B', 'A', 'C', 'B']})\n",
        "    X_test_cat_demo = pd.DataFrame({'CategoryFeature': ['B', 'A', 'D', 'C']}) # 'D' is unseen\n",
        "\n",
        "    one_hot_encoder_sklearn = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "\n",
        "    # Fit on training data\n",
        "    one_hot_encoder_sklearn.fit(X_train_cat_demo[['CategoryFeature']]) # Needs 2D array-like\n",
        "\n",
        "    # Transform training and test data\n",
        "    X_train_cat_encoded_sklearn = one_hot_encoder_sklearn.transform(X_train_cat_demo[['CategoryFeature']])\n",
        "    X_test_cat_encoded_sklearn = one_hot_encoder_sklearn.transform(X_test_cat_demo[['CategoryFeature']])\n",
        "\n",
        "    # Get feature names for the new columns\n",
        "    encoded_feature_names = one_hot_encoder_sklearn.get_feature_names_out(['CategoryFeature'])\n",
        "    print(\"\\nSklearn OneHotEncoder categories learned:\", one_hot_encoder_sklearn.categories_)\n",
        "    print(\"Sklearn OneHotEncoded training data (column names: \", encoded_feature_names, \"):\\n\", X_train_cat_encoded_sklearn)\n",
        "    print(\"Sklearn OneHotEncoded test data (with unseen 'D'):\\n\", X_test_cat_encoded_sklearn)\n",
        "    # Unseen 'D' in test data results in [0., 0., 0.] for that row for CategoryFeature_A, CategoryFeature_B, CategoryFeature_C."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ESNgLkOEnQaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Challenge with One-Hot Encoding:** Can significantly increase the number of features (dimensionality) if a categorical variable has many unique values (high cardinality). This can sometimes lead to the \"curse of dimensionality,\" making models harder to train, more prone to overfitting, and computationally more expensive.\n",
        "\n",
        "* **Label Encoding:**\n",
        "    * **Concept:** Assigns a unique integer to each category (e.g., \"Red\" -> 0, \"Blue\" -> 1, \"Green\" -> 2).\n",
        "    * **Scikit-learn:** `LabelEncoder` from `sklearn.preprocessing`.\n",
        "        * `fit(y_cat_train)`: Learns the mapping from categories to integers.\n",
        "        * `transform(y_cat_data)`: Applies the learned mapping.\n",
        "        * `fit_transform()`\n",
        "        * `inverse_transform()`: Converts integers back to original labels.\n",
        "    * **Appropriateness & Caution:** *[84]*\n",
        "        * **Suitable for:**\n",
        "            * **Ordinal categorical variables:** Where the categories have a meaningful, inherent order (e.g., \"Low\" < \"Medium\" < \"High\"; \"Small\" < \"Medium\" < \"Large\").\n",
        "            * **Target variable in classification:** Many Scikit-learn classifiers expect the target variable `y` to be label encoded.\n",
        "        * **Caution for Nominal Features in `X`:** If used with nominal categorical features (no inherent order, like \"Color\": \"Red\", \"Blue\", \"Green\") as input features `X` for certain models, the model might incorrectly interpret the encoded integers as having an ordinal relationship or magnitude (e.g., assume Green (2) is \"greater than\" Blue (1)). This is generally undesirable for linear models, distance-based algorithms (KNN, SVM), and neural networks.\n",
        "        * **Tree-based models** (Decision Trees, Random Forests) can often handle label-encoded nominal features correctly because they make splits based on thresholds (e.g., \"is feature_value <= 1?\") and don't assume a continuous magnitude between encoded values."
      ],
      "metadata": {
        "id": "V2REcl_tnQaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "    # Example ordinal data\n",
        "    ordinal_data_series = pd.Series(['Low', 'Medium', 'High', 'Low', 'Medium', 'Very High', 'Medium'])\n",
        "    print(\"\\nOriginal ordinal data:\\n\", ordinal_data_series)\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "\n",
        "    # Fit and transform\n",
        "    ordinal_encoded = label_encoder.fit_transform(ordinal_data_series)\n",
        "    print(\"Label encoded ordinal data:\\n\", ordinal_encoded) # e.g., High=0, Low=1, Medium=2, Very High=3 (order depends on first seen)\n",
        "    print(\"Label encoder classes (mapping learned):\\n\", label_encoder.classes_) # Shows the actual mapping\n",
        "\n",
        "    # Example: Applying to a nominal feature (use with caution for X)\n",
        "    nominal_data_series = X_train_cat_demo['CategoryFeature'].copy() # from OneHotEncoder example\n",
        "    nominal_encoded_caution = label_encoder.fit_transform(nominal_data_series)\n",
        "    print(\"\\nLabel encoded nominal data (use with caution for features X):\\n\", nominal_encoded_caution)\n",
        "    print(\"Mapping for nominal data:\", list(zip(label_encoder.classes_, range(len(label_encoder.classes_)))))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "kckWLa2lnQa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Other Encoding Techniques (for high cardinality features):** Target Encoding, Count Encoding, Embedding Layers (for neural networks). These are more advanced.\n",
        "\n",
        "**6.4 Binning (Discretization) Continuous Variables**\n",
        "\n",
        "Binning (or discretization) is the process of converting continuous numerical variables into discrete categorical variables by grouping values into a set of intervals or \"bins.\" *[85]*\n",
        "\n",
        "* **Purpose:**\n",
        "    * **Simplify Data & Reduce Noise:** Can reduce the impact of minor observation errors or fluctuations. *[86]*\n",
        "    * **Handle Non-linear Relationships:** Can help linear models capture non-linear relationships by transforming continuous features. After binning, one-hot encoding can be applied to the binned categories. *[86]*\n",
        "    * **Convert to Categorical Format:** Makes continuous data suitable for algorithms that primarily require categorical input or perform better with it.\n",
        "    * **Improve Interpretability:** \"Age Group 20-30\" can be more interpretable than a raw age of \"27\".\n",
        "\n",
        "* **Pandas `pd.cut(x, bins, labels=None, right=True, include_lowest=False, retbins=False)`:** *[85]*\n",
        "    * Segments and sorts data values into discrete bins based on specified bin edges.\n",
        "    * `x`: The input array or Series to be binned.\n",
        "    * `bins`:\n",
        "        * **Integer:** Defines the number of equal-width bins to create over the range of `x`. Pandas calculates the bin edges to be evenly spaced.\n",
        "        * **Sequence of scalars (list/array):** Defines the explicit bin edges. E.g., `[0, 18, 35, 60, 100]` creates bins (0,18], (18,35], (35,60], (60,100]. *[85]*\n",
        "    * `labels`: Array or `False`. Specifies the labels for the returned bins.\n",
        "        * If `None` (default), integer indicators of the bins are returned if `retbins=False`, or Interval objects.\n",
        "        * If `False`, returns only integer indicators of the bins (0-indexed).\n",
        "        * If an array/list of strings, its length must match the number of bins created (i.e., `len(bin_edges) - 1`). E.g., `['Child', 'YoungAdult', 'Adult', 'Senior']`.\n",
        "    * `right=True` (default): Indicates whether bins include the rightmost edge or not.\n",
        "        * `True`: Bins are `(edge1, edge2]`, meaning edge1 < x <= edge2.\n",
        "        * `False`: Bins are `[edge1, edge2)`, meaning edge1 <= x < edge2.\n",
        "    * `include_lowest=False` (default): Whether the first interval should be left-inclusive or not. If `bins` is a sequence, `include_lowest=True` makes the first bin inclusive of its left edge.\n",
        "    * `retbins=False` (default): Whether to return the bins or not. If `True`, returns a tuple of `(binned_data, bins_array)`."
      ],
      "metadata": {
        "id": "ogigJ3PpnQa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ages_series = pd.Series([5, 15, 22, 35, 45, 58, 62, 75, 80, 25, 10, 60])\n",
        "    print(\"\\nOriginal Ages Series:\\n\", ages_series)\n",
        "\n",
        "    # Equal-width binning (Pandas determines bin width)\n",
        "    age_bins_equal_width = pd.cut(ages_series, bins=4) # Divide into 4 bins of equal width based on min/max age\n",
        "    print(\"\\nAges binned into 4 equal-width intervals (default labels are Interval objects):\\n\", age_bins_equal_width)\n",
        "    print(\"Counts per bin (equal width):\\n\", age_bins_equal_width.value_counts().sort_index())\n",
        "\n",
        "    # Custom bin edges and labels\n",
        "    custom_age_edges = [0, 18, 35, 60, 100] # Defines 4 bins: (0,18], (18,35], (35,60], (60,100]\n",
        "    custom_age_labels = ['Child/Teen', 'Young Adult', 'Adult', 'Senior']\n",
        "    age_bins_custom = pd.cut(ages_series,\n",
        "                             bins=custom_age_edges,\n",
        "                             labels=custom_age_labels,\n",
        "                             right=True, # Default, (edge1, edge2]\n",
        "                             include_lowest=True) # Makes the first bin [0, 18] effectively\n",
        "    print(\"\\nAges binned with custom edges and labels:\\n\", age_bins_custom)\n",
        "    print(\"Counts per custom bin:\\n\", age_bins_custom.value_counts().sort_index())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "r6KCCO4tnQa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Pandas `pd.qcut(x, q, labels=None, retbins=False, duplicates='raise')`**:\n",
        "    * Discretizes variable into equal-sized buckets based on rank or sample quantiles (e.g., quartiles, deciles). Each bin will have approximately the same number of observations. *[86]*\n",
        "    * `x`: Input array or Series.\n",
        "    * `q`:\n",
        "        * Integer: Number of quantiles (e.g., `4` for quartiles, `10` for deciles).\n",
        "        * List of quantiles: E.g., `[0, 0.25, 0.5, 0.75, 1.0]` for quartiles (defines bin edges by quantiles).\n",
        "    * `labels`: Similar to `pd.cut()`.\n",
        "    * `duplicates='raise'` (default) or `'drop'`: How to handle duplicate edges that can arise if data is not continuous or has many identical values. If bin edges are not unique, `'raise'` will cause an error. `'drop'` will use unique bin edges, potentially resulting in fewer bins than specified by `q`."
      ],
      "metadata": {
        "id": "plHRnt2qnQa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "income_series = pd.Series([20000, 25000, 22000, 30000, 70000, 85000, 90000, 35000, 40000, 120000, 20000, 50000])\n",
        "    print(\"\\nOriginal Income Series:\\n\", income_series)\n",
        "\n",
        "    # Bin income into quartiles (4 groups with roughly equal number of observations)\n",
        "    income_quartiles = pd.qcut(income_series, q=4)\n",
        "    print(\"\\nIncome binned into quartiles (qcut - default Interval labels):\\n\", income_quartiles)\n",
        "    print(\"Counts per income quantile (should be roughly equal):\\n\", income_quartiles.value_counts().sort_index())\n",
        "\n",
        "    # qcut with custom labels\n",
        "    quantile_labels = ['Q1 (Lowest)', 'Q2', 'Q3', 'Q4 (Highest)']\n",
        "    income_quantiles_labeled = pd.qcut(income_series, q=4, labels=quantile_labels)\n",
        "    print(\"\\nIncome binned into quartiles with custom labels:\\n\", income_quantiles_labeled)\n",
        "    print(\"Counts per labeled income quantile:\\n\", income_quantiles_labeled.value_counts().sort_index())\n",
        "\n",
        "    # Handling duplicates in qcut if data has many identical values\n",
        "    data_with_duplicates_for_qcut = pd.Series([1, 1, 1, 1, 5, 5, 5, 10, 10, 20])\n",
        "    income_qcut_drop_duplicates = pd.qcut(data_with_duplicates_for_qcut, q=4, labels=False, duplicates='drop')\n",
        "    print(\"\\nQcut with duplicates='drop':\\n\", income_qcut_drop_duplicates)\n",
        "    print(\"Counts per bin (duplicates='drop'):\\n\", income_qcut_drop_duplicates.value_counts().sort_index())\n",
        "    # May result in fewer than q bins if many duplicate values exist at quantile boundaries."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "fNhKgOHynQa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **`cut` vs. `qcut`**:\n",
        "        * `cut`: Bins are of equal width (range of values in each bin is the same). Number of observations per bin can vary greatly. Sensitive to outliers creating sparse bins.\n",
        "        * `qcut`: Bins have roughly the same number of observations. Bin widths can vary greatly, especially for skewed data.\n",
        "    * `[Diagram: Two histograms. One showing data binned by `pd.cut` (equal width bins, unequal heights). Another showing the same data binned by `pd.qcut` (unequal width bins, roughly equal heights/frequencies).]`\n",
        "\n",
        "* **Using NumPy for Bin Edges:**\n",
        "    * `np.linspace(start, stop, num)` can generate evenly spaced numbers over a specified interval, which can then be used as `bins` in `pd.cut()`. *[87]*"
      ],
      "metadata": {
        "id": "VM7d0etcnQa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_for_linspace = pd.Series(np.random.rand(100) * 100) # Values from approx 0 to 100\n",
        "    num_bins_linspace = 5\n",
        "\n",
        "    # Create 5 equal-width bins using linspace for bin edges\n",
        "    # num_bins + 1 because linspace needs number of points, and N points define N-1 intervals\n",
        "    bin_edges_np_linspace = np.linspace(start=data_for_linspace.min(),\n",
        "                                        stop=data_for_linspace.max(),\n",
        "                                        num=num_bins_linspace + 1)\n",
        "\n",
        "    print(f\"\\nBin edges generated by np.linspace for {num_bins_linspace} bins:\\n\", bin_edges_np_linspace)\n",
        "    binned_data_linspace = pd.cut(data_for_linspace,\n",
        "                                  bins=bin_edges_np_linspace,\n",
        "                                  include_lowest=True, # Ensure the min value is included\n",
        "                                  right=True,\n",
        "                                  labels=False) # Get integer bin identifiers\n",
        "    print(f\"Value counts for {num_bins_linspace} bins created with linspace edges:\\n\",\n",
        "          pd.Series(binned_data_linspace).value_counts().sort_index())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "hq7JLwAunQa1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binning can be visualized using histograms, where the bars represent the bins and their heights represent the frequency of data points. *[55]*\n",
        "\n",
        "---\n",
        "\n",
        "**Module 6: Practice Questions**\n",
        "\n",
        "96.  **Terminology:** In machine learning, what is another common name for \"features\"? What about for the \"target variable\"?\n",
        "97.  **Coding:** You have a DataFrame `df_housing` with columns `['SquareFeet', 'NumBedrooms', 'Garden (Yes/No)', 'SalePrice']`. You want to predict 'SalePrice'. Write Python code to separate `df_housing` into a features DataFrame `X_house` and a target Series `y_house`.\n",
        "98.  **Concept:** Why is feature scaling important for algorithms like KNN or SVM?\n",
        "99.  **Data Leakage:** Explain in your own words what data leakage is in the context of feature scaling and why it's crucial to fit scalers *only* on the training data.\n",
        "100. **MCQ:** Which feature scaling technique transforms data to have a mean of 0 and a standard deviation of 1?\n",
        "     A) Min-Max Scaling\n",
        "     B) Robust Scaling\n",
        "     C) Standardization\n",
        "     D) Log Transformation\n",
        "101. **MCQ:** Which feature scaling technique is generally most robust to outliers?\n",
        "     A) Min-Max Scaling\n",
        "     B) Standardization\n",
        "     C) Robust Scaling\n",
        "     D) Normalization (L2 norm)\n",
        "102. **Coding:** Assume you have `X_train_num` and `X_test_num` DataFrames containing numerical features. Write the Python code to apply Min-Max scaling, ensuring the scaler is fit only on `X_train_num`.\n",
        "103. **Concept:** Why is it necessary to encode categorical data for most machine learning algorithms?\n",
        "104. **Encoding:** What is the main difference between One-Hot Encoding and Label Encoding in terms of how they represent categories?\n",
        "105. **`pd.get_dummies()`:** What does the `drop_first=True` parameter do in `pd.get_dummies()` and why might it be useful?\n",
        "106. **`OneHotEncoder`:** When using Scikit-learn's `OneHotEncoder`, what does the `handle_unknown='ignore'` parameter achieve when transforming test data?\n",
        "107. **Label Encoding Application:** For which type of categorical variable is Label Encoding most appropriate? Give an example.\n",
        "108. **Binning:** What is the primary purpose of binning (discretization) continuous variables? List two reasons.\n",
        "109. **`pd.cut` vs. `pd.qcut`:** Explain the main difference in how `pd.cut()` and `pd.qcut()` create bins.\n",
        "110. **Coding:** You have a Pandas Series `temperatures = pd.Series([12, 15, 22, 28, 33, 18, 25])`. Write code to bin these temperatures into 3 equal-width bins using `pd.cut()`.\n",
        "111. **Coding:** Using the same `temperatures` Series, bin it into 3 bins with roughly equal numbers of observations using `pd.qcut()`, and label them 'Cold', 'Mild', 'Warm'.\n",
        "112. **Critical Thinking:** You have a feature \"Income\" which is heavily right-skewed with some very high earners (outliers). If you need to scale this feature for a distance-based algorithm, would `MinMaxScaler` or `RobustScaler` likely be a better initial choice? Why?\n",
        "113. **Feature Engineering:** After one-hot encoding a categorical feature 'City' which has 50 unique city names, how many new columns would be added to your DataFrame (assuming `drop_first=False`)? What potential issue might this lead to?\n",
        "114. **Scaling Impact:** Would you expect feature scaling to have a significant impact on the performance of a Random Forest model? Why or why not?\n",
        "115. **`inverse_transform`:** What is the purpose of the `inverse_transform` method available in Scikit-learn scalers like `MinMaxScaler` or `StandardScaler`?\n",
        "\n",
        "---\n",
        "*(Continued in next response due to length limitations)*"
      ],
      "metadata": {
        "id": "GaWI-eZznQa1"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}