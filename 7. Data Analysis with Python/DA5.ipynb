{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Okay, here is the final part of your comprehensive notes, covering Module 8.\n",
        "Since the provided transcript for Module 8 was incomplete, this section has been completed with standard information on Model Evaluation and Validation for regression, which typically follows the topics covered in Module 7.\n",
        "\n",
        "---\n",
        "\n",
        "**Module 8: Model Evaluation and Validation**\n",
        "\n",
        "Once you've trained a regression model, it's crucial to evaluate its performance on unseen data. This helps you understand how well your model generalizes to new, independent data and allows you to compare different models or different versions of the same model (e.g., with different hyperparameters). Relying solely on performance on the training data can be misleading due to overfitting.\n",
        "\n",
        "**8.1 The Importance of Splitting Data: Training and Testing Sets**\n",
        "\n",
        "As covered in Module 6 (Feature Scaling), the first step before any model training or evaluation is to split your dataset into (at least) two parts:\n",
        "\n",
        "* **Training Set:** Used to train the model (i.e., to learn the model parameters/coefficients).\n",
        "* **Testing Set (or Hold-out Set):** Used to evaluate the performance of the *trained* model on unseen data. This gives an unbiased estimate of how the model is likely to perform on new, real-world data."
      ],
      "metadata": {
        "id": "jj3Z0sQDoWAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Assuming X_data and y_data are your full feature set and target variable\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
        "# test_size=0.2 means 20% of data for testing, 80% for training.\n",
        "# random_state ensures reproducibility of the split."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "2SYjzyMvoWAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Never evaluate your model on the data it was trained on to estimate its future performance.** This would likely give an overly optimistic view.\n",
        "\n",
        "**8.2 Common Regression Metrics**\n",
        "\n",
        "Several metrics are used to evaluate the performance of regression models. These metrics quantify the difference between the actual (true) values and the values predicted by the model.\n",
        "\n",
        "Let:\n",
        "* `$n$` = number of observations\n",
        "* `$y_i$` = actual value for the *i*-th observation\n",
        "* `$\\hat{y}_i$` = predicted value for the *i*-th observation\n",
        "* `$\\bar{y}$` = mean of the actual values\n",
        "\n",
        "* **Mean Absolute Error (MAE):**\n",
        "    * **Definition:** The average of the absolute differences between predicted and actual values.\n",
        "    * **Formula:** $$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$\n",
        "    * **Interpretation:** Measures the average magnitude of errors in a set of predictions, without considering their direction. It's in the same units as the target variable.\n",
        "    * **Pros:** Robust to outliers compared to MSE. Easy to understand.\n",
        "    * **Cons:** Doesn't penalize large errors as heavily as MSE.\n",
        "    * **Scikit-learn:** `sklearn.metrics.mean_absolute_error`"
      ],
      "metadata": {
        "id": "dtcQjWEMoWAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "        # Assuming y_test (actual values) and y_pred (model's predictions on X_test) are available\n",
        "        # mae = mean_absolute_error(y_test, y_pred)\n",
        "        # print(f\"Mean Absolute Error (MAE): {mae:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "SjlawMHuoWAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Mean Squared Error (MSE):**\n",
        "    * **Definition:** The average of the squared differences between predicted and actual values.\n",
        "    * **Formula:** $$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
        "    * **Interpretation:** Also measures the average magnitude of errors. Squaring the errors penalizes larger errors more heavily than smaller ones. The units are the square of the target variable's units (e.g., if `y` is in dollars, MSE is in dollars-squared).\n",
        "    * **Pros:** Penalizes large errors significantly. Mathematically convenient (differentiable).\n",
        "    * **Cons:** Less intuitive due to squared units. More sensitive to outliers than MAE.\n",
        "    * **Scikit-learn:** `sklearn.metrics.mean_squared_error`"
      ],
      "metadata": {
        "id": "i6W1BIH-oWAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "        # mse = mean_squared_error(y_test, y_pred)\n",
        "        # print(f\"Mean Squared Error (MSE): {mse:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "qUXIMMvNoWAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Root Mean Squared Error (RMSE):**\n",
        "    * **Definition:** The square root of the Mean Squared Error.\n",
        "    * **Formula:** $$RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n",
        "    * **Interpretation:** Similar to MSE in that it penalizes large errors more, but it's in the same units as the target variable, making it more interpretable than MSE.\n",
        "    * **Pros:** In the same units as `y`. Penalizes large errors.\n",
        "    * **Cons:** Still sensitive to outliers (due to the underlying MSE).\n",
        "    * **Scikit-learn:** No direct function, but easily calculated from MSE."
      ],
      "metadata": {
        "id": "kw5lJ9ZDoWAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "        # rmse = np.sqrt(mse) # mse calculated as above\n",
        "        # Or: rmse = mean_squared_error(y_test, y_pred, squared=False) # Newer sklearn versions\n",
        "        # print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "FPx8SO8SoWAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **R-squared (Coefficient of Determination, R²):**\n",
        "    * **Definition:** Represents the proportion of the variance in the dependent variable (`y`) that is predictable from the independent variables (`X`).\n",
        "    * **Formula:** $$R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} = 1 - \\frac{RSS}{TSS}$$\n",
        "        Where RSS (Residual Sum of Squares) is `$\\sum (y_i - \\hat{y}_i)^2$` and TSS (Total Sum of Squares) is `$\\sum (y_i - \\bar{y})^2$`.\n",
        "    * **Interpretation:**\n",
        "        * Ranges from `-$\\infty$` to 1 (though typically 0 to 1 for decent models).\n",
        "        * `$R^2 = 1$`: Perfect fit, the model explains all the variability in `y`.\n",
        "        * `$R^2 = 0$`: The model explains none of the variability (performs no better than a model predicting the mean of `y`).\n",
        "        * `$R^2 < 0$`: The model performs worse than predicting the mean (a very poor fit).\n",
        "        * An R² of 0.75 means that 75% of the variance in the target variable can be explained by the features in the model.\n",
        "    * **Pros:** Provides a relative measure of goodness-of-fit. Widely used.\n",
        "    * **Cons:** R² always increases or stays the same when more predictors are added to the model, even if those predictors are not actually useful. This can be misleading.\n",
        "    * **Scikit-learn:** `sklearn.metrics.r2_score` or `model.score(X_test, y_test)` for many regressors."
      ],
      "metadata": {
        "id": "VXsn7tO-oWAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "        # r2 = r2_score(y_test, y_pred)\n",
        "        # Or for a fitted model: r2_on_test = your_model.score(X_test, y_test)\n",
        "        # print(f\"R-squared (R²): {r2:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "a2uy16ywoWAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Adjusted R-squared:**\n",
        "    * **Definition:** A modified version of R² that adjusts for the number of predictors in the model. It only increases if the new predictor improves the model more than would be expected by chance.\n",
        "    * **Formula:** $$\\text{Adjusted } R^2 = 1 - \\frac{(1-R^2)(n-1)}{n-p-1}$$\n",
        "        Where `p` is the number of predictors (features).\n",
        "    * **Interpretation:** More suitable for comparing models with different numbers of features. Adjusted R² can decrease if a new predictor doesn't add sufficient explanatory power.\n",
        "    * **Scikit-learn:** No direct function, but can be calculated using the R² value, `n` (number of samples), and `p` (number of features).\n",
        "\n",
        "**8.3 Visual Evaluation Methods**\n",
        "\n",
        "Numerical metrics are essential, but visual inspection of model performance can provide deeper insights.\n",
        "\n",
        "* **Actual vs. Predicted Plot:**\n",
        "    * **Concept:** A scatter plot with actual target values (`$y_i$`) on one axis (e.g., x-axis) and predicted target values (`$\\hat{y}_i$`) on the other axis (e.g., y-axis).\n",
        "    * **Interpretation:**\n",
        "        * If the model is perfect, all points would lie on the 45-degree diagonal line (where actual = predicted).\n",
        "        * The closer the points are to this diagonal, the better the model's predictions.\n",
        "        * Systematic deviations from the diagonal can indicate issues (e.g., model consistently under-predicting or over-predicting in certain ranges)."
      ],
      "metadata": {
        "id": "mNhz7oxSoWAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(8, 8))\n",
        "    # plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "    # plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2) # Diagonal line\n",
        "    # plt.xlabel(\"Actual Values\")\n",
        "    # plt.ylabel(\"Predicted Values\")\n",
        "    # plt.title(\"Actual vs. Predicted Values\")\n",
        "    # plt.axis('equal') # Ensure scales are equal for a true 45-degree line\n",
        "    # plt.show()\n",
        "    # [Image: actual_vs_predicted_plot.png - A scatter plot with actual values on x-axis and predicted values on y-axis. A dashed diagonal line (y=x) is shown. Points should ideally cluster around this line.]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "sOopRkVMoWAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Residual Plot:**\n",
        "    * **Concept:** A scatter plot of the residuals (`$e_i = y_i - \\hat{y}_i$`) on the y-axis against the predicted values (`$\\hat{y}_i$`) or against an independent variable (`$x_i$`) on the x-axis.\n",
        "    * **Interpretation (What to look for in a good residual plot):**\n",
        "        1.  **Random Scatter:** Residuals should be randomly scattered around the horizontal line at zero.\n",
        "        2.  **No Clear Patterns:** There should be no discernible patterns (e.g., curves, funnels/cones). A pattern suggests the model is missing some systematic information or that the relationship is not purely linear (if using a linear model).\n",
        "        3.  **Constant Variance (Homoscedasticity):** The spread (variance) of residuals should be roughly constant across all levels of predicted values. If the spread increases or decreases with `$\\hat{y}$` (forming a cone shape), this is called heteroscedasticity, which violates an assumption of OLS regression and can affect the reliability of coefficient estimates and standard errors."
      ],
      "metadata": {
        "id": "bxHaWEhfoWAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# residuals = y_test - y_pred\n",
        "    # plt.figure(figsize=(10, 6))\n",
        "    # plt.scatter(y_pred, residuals, alpha=0.5)\n",
        "    # plt.hlines(0, xmin=y_pred.min(), xmax=y_pred.max(), colors='red', linestyles='--')\n",
        "    # plt.xlabel(\"Predicted Values\")\n",
        "    # plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
        "    # plt.title(\"Residual Plot\")\n",
        "    # plt.show()\n",
        "    # [Image: good_residual_plot.png - A scatter plot with predicted values on x-axis and residuals on y-axis. Points are randomly scattered around the y=0 line with no obvious pattern and roughly constant variance.]\n",
        "    # [Image: bad_residual_plot_pattern.png - A residual plot showing a clear curve or U-shape, indicating non-linearity not captured.]\n",
        "    # [Image: bad_residual_plot_heteroscedasticity.png - A residual plot showing a funnel shape (variance of residuals increases with predicted values), indicating heteroscedasticity.]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "b_Nc1pxJoWA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.4 Cross-Validation**\n",
        "\n",
        "Splitting data into a single training set and a single test set is good, but the model's performance on that one test set can still be subject to some randomness depending on which particular observations ended up in the test set. Cross-validation (CV) provides a more robust and reliable estimate of model performance.\n",
        "\n",
        "* **Concept:** A resampling procedure used to evaluate machine learning models on a limited data sample. The idea is to repeatedly split the *training data* into smaller training and validation subsets, train the model on the training subset, and validate it on the validation subset. The results from these folds are then averaged.\n",
        "* **Purpose:**\n",
        "    * **More Robust Performance Estimate:** Reduces the variance associated with a single train-test split.\n",
        "    * **Better Use of Data:** More of the data gets to be used for both training and validation across different folds.\n",
        "    * **Helps Detect Overfitting:** If performance is high on training folds but low on validation folds, it suggests overfitting.\n",
        "* **K-Fold Cross-Validation:**\n",
        "    1.  The original *training dataset* is randomly partitioned into `k` equal-sized (or nearly equal-sized) \"folds\".\n",
        "    2.  For each fold `i` (from 1 to `k`):\n",
        "        * Fold `i` is used as the **validation set**.\n",
        "        * The remaining `k-1` folds are combined to form the **training set**.\n",
        "        * The model is trained on this training set and evaluated on the validation set (fold `i`).\n",
        "    3.  The performance metric (e.g., MSE, R²) is calculated for each fold.\n",
        "    4.  The `k` performance scores are then averaged (and standard deviation can be noted) to get an overall performance estimate.\n",
        "    * Common values for `k` are 5 or 10.\n",
        "    * `[Diagram: Visual representation of K-Fold Cross-Validation (e.g., for K=5). A bar representing the dataset is shown divided into 5 folds. Iteration 1: Fold 1 is validation, Folds 2-5 are training. Iteration 2: Fold 2 is validation, Folds 1,3-5 are training, and so on.]`\n",
        "\n",
        "* **Scikit-learn for Cross-Validation:**\n",
        "    * `sklearn.model_selection.cross_val_score`: A convenient function to get scores from multiple CV folds.\n",
        "    * `sklearn.model_selection.cross_validate`: More flexible, can return multiple metrics and training scores.\n",
        "    * Various CV iterators are available (e.g., `KFold`, `StratifiedKFold` for classification)."
      ],
      "metadata": {
        "id": "v5VuTPAKoWA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "    # Assuming 'your_model' is an instantiated Scikit-learn model (e.g., LinearRegression(), Ridge())\n",
        "    # and 'X_scaled_data', 'y_data' are your FULL preprocessed feature set and target (before the initial train-test split for final hold-out testing).\n",
        "    # Cross-validation is typically done on what would otherwise be your main training set.\n",
        "\n",
        "    # Example using the previously defined mlr_model and X_mlr_features_df, y_mlr_target\n",
        "    # It's generally better to perform CV on the data *before* the final hold-out split,\n",
        "    # or on the X_train, y_train portion after the initial split.\n",
        "    # For this example, let's use the full dataset X_mlr_features_df for simplicity in demonstrating CV.\n",
        "    # In a real project, you'd do CV on X_train, y_train.\n",
        "\n",
        "    # kf = KFold(n_splits=5, shuffle=True, random_state=42) # Define the K-Fold strategy\n",
        "\n",
        "    # Get scores for a chosen metric (e.g., R-squared, which is default for .score method of many regressors)\n",
        "    # If using a metric that needs to be explicitly stated (like MSE), use the 'scoring' parameter:\n",
        "    # e.g., scoring='neg_mean_squared_error' (Scikit-learn uses negative MSE as scores should be maximized)\n",
        "    # cv_r2_scores = cross_val_score(mlr_model, X_mlr_features_df, y_mlr_target, cv=kf, scoring='r2')\n",
        "    # cv_mse_scores = cross_val_score(mlr_model, X_mlr_features_df, y_mlr_target, cv=kf, scoring='neg_mean_squared_error')\n",
        "\n",
        "    # print(f\"\\nCross-Validation R2 Scores for each fold: {cv_r2_scores}\")\n",
        "    # print(f\"Average Cross-Validation R2 Score: {cv_r2_scores.mean():.4f}\")\n",
        "    # print(f\"Standard Deviation of CV R2 Scores: {cv_r2_scores.std():.4f}\")\n",
        "\n",
        "    # print(f\"\\nCross-Validation Negative MSE Scores: {cv_mse_scores}\")\n",
        "    # print(f\"Average Cross-Validation MSE: {-cv_mse_scores.mean():.4f}\") # Invert sign for actual MSE"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "8QO8XjVToWA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note on `cross_val_score` and Preprocessing:** If your model involves preprocessing steps like scaling, it's crucial to include these steps *within each fold* of the cross-validation correctly. This is best handled using Scikit-learn `Pipeline` objects. If you scale the entire dataset *before* cross-validation, you are leaking information from the validation folds into the training process of other folds.\n",
        "\n",
        "**8.5 Hyperparameter Tuning (Brief Introduction)**\n",
        "\n",
        "* **What are Hyperparameters?**\n",
        "    * Parameters of a learning algorithm that are set *before* the learning process begins. They are not learned from the data directly (unlike model parameters like coefficients in linear regression, which *are* learned during `fit()`).\n",
        "    * Examples:\n",
        "        * `alpha` in Ridge or Lasso Regression.\n",
        "        * `degree` in PolynomialFeatures.\n",
        "        * `n_neighbors` in K-Nearest Neighbors.\n",
        "        * `max_depth` or `n_estimators` in Random Forest.\n",
        "* **Hyperparameter Tuning (or Optimization):**\n",
        "    * The process of finding the combination of hyperparameter values for a given model that results in the best performance (as measured by a chosen evaluation metric, typically using cross-validation).\n",
        "* **Common Strategies:**\n",
        "    * **Grid Search (`GridSearchCV` in Scikit-learn):**\n",
        "        * You define a \"grid\" of hyperparameter values you want to try.\n",
        "        * `GridSearchCV` exhaustively tries every combination of these values.\n",
        "        * For each combination, it performs k-fold cross-validation.\n",
        "        * It then selects the combination of hyperparameters that yielded the best average CV score.\n",
        "    * **Random Search (`RandomizedSearchCV` in Scikit-learn):**\n",
        "        * You define a distribution or a list of values for each hyperparameter.\n",
        "        * `RandomizedSearchCV` samples a fixed number of combinations from these distributions.\n",
        "        * Often more efficient than Grid Search, especially when the hyperparameter space is large.\n",
        "    * More advanced methods: Bayesian Optimization, Genetic Algorithms."
      ],
      "metadata": {
        "id": "D3dKgPTCoWA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conceptual example of GridSearchCV for Ridge Regression's alpha\n",
        "    from sklearn.model_selection import GridSearchCV\n",
        "    from sklearn.linear_model import Ridge\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.pipeline import Pipeline\n",
        "\n",
        "    # Assume X_train_full, y_train_full are your complete training data\n",
        "    # X_train_full, y_train_full should be scaled if Ridge is used.\n",
        "    # For demonstration, let's use X_mlr_features_df and y_mlr_target again\n",
        "    # In practice, use your actual training set (e.g., result of initial train_test_split)\n",
        "\n",
        "    # Create a pipeline for scaling and then Ridge\n",
        "    # pipe_for_grid = Pipeline([\n",
        "    #     ('scaler', StandardScaler()),\n",
        "    #     ('ridge', Ridge())\n",
        "    # ])\n",
        "\n",
        "    # Define the grid of alpha values to search\n",
        "    # param_grid_ridge = {'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} # Note: 'ridge__' prefix for pipeline step\n",
        "\n",
        "    # Instantiate GridSearchCV\n",
        "    # grid_search = GridSearchCV(estimator=pipe_for_grid,\n",
        "    #                            param_grid=param_grid_ridge,\n",
        "    #                            cv=5, # 5-fold cross-validation\n",
        "    #                            scoring='neg_mean_squared_error', # Example metric\n",
        "    #                            verbose=1) # To see progress\n",
        "\n",
        "    # Fit GridSearchCV to the data\n",
        "    # grid_search.fit(X_mlr_features_df, y_mlr_target) # This should ideally be X_train, y_train\n",
        "\n",
        "    # Best parameters and best score\n",
        "    # print(f\"\\nBest alpha found by GridSearchCV: {grid_search.best_params_}\")\n",
        "    # print(f\"Best CV (Negative MSE) score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "    # The best_estimator_ attribute holds the model refitted on the whole training data with the best found C\n",
        "    # best_ridge_model = grid_search.best_estimator_"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "f6OTFLuBoWA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning is a critical step for optimizing model performance beyond using default settings.\n",
        "\n",
        "---\n",
        "\n",
        "**Module 8: Practice Questions**\n",
        "\n",
        "136. **Concept:** Why is it important to evaluate a machine learning model on a separate test set rather than on the training data?\n",
        "137. **MAE vs. MSE:** Explain the main difference between Mean Absolute Error (MAE) and Mean Squared Error (MSE) in terms of how they treat errors. Which one is more sensitive to outliers?\n",
        "138. **RMSE Interpretation:** If the RMSE for a house price prediction model is $25,000, what does this value represent?\n",
        "139. **R-squared:** What does an R-squared value of 0.85 signify about a regression model?\n",
        "140. **Adjusted R-squared:** Why might Adjusted R-squared be preferred over R-squared when comparing models with different numbers of features?\n",
        "141. **Coding Metrics:** Assuming you have `y_true_values` and `y_predicted_values`, write Python code using Scikit-learn to calculate MAE, MSE, and R².\n",
        "142. **Actual vs. Predicted Plot:** In an \"Actual vs. Predicted\" plot, what would an ideal distribution of points look like?\n",
        "143. **Residual Plot:** What are three key characteristics you would look for in a \"good\" residual plot for a linear regression model?\n",
        "144. **Heteroscedasticity:** If a residual plot shows a funnel shape (residuals fanning out), what problem does this indicate?\n",
        "145. **Cross-Validation Purpose:** What is the primary benefit of using k-fold cross-validation compared to a single train-test split?\n",
        "146. **K-Fold CV:** Describe how K-Fold Cross-Validation works in a few steps.\n",
        "147. **`cross_val_score`:** What does the `cross_val_score` function in Scikit-learn return?\n",
        "148. **Hyperparameters:** What is the difference between a model parameter (like a coefficient in linear regression) and a hyperparameter (like `alpha` in Ridge regression)?\n",
        "149. **Grid Search:** How does `GridSearchCV` work to find optimal hyperparameters?\n",
        "150. **Pipeline and CV:** Why is it crucial to use a Scikit-learn `Pipeline` when performing cross-validation if your modeling process includes preprocessing steps like scaling?\n",
        "151. **Critical Thinking:** If a model has a very high R-squared on the training data but a significantly lower R-squared on the test data (and cross-validation scores are also low), what common problem might this indicate?\n",
        "152. **Choosing a Metric:** You are predicting product delivery times in days. Your company is particularly concerned about very late deliveries. Would MAE or RMSE be a more appropriate primary metric to optimize for? Why?\n",
        "153. **Interpreting Residuals:** If the residuals in a residual plot (residuals vs. predicted values) are mostly positive for low predicted values and mostly negative for high predicted values, what might this suggest about your model?\n",
        "154. **Number of Folds:** What are common choices for `k` in K-Fold Cross-Validation? What might be a downside of choosing a very large `k` (e.g., `k=n`, also known as Leave-One-Out CV)?\n",
        "155. **`RandomizedSearchCV`:** What is an advantage of using `RandomizedSearchCV` over `GridSearchCV` for hyperparameter tuning, especially with many hyperparameters?\n",
        "\n",
        "---\n",
        "\n",
        "This concludes the comprehensive notes based on your provided transcript and the general completion of Module 8. Remember to insert actual images/diagrams where the `[Image: ...]` or `[Diagram: ...]` placeholders are located. Good luck with your data analysis journey!"
      ],
      "metadata": {
        "id": "DEMvhDCxoWA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"md-recitation\">\n",
        "  Sources\n",
        "  <ol>\n",
        "  <li><a href=\"https://github.com/Ash7540/Statistics_AI\">https://github.com/Ash7540/Statistics_AI</a></li>\n",
        "  <li><a href=\"https://github.com/AbhinavKDsharma17/all.for.one\">https://github.com/AbhinavKDsharma17/all.for.one</a></li>\n",
        "  <li><a href=\"https://github.com/mohitmaheshwari645/ML-assignment2-\">https://github.com/mohitmaheshwari645/ML-assignment2-</a></li>\n",
        "  <li><a href=\"https://github.com/byron-013/stock_analyzer_multivariate_regression\">https://github.com/byron-013/stock_analyzer_multivariate_regression</a></li>\n",
        "  <li><a href=\"https://github.com/MaggyRoseFoote/social-media-analysis-\">https://github.com/MaggyRoseFoote/social-media-analysis-</a></li>\n",
        "  <li><a href=\"https://github.com/Gayatri61103/Milk-grade-guard\">https://github.com/Gayatri61103/Milk-grade-guard</a></li>\n",
        "  <li><a href=\"https://github.com/ank1tha2003/AspireNex\">https://github.com/ank1tha2003/AspireNex</a></li>\n",
        "  <li><a href=\"https://github.com/Mamtasadani/-Iris-Dataset\">https://github.com/Mamtasadani/-Iris-Dataset</a></li>\n",
        "  <li><a href=\"https://www.cnblogs.com/Yanjy-OnlyOne/p/12520782.html\">https://www.cnblogs.com/Yanjy-OnlyOne/p/12520782.html</a></li>\n",
        "  </ol>\n",
        "</div>"
      ],
      "metadata": {
        "id": "dWexEIRgoWA1"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}