{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Okay, here is the continuation of your comprehensive notes, covering Module 7.\n",
        "\n",
        "---\n",
        "\n",
        "**Module 7: Introduction to Regression Modeling with Scikit-Learn**\n",
        "\n",
        "This module introduces fundamental regression models available in the Scikit-learn library. Regression analysis is a statistical method used to model the relationship between a dependent (target) variable and one or more independent (feature) variables. The goal is typically to predict a continuous outcome. We'll cover the concepts, Scikit-learn implementation, and how to interpret the model coefficients for Simple Linear Regression, Multiple Linear Regression, Polynomial Regression, and Ridge Regression.\n",
        "\n",
        "**7.1 Overview of Scikit-learn for Machine Learning**\n",
        "\n",
        "Scikit-learn is a cornerstone library for machine learning in Python, offering a vast array of tools for both supervised (like regression and classification) and unsupervised (like clustering and dimensionality reduction) learning. It's known for its clean, consistent API and comprehensive documentation. *[13]*\n",
        "\n",
        "* **Key Concepts in Scikit-learn's API:**\n",
        "    * **Estimators:** The core objects in Scikit-learn. Any object that can learn from data (either to predict, classify, transform, or cluster) is an estimator.\n",
        "        * All estimators implement a `fit(X, y)` method (for supervised learning) or `fit(X)` (for unsupervised learning) to learn from the training data. `X` represents the features and `y` the target variable. *[13]*\n",
        "        * Supervised estimators (like regressors and classifiers) typically have a `predict(X_new)` method to make predictions on new, unseen data.\n",
        "        * Some estimators also have a `score(X, y)` method to evaluate performance.\n",
        "    * **Transformers:** Estimators that can also transform data. Examples include scalers (`StandardScaler`, `MinMaxScaler`), encoders (`OneHotEncoder`), and feature extractors/creators (`PolynomialFeatures`).\n",
        "        * Transformers have a `transform(X)` method to apply the learned transformation.\n",
        "        * They often have a `fit_transform(X, y=None)` method, which is a convenient shortcut for fitting and then transforming the same data. *[13]*\n",
        "    * **Pipelines (`sklearn.pipeline.Pipeline`):**\n",
        "        * Allow chaining multiple steps (e.g., a scaler followed by a feature creator, then a regressor) into a single estimator object.\n",
        "        * This simplifies workflows, makes code cleaner, and is crucial for preventing data leakage (e.g., ensuring transformations are learned only on training data and consistently applied). *[13]*\n",
        "        * A pipeline itself has `fit()`, `predict()`, and `score()` methods.\n",
        "\n",
        "* **General Workflow for Supervised Learning in Scikit-learn:**\n",
        "    1.  **Import Model:** Import the desired model class (e.g., `from sklearn.linear_model import LinearRegression`).\n",
        "    2.  **Prepare Data:**\n",
        "        * Separate features (`X`) and target (`y`).\n",
        "        * Split data into training and testing sets (e.g., using `train_test_split` from `sklearn.model_selection`).\n",
        "        * Perform necessary preprocessing (scaling, encoding) – often done within a Pipeline.\n",
        "    3.  **Instantiate Model:** Create an instance of the model, specifying any hyperparameters (e.g., `model = LinearRegression()`). Hyperparameters are settings that are not learned from the data directly but are set before training.\n",
        "    4.  **Fit Model:** Train the model on the **training data**: `model.fit(X_train, y_train)`. *[90]*\n",
        "    5.  **Make Predictions:** Use the trained model to make predictions on **new data** (typically the test set): `y_pred = model.predict(X_test)`. *[90]*\n",
        "    6.  **Evaluate Model:** Assess the model's performance using appropriate metrics (e.g., Mean Squared Error, R-squared for regression).\n",
        "\n",
        "**7.2 Simple Linear Regression (SLR)**\n",
        "\n",
        "Simple Linear Regression (SLR) models the linear relationship between a **single independent variable** (feature, `x`) and a **continuous dependent variable** (target, `y`). It aims to find the best-fitting straight line through the data points. *[90]*\n",
        "\n",
        "* **Concept and Equation:**\n",
        "    The relationship is described by the equation:\n",
        "    $$y = \\beta_0 + \\beta_1 x + \\epsilon$$\n",
        "    Where: *[90]*\n",
        "    * `y`: Dependent variable (what we want to predict).\n",
        "    * `x`: Independent variable (the predictor).\n",
        "    * `\\beta_0` (beta-zero): **Intercept** – the predicted value of `y` when `x` is 0. It's where the regression line crosses the y-axis.\n",
        "    * `\\beta_1` (beta-one): **Slope** (or coefficient for `x`) – the change in the predicted value of `y` for a one-unit increase in `x`. It represents the steepness and direction of the line.\n",
        "    * `\\epsilon` (epsilon): **Error term** – the difference between the observed `y` value and the value predicted by the line (`\\hat{y} = \\beta_0 + \\beta_1 x`). It accounts for random variability or factors not included in the model.\n",
        "    The goal of SLR is to find the values of `\\beta_0` and `\\beta_1` that minimize the sum of the squared errors (residuals), often called the Ordinary Least Squares (OLS) method.\n",
        "\n",
        "* **Implementation with Scikit-learn:**\n",
        "    The `LinearRegression` class from `sklearn.linear_model` is used. *[90]*"
      ],
      "metadata": {
        "id": "977pf8-3n2IU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "    # Sample data for SLR\n",
        "    np.random.seed(0) # for reproducibility\n",
        "    # X_slr_data represents a single feature, needs to be 2D for scikit-learn ([n_samples, n_features])\n",
        "    X_slr_feature = np.random.rand(50, 1) * 10  # Single feature, e.g., Hours Studied\n",
        "    # y = 2.5x + 5 + noise (True relationship: intercept=5, slope=2.5)\n",
        "    y_slr_target = 2.5 * X_slr_feature.squeeze() + np.random.randn(50) * 5 + 5 # Target, e.g., Exam Score\n",
        "\n",
        "    # Split data\n",
        "    X_slr_train, X_slr_test, y_slr_train, y_slr_test = train_test_split(\n",
        "        X_slr_feature, y_slr_target, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # 1. Instantiate the model\n",
        "    slr_model = LinearRegression()\n",
        "\n",
        "    # 2. Fit the model to the training data\n",
        "    slr_model.fit(X_slr_train, y_slr_train)\n",
        "    print(\"SLR model fitting complete.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "sdyGe8xin2IY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Interpreting Coefficients:**\n",
        "    After fitting the model, the estimated intercept (`\\hat{\\beta_0}`) and coefficient (`\\hat{\\beta_1}`) can be accessed:\n",
        "    * `slr_model.intercept_`: Stores the estimated intercept `\\hat{\\beta_0}`. *[92]*\n",
        "    * `slr_model.coef_`: Stores the estimated coefficient(s) as a NumPy array. For SLR, it will be an array with one element, `\\hat{\\beta_1}`. *[92]*\n",
        "\n",
        "    * **Sign of Coefficient (`\\hat{\\beta_1}`):** *[94]*\n",
        "        * Positive `\\hat{\\beta_1}`: Indicates a positive linear relationship (as `x` increases, `y` tends to increase).\n",
        "        * Negative `\\hat{\\beta_1}`: Indicates a negative linear relationship (as `x` increases, `y` tends to decrease).\n",
        "    * **Magnitude of Coefficient (`\\hat{\\beta_1}`):** *[94]*\n",
        "        * The value of `\\hat{\\beta_1}` represents the average change in the dependent variable (`y`) for a one-unit increase in the independent variable (`x`)."
      ],
      "metadata": {
        "id": "fq6EwmgNn2Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimated_intercept_slr = slr_model.intercept_\n",
        "    estimated_coefficient_slr = slr_model.coef_[0] # For SLR, coef_ is an array with one value\n",
        "\n",
        "    print(f\"SLR Estimated Intercept (beta_0_hat): {estimated_intercept_slr:.4f}\")\n",
        "    print(f\"SLR Estimated Coefficient for X (beta_1_hat): {estimated_coefficient_slr:.4f}\")\n",
        "\n",
        "    # Example Interpretation:\n",
        "    # If intercept is 5.123 and coefficient is 2.450 for 'Hours Studied' vs 'Exam Score':\n",
        "    # - For 0 hours studied, the predicted exam score is 5.123. (Often, intercept interpretation needs context, as x=0 might be unrealistic).\n",
        "    # - For each additional hour studied, the exam score is predicted to increase by 2.450 points, on average.\n",
        "\n",
        "    # 3. Make predictions\n",
        "    y_slr_pred_train = slr_model.predict(X_slr_train)\n",
        "    y_slr_pred_test = slr_model.predict(X_slr_test)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "2IZiU_3tn2Ib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Visualization:**\n",
        "    A scatter plot of the data points (`x` vs `y`) with the fitted regression line (`\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x`) is a common way to visualize an SLR model. Seaborn's `regplot` can do this directly, or you can plot the scatter of actuals and then overlay the predicted line. *[91]*"
      ],
      "metadata": {
        "id": "YGYKEm5En2Ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "    # Scatter plot of the actual test data\n",
        "    plt.scatter(X_slr_test, y_slr_test, color='skyblue', label='Actual Test Data', alpha=0.7)\n",
        "    # Plot the regression line using test predictions\n",
        "    plt.plot(X_slr_test, y_slr_pred_test, color='red', linewidth=2, label='Fitted Regression Line (Predictions)')\n",
        "\n",
        "    plt.title('Simple Linear Regression Fit', fontsize=16)\n",
        "    plt.xlabel('Independent Variable (X - e.g., Hours Studied)', fontsize=12)\n",
        "    plt.ylabel('Dependent Variable (y - e.g., Exam Score)', fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "    plt.show()\n",
        "    # [Image: slr_fit.png - A scatter plot of X_slr_test vs y_slr_test with the red regression line (y_slr_pred_test) plotted over it. Clear labels for axes and legend.]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "JN6j9aOrn2Ic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, using Seaborn's `regplot`:"
      ],
      "metadata": {
        "id": "XiIsrpaOn2Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "    # Combine X_slr_test and y_slr_test into a DataFrame for Seaborn\n",
        "    df_slr_test = pd.DataFrame({'X': X_slr_test.squeeze(), 'y': y_slr_test})\n",
        "    sns.regplot(x='X', y='y', data=df_slr_test, scatter_kws={'color': 'skyblue', 'alpha':0.7}, line_kws={'color':'red'})\n",
        "    plt.title('Simple Linear Regression using Seaborn regplot', fontsize=16)\n",
        "    plt.xlabel('Independent Variable (X)', fontsize=12)\n",
        "    plt.ylabel('Dependent Variable (y)', fontsize=12)\n",
        "    plt.show()\n",
        "    # [Image: slr_seaborn_regplot.png - Similar to slr_fit.png, but generated using sns.regplot, possibly showing a confidence interval band around the regression line.]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "me1wp0RQn2Id"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.3 Multiple Linear Regression (MLR)**\n",
        "\n",
        "Multiple Linear Regression (MLR) extends SLR to model the relationship between **two or more independent variables** (`x_1, x_2, ..., x_p`) and a single continuous dependent variable (`y`). *[96]*\n",
        "\n",
        "* **Concept and Equation:**\n",
        "    The relationship is described by:\n",
        "    $$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon$$\n",
        "    Where: *[92]*\n",
        "    * `\\beta_0`: Intercept.\n",
        "    * `\\beta_1, \\beta_2, ..., \\beta_p`: Coefficients for each independent variable `x_1, x_2, ..., x_p`.\n",
        "    * `\\epsilon`: Error term.\n",
        "* **Implementation with Scikit-learn:**\n",
        "    The same `LinearRegression` class is used. The input `X` will now be a DataFrame or NumPy array with multiple columns (features). *[96]*"
      ],
      "metadata": {
        "id": "EVPz16OYn2Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data for MLR\n",
        "    np.random.seed(1) # for reproducibility\n",
        "    # X_mlr_data will have 3 features\n",
        "    X_mlr_features_df = pd.DataFrame(np.random.rand(100, 3) * 10, columns=['Feature1_Hrs', 'Feature2_PrevScore', 'Feature3_Projects'])\n",
        "    # True relationship: y = 3*x1 + 1.5*x2 - 0.8*x3 + 7 + noise\n",
        "    y_mlr_target = (3 * X_mlr_features_df['Feature1_Hrs'] +\n",
        "                    1.5 * X_mlr_features_df['Feature2_PrevScore'] -\n",
        "                    0.8 * X_mlr_features_df['Feature3_Projects'] +\n",
        "                    np.random.randn(100) * 7 + 7)\n",
        "\n",
        "    X_mlr_train, X_mlr_test, y_mlr_train, y_mlr_test = train_test_split(\n",
        "        X_mlr_features_df, y_mlr_target, test_size=0.25, random_state=42\n",
        "    )\n",
        "\n",
        "    mlr_model = LinearRegression()\n",
        "    mlr_model.fit(X_mlr_train, y_mlr_train)\n",
        "    print(\"\\nMLR model fitting complete.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "zOCz9FXon2Ie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Interpreting Coefficients:**\n",
        "    * `mlr_model.intercept_`: Estimated `\\hat{\\beta_0}`.\n",
        "    * `mlr_model.coef_`: A NumPy array containing the estimated coefficients `\\hat{\\beta_1}, \\hat{\\beta_2}, ..., \\hat{\\beta_p}`. The order corresponds to the order of columns in `X_mlr_train`.\n",
        "    * **Crucial Interpretation Point:** Each coefficient `\\hat{\\beta_j}` represents the average change in the dependent variable `y` for a one-unit increase in the independent variable `x_j`, **holding all other independent variables in the model constant**. *[94]* This \"ceteris paribus\" condition is vital."
      ],
      "metadata": {
        "id": "vFooblGNn2Ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimated_intercept_mlr = mlr_model.intercept_\n",
        "    estimated_coefficients_mlr = mlr_model.coef_\n",
        "\n",
        "    print(f\"\\nMLR Estimated Intercept: {estimated_intercept_mlr:.4f}\")\n",
        "    # Create a Series for easier viewing of coefficients with feature names\n",
        "    coeffs_df = pd.Series(estimated_coefficients_mlr, index=X_mlr_features_df.columns)\n",
        "    print(\"MLR Estimated Coefficients (betas):\\n\", coeffs_df)\n",
        "\n",
        "    # Example Interpretation (hypothetical coefficients):\n",
        "    # Feature1_Hrs: 2.95  => For a one-unit increase in Feature1_Hrs, y is predicted to increase by 2.95 units,\n",
        "    #                         assuming Feature2_PrevScore and Feature3_Projects are held constant.\n",
        "    # Feature2_PrevScore: 1.48 => For a one-unit increase in Feature2_PrevScore, y is predicted to increase by 1.48 units,\n",
        "    #                         assuming Feature1_Hrs and Feature3_Projects are held constant.\n",
        "    # Feature3_Projects: -0.75 => For a one-unit increase in Feature3_Projects, y is predicted to decrease by 0.75 units,\n",
        "    #                         assuming Feature1_Hrs and Feature2_PrevScore are held constant.\n",
        "\n",
        "    y_mlr_pred_test = mlr_model.predict(X_mlr_test)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "EW42yZSqn2Ie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Multicollinearity:** If independent variables in an MLR model are highly correlated with each other (multicollinearity), the coefficient estimates can become unstable, have large standard errors, and be difficult to interpret reliably. The overall model might still predict well, but individual coefficient interpretations become suspect. *[90]*\n",
        "\n",
        "**7.4 Polynomial Regression**\n",
        "\n",
        "Polynomial Regression is a type of regression analysis where the relationship between the independent variable `x` and the dependent variable `y` is modeled as an *n*-th degree polynomial in `x`. Although it models non-linear relationships in the original feature space, it's considered a **special case of multiple linear regression** because the model is linear in terms of its *coefficients*. *[98]*\n",
        "\n",
        "* **Concept and Equation:**\n",
        "    For a single feature `x`, a polynomial regression of degree `d` is:\n",
        "    $$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + ... + \\beta_d x^d + \\epsilon$$\n",
        "    New features (`x^2, x^3, ..., x^d`) are created from the original feature `x`. The model then fits a linear relationship to these original and transformed features. *[98]*\n",
        "\n",
        "* **Scikit-learn `PolynomialFeatures`:**\n",
        "    This transformer from `sklearn.preprocessing` generates a new feature matrix consisting of all polynomial combinations of the features with a degree less than or equal to the specified degree. *[98]*\n",
        "    * `degree`: The degree of the polynomial features to generate (e.g., `degree=2` with input `[x]` generates `[1, x, x^2]`; with input `[x1, x2]` generates `[1, x1, x2, x1^2, x1*x2, x2^2]`). *[99]*\n",
        "    * `include_bias=True` (default): Includes a column of ones (the bias or intercept term). Usually set to `False` when used with `LinearRegression` (which handles its own intercept), or within a Pipeline where `LinearRegression` is the final step.\n",
        "    * `interaction_only=False` (default): If `True`, only interaction features (e.g., `x1*x2`) are produced, not higher-order terms of single features (e.g., `x1^2`). *[101]*\n",
        "\n",
        "* **Implementation:**\n",
        "    Typically involves a two-step process (or streamlined using a Scikit-learn `Pipeline` *[89]*):\n",
        "    1.  Transform the input features using `PolynomialFeatures`.\n",
        "    2.  Fit a `LinearRegression` model on these new, transformed features."
      ],
      "metadata": {
        "id": "XELKPjMyn2If"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "    from sklearn.pipeline import Pipeline # For easily combining steps\n",
        "\n",
        "    # Generate some non-linear data for better illustration\n",
        "    np.random.seed(0)\n",
        "    X_poly_feature = np.sort(np.random.rand(70, 1) * 10 - 5, axis=0) # Single feature from -5 to 5\n",
        "    # True relationship: y = 0.5x^2 + x + 2 + noise\n",
        "    y_poly_target = 0.5 * X_poly_feature.squeeze()**2 + X_poly_feature.squeeze() + 2 + np.random.randn(70) * 3\n",
        "\n",
        "    X_poly_train, X_poly_test, y_poly_train, y_poly_test = train_test_split(\n",
        "        X_poly_feature, y_poly_target, test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    # Define the degree of the polynomial\n",
        "    degree = 2 # Try changing to 1 (linear), 3, 10 (overfitting) to see effect\n",
        "\n",
        "    # Create a pipeline: 1. PolynomialFeatures, 2. LinearRegression\n",
        "    poly_reg_pipeline = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=degree, include_bias=False)), # No bias needed for LinearRegression step\n",
        "        (\"lin_reg\", LinearRegression())\n",
        "    ])\n",
        "\n",
        "    poly_reg_pipeline.fit(X_poly_train, y_poly_train)\n",
        "    y_poly_pred_test = poly_reg_pipeline.predict(X_poly_test)\n",
        "\n",
        "    # Accessing coefficients (from the linear regression step in the pipeline)\n",
        "    # poly_transformer = poly_reg_pipeline.named_steps['poly_features']\n",
        "    linear_regressor_in_pipe = poly_reg_pipeline.named_steps['lin_reg']\n",
        "    # print(f\"Polynomial features names (example): {poly_transformer.get_feature_names_out(['x'])}\") # ['x', 'x^2'] for degree 2\n",
        "    print(f\"\\nPolynomial Regression (degree {degree}) Intercept: {linear_regressor_in_pipe.intercept_:.4f}\")\n",
        "    print(f\"Polynomial Regression (degree {degree}) Coefficients: {linear_regressor_in_pipe.coef_}\")\n",
        "    # For degree 2, coef_ will have two values: for x and x^2 terms."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "x84xCgVmn2If"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Polynomial regression can capture more complex, non-linear patterns in data than simple linear regression. *[100]*\n",
        "    * **Risk of Overfitting:** Choosing a very high degree can lead to overfitting. The model might fit the training data noise too closely and perform poorly on unseen test data. *[100]*\n",
        "    * `[Diagram: A plot showing underfitting (a straight line trying to fit curved data), good fit (a curve of appropriate degree fitting the data well), and overfitting (a very wiggly curve passing through almost all training points, likely to generalize poorly).]`\n",
        "\n",
        "* **Visualization for Polynomial Regression:**"
      ],
      "metadata": {
        "id": "tGe8tI4wn2If"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "    plt.scatter(X_poly_train, y_poly_train, color='skyblue', label='Training Data', alpha=0.6)\n",
        "    plt.scatter(X_poly_test, y_poly_test, color='orange', label='Test Data', alpha=0.8)\n",
        "\n",
        "    # For a smooth line, predict on a sorted range of X values\n",
        "    X_plot_range = np.linspace(X_poly_feature.min(), X_poly_feature.max(), 100).reshape(-1, 1)\n",
        "    y_plot_pred_poly = poly_reg_pipeline.predict(X_plot_range)\n",
        "\n",
        "    plt.plot(X_plot_range, y_plot_pred_poly, color='red', linewidth=2,\n",
        "             label=f'Polynomial Fit (degree={degree})')\n",
        "\n",
        "    # For comparison: fit and plot a simple linear regression\n",
        "    simple_lin_reg_on_poly = LinearRegression()\n",
        "    simple_lin_reg_on_poly.fit(X_poly_train, y_poly_train)\n",
        "    y_plot_pred_simple = simple_lin_reg_on_poly.predict(X_plot_range)\n",
        "    plt.plot(X_plot_range, y_plot_pred_simple, color='green', linestyle='--', linewidth=2,\n",
        "             label='Simple Linear Fit')\n",
        "\n",
        "    plt.title(f'Polynomial Regression (Degree {degree}) vs. Simple Linear Regression', fontsize=16)\n",
        "    plt.xlabel('Independent Variable (X)', fontsize=12)\n",
        "    plt.ylabel('Dependent Variable (y)', fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.ylim(y_poly_target.min()-10, y_poly_target.max()+10) # Adjust y-limits for better view\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "    plt.show()\n",
        "    # [Image: polynomial_fit_comparison.png - A scatter plot of non-linear data points (X_poly vs y_poly).\n",
        "    # A red curve shows the polynomial regression fit (e.g., degree 2 or 3), following the data's curvature.\n",
        "    # A green dashed line shows a simple linear regression fit, which poorly captures the trend.\n",
        "    # Axes, legend, and title included.]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "aGcJ2sBMn2Ig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.5 Ridge Regression (L2 Regularization)**\n",
        "\n",
        "Ridge Regression is a linear regression model that includes an **L2 regularization penalty**. Regularization is a technique used to prevent overfitting (especially in models with many features or multicollinearity) by adding a penalty term to the loss function that discourages overly complex models (i.e., models with very large coefficient values). *[104]*\n",
        "\n",
        "* **Concept and Loss Function:**\n",
        "    The standard Ordinary Least Squares (OLS) method (used in `LinearRegression`) minimizes the sum of squared residuals (RSS). Ridge Regression modifies this by adding a penalty proportional to the sum of the squares of the coefficients:\n",
        "    $$\\text{Loss Function (Ridge)} = \\text{RSS} + \\alpha \\sum_{j=1}^{p} \\beta_j^2$$\n",
        "    $$\\text{Loss Function (Ridge)} = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij}))^2 + \\alpha \\sum_{j=1}^{p} \\beta_j^2$$\n",
        "    Where: *[105]*\n",
        "    * `\\alpha` (alpha, also sometimes denoted as lambda `\\lambda`): The **regularization strength parameter**. It's a non-negative hyperparameter that controls the amount of shrinkage applied to the coefficients. *[104]*\n",
        "        * If `\\alpha = 0`, Ridge Regression becomes identical to OLS (Simple/Multiple Linear Regression).\n",
        "        * As `\\alpha \\rightarrow \\infty` (alpha gets very large), the coefficients are shrunk strongly towards zero. This leads to a simpler model with potentially higher bias but lower variance (better generalization). *[106]*\n",
        "        * If `\\alpha` is small, the penalty has less effect.\n",
        "    * `\\beta_j^2`: The square of the coefficient for the *j*-th feature. The intercept `\\beta_0` is typically *not* penalized.\n",
        "\n",
        "* **Purpose of Ridge Regression:**\n",
        "    * **Reduces Model Complexity & Prevents Overfitting:** Especially useful when the number of predictors (`p`) is large relative to the number of observations (`n`), or when features are highly correlated.\n",
        "    * **Handles Multicollinearity:** More effectively than OLS. When predictors are highly correlated, OLS coefficients can have high variance (be very sensitive to small changes in data). Ridge tends to shrink correlated coefficients towards each other and stabilize them.\n",
        "    * **Coefficient Shrinkage:** Shrinks coefficients towards zero but **does not set them exactly to zero** (unless `\\alpha` is infinitely large). This means it keeps all features in the model but reduces their impact. This is a key difference from Lasso Regression (L1 regularization), which can perform feature selection by setting some coefficients to exactly zero.\n",
        "\n",
        "* **Scikit-learn: `Ridge` class from `sklearn.linear_model`.** *[104]*\n",
        "    * The `alpha` parameter corresponds to `\\alpha` in the loss function.\n",
        "    * **Important:** Feature scaling (e.g., Standardization using `StandardScaler`) is generally **highly recommended** before applying Ridge Regression. The L2 penalty (`\\sum \\beta_j^2`) is sensitive to the scale of the features; features with larger values (and thus potentially larger coefficients even if not more important) might be penalized more heavily simply due to their scale. Scaling ensures features are on a comparable basis before the penalty is applied."
      ],
      "metadata": {
        "id": "WzQSp5ian2Ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "    from sklearn.preprocessing import StandardScaler # Scaling is important for Ridge\n",
        "\n",
        "    # Using MLR data (X_mlr_train, y_mlr_train, X_mlr_test)\n",
        "    # 1. Scale data first (fit on train, transform train and test)\n",
        "    scaler_for_ridge = StandardScaler()\n",
        "    X_mlr_train_scaled = scaler_for_ridge.fit_transform(X_mlr_train)\n",
        "    X_mlr_test_scaled = scaler_for_ridge.transform(X_mlr_test) # Use same scaler\n",
        "\n",
        "    # 2. Instantiate Ridge model with a chosen alpha\n",
        "    # The optimal alpha is usually found via cross-validation (covered in Module 8)\n",
        "    alpha_value = 1.0 # Common default starting point\n",
        "    ridge_model = Ridge(alpha=alpha_value)\n",
        "\n",
        "    # 3. Fit the Ridge model\n",
        "    ridge_model.fit(X_mlr_train_scaled, y_mlr_train)\n",
        "\n",
        "    print(f\"\\nRidge Regression (alpha={alpha_value}) Intercept: {ridge_model.intercept_:.4f}\")\n",
        "    ridge_coeffs_df = pd.Series(ridge_model.coef_, index=X_mlr_features_df.columns) # Assuming X_mlr_features_df had original column names\n",
        "    print(f\"Ridge Regression (alpha={alpha_value}) Coefficients:\\n\", ridge_coeffs_df)\n",
        "\n",
        "    # Compare with OLS coefficients (from mlr_model, assuming X_mlr_train was unscaled there)\n",
        "    # For a fair comparison, fit an OLS model on SCALED data too:\n",
        "    ols_on_scaled_data = LinearRegression()\n",
        "    ols_on_scaled_data.fit(X_mlr_train_scaled, y_mlr_train)\n",
        "    ols_coeffs_scaled_df = pd.Series(ols_on_scaled_data.coef_, index=X_mlr_features_df.columns)\n",
        "    print(\"\\nOLS (LinearRegression) Coefficients on SCALED data for comparison:\\n\", ols_coeffs_scaled_df)\n",
        "    # Expect Ridge coefficients to be smaller in magnitude (shrunk towards zero) than OLS coefficients on scaled data, especially if alpha > 0.\n",
        "\n",
        "    y_ridge_pred_test = ridge_model.predict(X_mlr_test_scaled)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "wnRxwfzVn2Ig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Effect of `alpha` on Coefficients:**\n",
        "    Higher values of `alpha` lead to greater shrinkage of coefficients towards zero. A common practice is to try a range of `alpha` values and use cross-validation to find the one that yields the best performance on unseen data. *[106]*"
      ],
      "metadata": {
        "id": "EWb0SGykn2Ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize effect of different alpha values on coefficients\n",
        "    n_alphas = 200\n",
        "    alphas_range = np.logspace(-4, 4, n_alphas) # Range of alpha values from 0.0001 to 10000\n",
        "    coefs_ridge_path = []\n",
        "\n",
        "    for current_alpha in alphas_range:\n",
        "        ridge_path_model = Ridge(alpha=current_alpha, fit_intercept=True) # fit_intercept is True by default\n",
        "        ridge_path_model.fit(X_mlr_train_scaled, y_mlr_train) # Using SCALED training data\n",
        "        coefs_ridge_path.append(ridge_path_model.coef_)\n",
        "\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    ax = plt.gca()\n",
        "    ax.plot(alphas_range, coefs_ridge_path)\n",
        "    ax.set_xscale('log') # Alpha is often viewed on a log scale\n",
        "    # ax.set_xlim(ax.get_xlim()[::-1]) # Optional: reverse x-axis for some conventions\n",
        "    plt.xlabel('Alpha (Regularization Strength) - Log Scale', fontsize=12)\n",
        "    plt.ylabel('Coefficient Magnitudes', fontsize=12)\n",
        "    plt.title('Ridge Coefficients as a Function of Regularization Strength (Alpha)', fontsize=16)\n",
        "    plt.axis('tight')\n",
        "    plt.legend(X_mlr_features_df.columns, loc='upper right', title=\"Features\") # Add legend for features\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "    plt.show()\n",
        "    # [Image: ridge_coefficients_vs_alpha.png - A plot with Alpha on a log-scaled x-axis and Coefficient values on the y-axis.\n",
        "    # Multiple lines (one for each feature in X_mlr_features_df) show how coefficients shrink towards zero as alpha increases.\n",
        "    # A legend identifies each line with its feature name.]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "5WCCYy9tn2Ih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge regression is a valuable tool for improving the stability and generalization of linear models, particularly in situations with many features, multicollinearity, or when trying to prevent overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "**Module 7: Practice Questions**\n",
        "\n",
        "116. **Scikit-learn API:** What is the primary method called on an estimator object in Scikit-learn to train it on data? What are the typical arguments to this method for supervised learning?\n",
        "117. **Scikit-learn API:** After training a supervised model, which method is used to get predictions on new data?\n",
        "118. **SLR Equation:** Write the basic equation for Simple Linear Regression and briefly define each term (`y`, `x`, `\\beta_0`, `\\beta_1`, `\\epsilon`).\n",
        "119. **SLR Coefficient Interpretation:** In an SLR model predicting 'Salary' (in thousands of dollars) based on 'YearsExperience', if the coefficient `\\hat{\\beta_1}` for 'YearsExperience' is 5.2, how would you interpret this value?\n",
        "120. **Coding SLR:** You have `X_train_feature` (a single feature, 2D) and `y_train_target`. Write the Scikit-learn code to instantiate and fit a Simple Linear Regression model.\n",
        "121. **MLR Concept:** How does Multiple Linear Regression differ from Simple Linear Regression?\n",
        "122. **MLR Coefficient Interpretation:** In an MLR model, if the coefficient for feature `x_k` is `\\hat{\\beta_k} = -2.5`, how is this interpreted regarding the relationship between `x_k` and `y`, considering other features in the model?\n",
        "123. **Polynomial Regression:** Why is Polynomial Regression considered a special case of Multiple Linear Regression, even though it can model non-linear relationships?\n",
        "124. **`PolynomialFeatures`:** If you use `PolynomialFeatures(degree=2, include_bias=False)` on a dataset with two original features `[a, b]`, what new features would be generated?\n",
        "125. **Overfitting:** What is a major risk when using a high degree in Polynomial Regression?\n",
        "126. **Pipeline:** Why is it often beneficial to use a `Pipeline` in Scikit-learn when combining steps like `PolynomialFeatures` and `LinearRegression`?\n",
        "127. **Regularization:** What is the primary purpose of regularization techniques like Ridge Regression?\n",
        "128. **Ridge Regression (Alpha):** In Ridge Regression, what does the hyperparameter `\\alpha` (alpha) control? What happens if `\\alpha = 0`? What happens if `\\alpha` is very large?\n",
        "129. **Ridge vs. OLS:** How do the coefficient estimates from Ridge Regression typically compare to those from Ordinary Least Squares (OLS) Linear Regression, especially when `\\alpha > 0`?\n",
        "130. **Feature Scaling for Ridge:** Why is feature scaling (e.g., standardization) particularly important before applying Ridge Regression?\n",
        "131. **Coding Ridge:** Assume you have scaled training data `X_train_scaled` and target `y_train`. Write Scikit-learn code to instantiate and fit a Ridge Regression model with `alpha = 0.5`.\n",
        "132. **Multicollinearity:** How might Ridge Regression help in situations where multicollinearity exists among predictor variables?\n",
        "133. **L2 Penalty:** The penalty term in Ridge Regression (`\\alpha \\sum \\beta_j^2`) is known as an L2 penalty. What does this mean in terms of how it penalizes coefficients?\n",
        "134. **Model Complexity:** Does increasing the `alpha` value in Ridge Regression increase or decrease the complexity of the resulting model? Explain.\n",
        "135. **Visualization:** What kind of plot is typically used to visualize the effect of different `alpha` values on the coefficients in Ridge Regression?\n",
        "\n",
        "---\n",
        "*(Module 8 will follow in the next response. The provided transcript for Module 8 is incomplete, so the notes will reflect that.)*"
      ],
      "metadata": {
        "id": "jX3ApWOBn2Ih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"md-recitation\">\n",
        "  Sources\n",
        "  <ol>\n",
        "  <li><a href=\"https://benny.istan.to/blog/20230616-regression-analysis-with-dummy-variables\">https://benny.istan.to/blog/20230616-regression-analysis-with-dummy-variables</a></li>\n",
        "  <li><a href=\"https://github.com/Bhumi45/MachineLearning\">https://github.com/Bhumi45/MachineLearning</a></li>\n",
        "  <li><a href=\"https://cyberleninka.ru/article/n/oil-production-forecasting-using-regression-algorithms\">https://cyberleninka.ru/article/n/oil-production-forecasting-using-regression-algorithms</a></li>\n",
        "  <li><a href=\"https://www.fastercapital.com/content/Polynomial-Regression--The-Curves-of-Discovery--Polynomial-Regression-for-a-Non-Linear-World.html\">https://www.fastercapital.com/content/Polynomial-Regression--The-Curves-of-Discovery--Polynomial-Regression-for-a-Non-Linear-World.html</a></li>\n",
        "  <li><a href=\"https://machinelearningmastery.com/capturing-curves-advanced-modeling-with-polynomial-regression/\">https://machinelearningmastery.com/capturing-curves-advanced-modeling-with-polynomial-regression/</a></li>\n",
        "  </ol>\n",
        "</div>"
      ],
      "metadata": {
        "id": "0wL2ie7Dn2Ih"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}