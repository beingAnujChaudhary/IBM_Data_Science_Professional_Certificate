{"cells":[{"cell_type":"markdown","source":["Okay, here are comprehensive notes on \"Data Analysis Using Python\" based on the provided transcript. These notes are designed to be understandable, include Python code examples with explanations, suggest diagrams/images, and provide practice questions. You should be able to copy and paste this into Google Docs, and then manually insert the images where indicated.\n","\n","**Comprehensive Notes for Data Analysis Using Python**\n","\n","---\n","\n","**Module 1: Introduction to Data Analysis and Python Environment**\n","\n","**1.1 What is Data Analysis?**\n","\n","* **Definition:** Data analysis is the systematic process of examining, cleaning, transforming, and interpreting raw data to extract meaningful insights, identify patterns and trends, answer questions, and support informed decision-making. Think of it as being a detective for data, looking for clues (insights) within a pile of evidence (raw data).\n","* **Goal:** To turn raw data into actionable knowledge.\n","* **Process:** It involves several stages:\n","    1.  **Asking the right questions:** What do you want to find out?\n","    2.  **Data Collection:** Gathering raw data from various sources.\n","    3.  **Data Cleaning (Inspection & Cleansing):** Identifying and handling errors, inconsistencies, missing values, and outliers in the data. This is like preparing your ingredients before cooking.\n","    4.  **Data Transformation:** Converting data into a suitable format for analysis and modeling. This might involve changing data types, creating new features, or restructuring data.\n","    5.  **Data Modeling & Analysis:** Applying statistical techniques, algorithms, and computational tools to explore the data, find patterns, test hypotheses, and build predictive models.\n","    6.  **Interpretation & Communication:** Understanding the results of the analysis and communicating the findings effectively, often through reports, visualizations, and dashboards.\n","* **Importance:** Data analysis is vital across many fields like business (to understand customer behavior, optimize marketing), science (to analyze experimental results), finance (for risk assessment, fraud detection), healthcare (to improve patient outcomes), and social sciences (to study societal trends).\n","\n","**1.2 Why Python for Data Analysis?**\n","\n","Python has become a dominant language for data analysis for several compelling reasons:\n","\n","* **Simplicity and Readability:**\n","    * **Understandable Definition:** Python's syntax is designed to be clear, concise, and human-readable, resembling plain English. This lowers the learning curve, especially for those new to programming.\n","    * **Example:** Compare `print(\"Hello\")` in Python to more verbose syntax in other languages.\n","* **Extensive Libraries:**\n","    * **Understandable Definition:** Python offers a vast collection of specialized toolkits (libraries) built specifically for data analysis tasks. These libraries provide pre-written, optimized code, so you don't have to build everything from scratch.\n","    * **Key Libraries:** Pandas (data manipulation), NumPy (numerical computing), Matplotlib/Seaborn (visualization), Scikit-learn (machine learning), Statsmodels (statistical modeling).\n","* **Large Community Support:**\n","    * **Understandable Definition:** A massive global community of Python users and developers means abundant learning resources (tutorials, blogs, courses), quick help when you're stuck (forums like Stack Overflow), and continuous improvement of the language and its libraries.\n","* **Integration Capabilities:**\n","    * **Understandable Definition:** Python plays well with other technologies. It can be easily integrated with other programming languages (like C++, Java), databases, web frameworks, and big data tools.\n","* **Scalability:**\n","    * **Understandable Definition:** Python can effectively handle datasets of varying sizes, from small spreadsheets to large enterprise databases. Performance for very large datasets can be enhanced using optimized libraries and techniques for parallel processing.\n","* **Versatility:** Python is a general-purpose language, meaning it's not just for data analysis. You can use it for web development, automation, AI, and more, making it a valuable skill overall.\n","\n","**1.3 Core Python Libraries for Data Analysis**\n","\n","These are the workhorse libraries you'll encounter frequently:\n","\n","* **Pandas:**\n","    * **Definition:** Pandas (Python Data Analysis Library) is an open-source library that provides high-performance, flexible, and intuitive data structures (like tables) and tools for data manipulation and analysis. It's the cornerstone for most data wrangling tasks in Python.\n","        * *Reference: [1] (Implied)*\n","    * **Key Features:**\n","        * **DataFrame Object:** A 2-dimensional, size-mutable, tabular data structure with labeled rows and columns (like a spreadsheet or SQL table). It can hold heterogeneous data types (numbers, strings, booleans, etc.). *[1]*\n","        * **Series Object:** A 1-dimensional labeled array, capable of holding any data type. Think of it as a single column in a DataFrame.\n","        * **Data Input/Output (I/O):** Comprehensive tools for reading data from and writing data to various file formats, including CSV, Excel, SQL databases, JSON, HTML, HDF5, and more. *[1]*\n","        * **Data Alignment and Missing Data Handling:** Smart alignment of data based on labels and robust tools for detecting, filtering, and filling missing data (e.g., `NaN`). *[1]*\n","        * **Reshaping and Pivoting:** Powerful functions for restructuring datasets, such as pivoting, stacking, unstacking, and melting. *[1]*\n","        * **Label-based Slicing, Indexing, and Subsetting:** Flexible ways to select, filter, and retrieve specific parts of large datasets using labels or positions. *[1]*\n","        * **Group By Engine:** Efficient mechanism for splitting data into groups based on some criteria, applying a function to each group (e.g., sum, mean), and then combining the results. *[1]*\n","        * **Merging and Joining:** SQL-like operations for combining datasets based on common columns or indices. *[1]*\n","        * **Time Series Functionality:** Specialized tools for working with time-stamped data, including date range generation, frequency conversion, moving window statistics, and date shifting. *[1]*\n","        * **Performance:** Critical internal operations are often written in Cython or C for speed. *[1]*\n","    * **Official Documentation:** [Pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/) (Link based on common knowledge, as specific URL for [1] isn't provided beyond context)\n","\n","* **NumPy:**\n","    * **Definition:** NumPy (Numerical Python) is the foundational package for numerical computation in Python. It provides powerful support for large, multi-dimensional arrays and matrices, along with a vast collection of mathematical functions to operate on them efficiently. Many other data science libraries, including Pandas, are built on top of NumPy.\n","        * *Reference: [3]*\n","    * **Key Features:**\n","        * **N-dimensional Array Object (`ndarray`):** An efficient, powerful data structure for storing and manipulating homogeneous numerical data (all elements of the same type). *[4]*\n","        * **Broadcasting Functions:** A mechanism that allows NumPy to perform arithmetic operations on arrays of different shapes, simplifying code and avoiding explicit loops. *[4]*\n","        * **Tools for Integrating C/C++ and Fortran Code:** Allows for leveraging code written in these languages for performance-critical tasks. *[4]*\n","        * **Linear Algebra, Fourier Transform, and Random Number Capabilities:** Comprehensive mathematical functions for advanced computations. *[4]*\n","    * **Official Documentation:** [NumPy Documentation](https://numpy.org/doc/) *[4]*\n","\n","* **Matplotlib:**\n","    * **Definition:** Matplotlib is a comprehensive and widely-used Python library for creating static, animated, and interactive visualizations in a variety of formats. It provides fine-grained control over every aspect of a plot.\n","        * *Reference: [6]*\n","    * **Key Features:**\n","        * **Variety of Plots:** Can generate line plots, scatter plots, bar charts, histograms, pie charts, error charts, contour plots, and much more. *[6]*\n","        * **Figures and Axes:** Plots are typically created within a `Figure` object, which can contain one or more `Axes` objects. An `Axes` represents an individual plot or subplot with its own coordinate system. *[6]*\n","        * **Artist Objects:** Almost everything you see on a Matplotlib figure is an `Artist` (e.g., text objects, lines, patches). This object-oriented approach allows for detailed customization. *[6]*\n","        * **Pyplot Module (`matplotlib.pyplot`):** A collection of functions that provide a MATLAB-like interface, making it easy to create plots quickly. *[8]*\n","        * **Customization:** Offers extensive control over colors, line styles, fonts, labels, legends, and annotations. *[6]*\n","    * **Official Documentation:** [Matplotlib Documentation](https://matplotlib.org/stable/contents.html) *[7]*\n","\n","* **Seaborn:**\n","    * **Definition:** Seaborn is a Python data visualization library built on top of Matplotlib. It provides a higher-level interface for creating attractive and informative statistical graphics, often requiring less code than Matplotlib for common statistical plots.\n","        * *Reference: [9]*\n","    * **Key Features:**\n","        * **Statistical Plotting:** Specialized functions for visualizing statistical relationships, distributions, and categorical data.\n","        * **Integration with Pandas:** Works seamlessly with Pandas DataFrames, making it easy to map DataFrame columns to plot aesthetics. *[9]*\n","        * **Aesthetic Defaults:** Comes with visually appealing default styles, themes, and color palettes. *[9]*\n","        * **High-Level Functions:** Simplifies the creation of complex plots like heatmaps, violin plots, pair plots, joint plots, and regression plots with built-in statistical estimation. *[11]*\n","    * **Official Documentation:** [Seaborn Documentation](https://seaborn.pydata.org/) *[10]*\n","\n","* **Scikit-learn:**\n","    * **Definition:** Scikit-learn is a comprehensive and robust open-source machine learning library for Python. It provides a wide range of tools for supervised and unsupervised learning, model selection, preprocessing, and evaluation.\n","        * *Reference: [13], [14]*\n","    * **Key Features:**\n","        * **Supervised Learning Algorithms:** Includes algorithms for classification (e.g., Logistic Regression, SVM, Decision Trees, Random Forests) and regression (e.g., Linear Regression, Ridge Regression). *[14]*\n","        * **Unsupervised Learning Algorithms:** Offers algorithms for clustering (e.g., K-Means, DBSCAN), dimensionality reduction (e.g., PCA, t-SNE), and anomaly detection. *[14]*\n","        * **Data Preprocessing:** Provides tools for feature scaling, normalization, encoding categorical data, handling missing values, and feature extraction. *[13]*\n","        * **Model Selection:** Includes methods for splitting data into training and testing sets, cross-validation, and hyperparameter tuning (e.g., GridSearchCV). *[13]*\n","        * **Model Evaluation:** Offers various metrics for assessing the performance of classification, regression, and clustering models. *[13]*\n","        * **Interoperability:** Designed to work well with NumPy and SciPy.\n","    * **Official Documentation:** [Scikit-learn Documentation](https://scikit-learn.org/stable/) *[14]*\n","\n","* **Statsmodels:**\n","    * **Definition:** Statsmodels is a Python library that focuses on providing classes and functions for estimating many different statistical models, conducting statistical tests, and performing statistical data exploration. It often provides more detailed statistical output and diagnostics compared to Scikit-learn for classical statistical models.\n","        * *Reference: [16]*\n","    * **Key Features:**\n","        * **Statistical Models:** Extensive support for linear models (OLS), generalized linear models (GLM), time series analysis (ARIMA, VAR), survival analysis, and more. *[17]*\n","        * **Statistical Tests:** A wide array of hypothesis tests, goodness-of-fit tests, and diagnostic tests.\n","        * **R-style Formulas:** Allows model specification using R-like formulas in conjunction with Pandas DataFrames (e.g., `response ~ predictor1 + predictor2`). *[17]*\n","        * **Detailed Results:** Provides comprehensive summary outputs for model estimators, including standard errors, p-values, confidence intervals, and various statistical metrics. *[17]*\n","    * **Official Documentation:** [Statsmodels Documentation](https://www.statsmodels.org/stable/index.html) *[17]*\n","\n","**1.4 Setting Up the Python Environment**\n","\n","A correctly configured Python environment is essential for a smooth data analysis workflow.\n","\n","* **Recommended Method: Anaconda Distribution**\n","    * **What it is:** Anaconda is a free and open-source distribution of Python and R, specifically tailored for scientific computing and data science.\n","    * **Benefits:** It comes with Python, the `conda` package and environment manager, and many of the core data analysis libraries (Pandas, NumPy, Matplotlib, Scikit-learn, etc.) pre-installed. This simplifies setup significantly.\n","    * **Installation:** Download from [anaconda.com](https://www.anaconda.com/products/distribution) and follow the installation instructions for your operating system.\n","* **Alternative: Standard Python + `pip`**\n","    1.  **Install Python:** Download Python directly from [python.org](https://www.python.org/downloads/).\n","    2.  **Use `pip` (Python Package Installer):** `pip` is usually included with Python. You can install libraries individually from the command line/terminal:\n","        ```bash\n","        pip install pandas numpy matplotlib seaborn scikit-learn statsmodels\n","        ```\n","* **Virtual Environments (Highly Recommended):**\n","    * **Why?** To manage dependencies for different projects separately. This prevents conflicts where one project needs a specific version of a library, and another project needs a different version.\n","    * **How?**\n","        * **Using `venv` (built-in with Python):**\n","            ```bash\n","            # Create a virtual environment (e.g., named 'myenv')\n","            python -m venv myenv\n","            # Activate it\n","            # On Windows:\n","            # myenv\\Scripts\\activate\n","            # On macOS/Linux:\n","            # source myenv/bin/activate\n","            # Install packages within the activated environment\n","            pip install ...\n","            # Deactivate when done\n","            # deactivate\n","            ```\n","        * **Using `conda` (with Anaconda):**\n","            ```bash\n","            # Create a virtual environment (e.g., named 'my_conda_env') with specific Python version\n","            conda create --name my_conda_env python=3.9\n","            # Activate it\n","            conda activate my_conda_env\n","            # Install packages (can specify conda or pip install)\n","            conda install pandas\n","            # or\n","            # pip install pandas\n","            # Deactivate when done\n","            conda deactivate\n","            ```\n","    * `[Diagram: Flowchart showing decision: New Project? -> Yes -> Create Virtual Environment -> Activate -> Install Libraries. If Existing Project -> Activate Environment.]`\n","\n","---\n","\n","**Module 1: Practice Questions**\n","\n","1.  **Short Answer:** In your own words, what is the primary goal of data analysis?\n","2.  **MCQ:** Which of the following is NOT a typical stage in the data analysis process?\n","    * A) Data Collection\n","    * B) Software Development\n","    * C) Data Cleaning\n","    * D) Interpretation & Communication\n","3.  **List:** Name three key advantages of using Python for data analysis.\n","4.  **Fill-in-the-Blanks:** The Pandas library's primary 2D data structure is called a \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_, while its 1D data structure is called a \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_.\n","5.  **MCQ:** Which Python library is fundamental for numerical computations and provides the `ndarray` object?\n","    * A) Pandas\n","    * B) Matplotlib\n","    * C) NumPy\n","    * D) Scikit-learn\n","6.  **Matching:** Match the library with its primary purpose:\n","    * Matplotlib         -> A) Machine Learning\n","    * Seaborn            -> B) Statistical Modeling, Detailed Output\n","    * Scikit-learn       -> C) Foundational Plotting\n","    * Statsmodels        -> D) High-level Statistical Visualization\n","7.  **Short Answer:** Why are virtual environments recommended for Python projects?\n","8.  **Command:** Write the `pip` command to install the Pandas and NumPy libraries.\n","9.  **Command:** Write the `conda` command to create a new environment named `data_analysis_env` with Python version 3.10.\n","10. **True/False:** Python's syntax is generally considered more verbose and harder to read than languages like Java or C++.\n","11. **Short Answer:** What is the role of the `pyplot` module in Matplotlib?\n","12. **MCQ:** Which library would you primarily use if you wanted to create an interactive heatmap of a correlation matrix with minimal code?\n","    * A) NumPy\n","    * B) Seaborn\n","    * C) Statsmodels\n","    * D) Base Python\n","13. **Explain:** What does it mean when it's said that critical code paths in Pandas are written in \"Cython or C\"?\n","14. **Short Answer:** Give an example of how Pandas' \"Group By Engine\" might be used.\n","15. **True/False:** Anaconda is a Python library, similar to Pandas or NumPy.\n","\n"],"metadata":{"id":"-9SF2ZAGmPVv"}},{"cell_type":"markdown","source":["---\n","\n","**Module 2: Pandas Fundamentals - Data Structures and Basic Operations**\n","\n","This module introduces the core data structures in Pandas, `DataFrame` and `Series`, and covers essential operations for loading, inspecting, selecting, and saving data.\n","\n","**2.1 Pandas Data Structures: `DataFrame` and `Series`**\n","\n","* **`Series`:**\n","    * **Definition:** A Pandas `Series` is a one-dimensional labeled array capable of holding data of any type (integers, strings, floating-point numbers, Python objects, etc.). The axis labels are collectively referred to as the **index**. You can think of a `Series` as a single column in a spreadsheet or a single column from a SQL table.\n","    * **Key Characteristics:**\n","        * One-dimensional.\n","        * Homogeneous data (typically, though can hold mixed types if `dtype=object`).\n","        * Labeled index (can be numbers, strings, dates, etc.).\n","        * ***Size-immutable (length cannot be changed once created, but values can be)***.\n","    * **Creation:**"],"metadata":{"id":"fSuUy5vfh4fI"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np"],"outputs":[],"execution_count":null,"metadata":{"id":"RsUtJvT_mPVz"}},{"cell_type":"code","source":["#From a Python List\n","\n","my_list = [10, 20, 30, 40, 50]\n","s_from_list = pd.Series(my_list, dtype='int64') #explicit dtype for clarity\n","print(\"Series from list:\\n\", s_from_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"36FqA6DNiE6z","executionInfo":{"status":"ok","timestamp":1747137978820,"user_tz":-330,"elapsed":54,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"2561dff1-983b-42c7-8dd4-9118666247c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Series from list:\n"," 0    10\n","1    20\n","2    30\n","3    40\n","4    50\n","dtype: int64\n"]}]},{"cell_type":"code","source":["#From a Python List with custom index\n","\n","s_list_custom_index = pd.Series([1, 3, 5, np.nan, 6, 8], index=['a', 'b', 'c', 'd', 'e', 'f'])\n","\n","print(\"\\nSeries from list with custom index:\\n\", s_list_custom_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zQ4flW1vif46","executionInfo":{"status":"ok","timestamp":1747138109427,"user_tz":-330,"elapsed":33,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"937c4bb7-0e45-4dd0-a561-fd3acb709081"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Series from list with custom index:\n"," a    1.0\n","b    3.0\n","c    5.0\n","d    NaN\n","e    6.0\n","f    8.0\n","dtype: float64\n"]}]},{"cell_type":"code","source":["# From a NumPy array with a specified index\n","s_numpy = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e'])\n","print(\"\\nSeries from NumPy array with index:\\n\", s_numpy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OPipbBXpi--V","executionInfo":{"status":"ok","timestamp":1747138215199,"user_tz":-330,"elapsed":9,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"c319a905-7f81-4d79-ae21-14ce61d30767"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Series from NumPy array with index:\n"," a   -0.578423\n","b    0.465654\n","c   -0.103389\n","d   -1.081938\n","e    0.507545\n","dtype: float64\n"]}]},{"cell_type":"code","source":["# From a Python dictionary\n","s_dict_data = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}\n","\n","s_dict = pd.Series(s_dict_data, dtype='int64')  # Explicit dtype for clarity\n","print(\"\\nSeries from dictionary:\\n\", s_dict)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qOiBDZORjYGe","executionInfo":{"status":"ok","timestamp":1747138312852,"user_tz":-330,"elapsed":17,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"27b47911-a656-4fbb-e6d0-d5d74b5815e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Series from dictionary:\n"," Ohio      35000\n","Texas     71000\n","Oregon    16000\n","Utah       5000\n","dtype: int64\n"]}]},{"cell_type":"markdown","source":["* **Explanation:**\n","            * The first example creates a `Series` from a list. Pandas automatically assigns a default integer index starting from 0.\n","            * The second example shows creating a `Series` with a custom index provided by a list of labels. `np.nan` represents a missing value.\n","            * The third example uses a NumPy array for data and specifies an index.\n","            * The fourth example demonstrates creating a `Series` from a Python dictionary. The dictionary keys are used as the index labels, and the dictionary values become the data for the `Series`.\n","\n"],"metadata":{"id":"_WnRYNxZmPV0"}},{"cell_type":"markdown","source":["* **`DataFrame`:**\n","    * **Definition:** A Pandas `DataFrame` is a two-dimensional, **size-mutable (can add/remove columns/rows),** and potentially heterogeneous (columns can have different data types) tabular data structure with labeled axes (rows and columns).** It's the most commonly used Pandas object and can be thought of as a dictionary of `Series` objects, an in-memory spreadsheet, a SQL table, or a structured collection of data.**\n","      \n","    * **Key Characteristics:**\n","        * Two-dimensional (rows and columns).\n","        * Can hold columns of different data types.\n","        * Labeled rows (index) and labeled columns.\n","        * Size-mutable (can change shape).\n","    * **Creation:**"],"metadata":{"id":"f4lxxI5Dj5Es"}},{"cell_type":"code","source":["\n","# From a dictionary of lists or Series\n","data_dict = {'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n","             'Age': [25, 30, 35, 28],\n","             'City': ['New York', 'Paris', 'London', 'Tokyo']}\n","\n","df_from_dict = pd.DataFrame(data_dict)\n","print(\"DataFrame from dictionary of lists:\\n\", df_from_dict)"],"outputs":[{"output_type":"stream","name":"stdout","text":["DataFrame from dictionary of lists:\n","       Name  Age      City\n","0    Alice   25  New York\n","1      Bob   30     Paris\n","2  Charlie   35    London\n","3    David   28     Tokyo\n"]}],"execution_count":null,"metadata":{"id":"0WhgFwrJmPV0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747138623675,"user_tz":-330,"elapsed":15,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"16e37c5c-85d1-41ae-e289-3c5c74a0172c"}},{"cell_type":"code","source":["# From a list of dictionaries\n","data_list_dict = [{'Name': 'Eve', 'Age': 22, 'City': 'Berlin'},\n","                  {'Name': 'Frank', 'Age': 29, 'City': 'Madrid', 'Occupation': 'Engineer'}]\n","\n","df_from_list_dict = pd.DataFrame(data_list_dict)  # Pandas handles missing keys by inserting NaN\n","print(\"\\nDataFrame from list of dictionaries:\\n\", df_from_list_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iRISsGFClMi0","executionInfo":{"status":"ok","timestamp":1747138814831,"user_tz":-330,"elapsed":10,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"ef55f696-6dd7-4cda-f922-7337d28b2cee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","DataFrame from list of dictionaries:\n","     Name  Age    City Occupation\n","0    Eve   22  Berlin        NaN\n","1  Frank   29  Madrid   Engineer\n"]}]},{"cell_type":"code","source":["# From a NumPy array, with a datetime index and labeled columns\n","dates = pd.date_range('2025-01-01', periods=6)  # Creates 6 dates starting from Jan 1, 2025\n","random_data = np.random.randn(6, 4)  # 6x4 array of random numbers\n","\n","df_from_numpy = pd.DataFrame(random_data, index=dates, columns=['A', 'B', 'C', 'D'])\n","print(\"\\nDataFrame from NumPy array with datetime index and labeled columns:\\n\", df_from_numpy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_SwXp47vlrjz","executionInfo":{"status":"ok","timestamp":1747138880382,"user_tz":-330,"elapsed":14,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"8afee9da-c229-48f4-a8c4-ab831316e260"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","DataFrame from NumPy array with datetime index and labeled columns:\n","                    A         B         C         D\n","2025-01-01 -0.714697  0.446932 -1.195579 -0.461952\n","2025-01-02 -2.442862  0.089916 -1.301305 -1.636605\n","2025-01-03 -0.743951 -0.902569 -1.504037 -0.742583\n","2025-01-04  0.184980  1.884691 -0.569112 -1.520992\n","2025-01-05 -0.554921  0.329456  2.055096 -0.484093\n","2025-01-06 -0.041739 -1.491716  0.037951 -0.693801\n"]}]},{"cell_type":"markdown","source":["* **Explanation:**\n","            * The first example constructs a `DataFrame` from a dictionary where keys are column names and values are lists representing column data.\n","            * The second example uses a list of dictionaries, where each dictionary forms a row. Pandas intelligently handles missing keys by filling with `NaN`.\n","            * The third example shows creating a `DataFrame` from a 2D NumPy array, explicitly providing row index labels (here, dates) and column labels.\n","            "],"metadata":{"id":"yRsCMJX8mPV1"}},{"cell_type":"markdown","source":["\n","\n","\n","**2.2 Loading Data into Pandas DataFrames**\n","\n","Pandas excels at reading data from various file formats into DataFrames.\n","\n","* **Definition of a Dataset:**\n","    * **Understandable Definition:** A dataset is simply a collection of data, usually organized in a structured way (like a table) so that it can be easily accessed, managed, and analyzed. Think of it as the raw material for your data analysis project.\n","\n","* **CSV File Format:**\n","    * **Understandable Definition:** A CSV (Comma-Separated Values) file is a plain text file where data is stored in a tabular format. Each line in the file represents a row of data, and the values within each row are separated by commas (or sometimes other characters like semicolons or tabs). It's a very common and simple format for exchanging tabular data.\n","\n","* **Header Row:**\n","    * **Understandable Definition:** In a tabular data file (like a CSV or spreadsheet), the header row is the first row that contains labels or names for each column. These labels make the data understandable by providing context for what each column represents."],"metadata":{"id":"IbQHF0b-mBoe"}},{"cell_type":"markdown","source":["\n","* **`pd.read_csv()` Function:**\n","    * **Purpose:** This is the primary Pandas function for reading data from a CSV file into a DataFrame.\n","\n","    * **Common Parameters:**\n","        * `filepath_or_buffer`: The path to the CSV file (as a string, e.g., `'data.csv'`, `'C:/Users/Me/Documents/data.csv'`) or a URL.\n","        * `sep` (or `delimiter`): The character used to separate values in the file. Default is `','`. Use `sep='\\t'` for tab-separated files, or `sep=';'` for semicolon-separated files. Pandas often auto-detects this if not `','`.\n","        * `header`: Specifies which row number to use as column names (0-indexed). Default is `0` (first row). If your file has no header, use `header=None`.\n","        * `names`: A list of column names to use. If you provide `names` and the file has a header, you should also specify `header=0` to overwrite the file's header or `header=None` if the file has no header and you're providing names.\n","        * `index_col`: Column(s) to use as the row labels (index) of the DataFrame. Can be an integer (column position) or a string (column name).\n","        * `usecols`: Allows you to read only a subset of columns, specified by name or position. E.g., `usecols=['Name', 'Age']`.\n","        * `skiprows`: Number of lines to skip at the beginning of the file, or a list of specific row numbers (0-indexed) to skip.\n","        * `nrows`: Number of rows to read from the file. Useful for reading just a sample of a very large file.\n","    * **Reading CSV with Header (Default Behavior):**\n","        * If your CSV file has a header row (column names in the first line), `pd.read_csv()` will usually detect and use it automatically."],"metadata":{"id":"wXca4aX7mPT4"}},{"cell_type":"code","source":["# Create a dummy CSV file with a header for demonstration\n","csv_data_with_header = \"\"\"Name,Age,City\n","Alice,25,New York\n","Bob,30,London\n","Carol,22,Paris\n","\"\"\""],"outputs":[],"execution_count":19,"metadata":{"id":"lPvVRWMGmPV1","executionInfo":{"status":"ok","timestamp":1747139158272,"user_tz":-330,"elapsed":16,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}}}},{"cell_type":"code","source":["# Write CSV data to a file\n","with open('data_with_header.csv', 'w') as f:\n","    f.write(csv_data_with_header)"],"metadata":{"id":"mf2HU2adm-dD","executionInfo":{"status":"ok","timestamp":1747139227207,"user_tz":-330,"elapsed":21,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Read CSV file\n","try:\n","    df_with_header = pd.read_csv('data_with_header.csv', index_col=False)\n","    print(\"DataFrame with header (inferred):\\n\", df_with_header)\n","except FileNotFoundError:\n","    print(\"Error: data_with_header.csv not found. Ensure it's created in the same directory or provide the full path.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tA1agBNVnS17","executionInfo":{"status":"ok","timestamp":1747139249117,"user_tz":-330,"elapsed":53,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"89f13907-89bc-45d6-ea71-62ae31fd5e7e"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["DataFrame with header (inferred):\n","     Name  Age      City\n","0  Alice   25  New York\n","1    Bob   30    London\n","2  Carol   22     Paris\n"]}]},{"cell_type":"markdown","source":["* **Reading CSV without Header:**\n","        * If the CSV file lacks a header row, you must tell Pandas by setting `header=None`. You can then let Pandas assign default integer column names (0, 1, 2, ...) or provide your own using the `names` parameter."],"metadata":{"id":"PRudIXdOmPV2"}},{"cell_type":"code","source":["# Create a dummy CSV file without a header\n","csv_data_no_header = \"\"\"Dave,35,Berlin\n","Eve,29,Madrid\n","Frank,40,Rome\n","\"\"\""],"metadata":{"id":"dBeTMOXsoRtp","executionInfo":{"status":"ok","timestamp":1747139502315,"user_tz":-330,"elapsed":25,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["with open('data_no_header.csv', 'w') as f:\n","    f.write(csv_data_no_header)\n","\n","try:\n","    # Method 1: Let Pandas assign default integer column names\n","    df_no_header_default_names = pd.read_csv('data_no_header.csv', header=None)\n","    print(\"\\nDataFrame without header (default names):\\n\", df_no_header_default_names)\n","\n","    # Method 2: Assign custom column names\n","    column_names = ['FullName', 'UserAge', 'UserCity']\n","    df_no_header_custom_names = pd.read_csv('data_no_header.csv', header=None, names=column_names)\n","    print(\"\\nDataFrame without header (custom names):\\n\", df_no_header_custom_names)\n","\n","except FileNotFoundError:\n","    print(\"data_no_header.csv not found. Ensure it's created or provide the full path.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ToP8pYPJoTaP","executionInfo":{"status":"ok","timestamp":1747139509598,"user_tz":-330,"elapsed":19,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"840e06bb-6dda-42df-f13e-017aa14b6864"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","DataFrame without header (default names):\n","        0   1       2\n","0   Dave  35  Berlin\n","1    Eve  29  Madrid\n","2  Frank  40    Rome\n","\n","DataFrame without header (custom names):\n","   FullName  UserAge UserCity\n","0     Dave       35   Berlin\n","1      Eve       29   Madrid\n","2    Frank       40     Rome\n"]}]},{"cell_type":"markdown","source":["**2.3 Inspecting DataFrames**\n","\n","Once data is loaded, the first step is to get a feel for its structure and content.\n","\n","* **`.head(n=5)` and `.tail(n=5)`:**\n","    * `df.head(n)`: Returns the first `n` rows of the DataFrame. Default is 5. Excellent for a quick peek at the beginning of your data. *[27]*\n","    * `df.tail(n)`: Returns the last `n` rows of the DataFrame. Default is 5. Useful for checking the end of your data, especially after sorting or appending. *[27], [28]*"],"metadata":{"id":"THbIbwZCmPV3"}},{"cell_type":"code","source":["\n","# Using df_with_header from the previous example (assuming it was loaded successfully)\n","if 'df_with_header' in locals():\n","    print(\"First 2 rows (head):\\n\", df_with_header.head(2))\n","    # Expected Output:\n","    #     Name  Age      City\n","    # 0  Alice   25  New York\n","    # 1    Bob   30    London\n","\n","    print(\"\\nLast 2 rows (tail):\\n\", df_with_header.tail(2))\n","    # Expected Output:\n","    #     Name  Age    City\n","    # 1    Bob   30  London\n","    # 2  Carol   22   Paris\n","else:\n","    # Create a sample DataFrame for inspection if previous failed\n","    data_inspect = {\n","        'colA': range(10),\n","        'colB': [chr(i) for i in range(ord('a'), ord('a')+10)],\n","        'colC': [1.0, 2.5, np.nan, 4.2, 5.1, np.nan, 7.8, 8.9, 9.0, 10.3]\n","    }\n","\n","    df_inspect = pd.DataFrame(data_inspect)\n","    print(\"Sample DataFrame for inspection:\\n\", df_inspect.head())\n","\n","    print(\"\\nFirst 3 rows (head):\\n\", df_inspect.head(3))\n","    print(\"\\nLast 2 rows (tail):\\n\", df_inspect.tail(2))\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["First 2 rows (head):\n","     Name  Age      City\n","0  Alice   25  New York\n","1    Bob   30    London\n","\n","Last 2 rows (tail):\n","     Name  Age    City\n","1    Bob   30  London\n","2  Carol   22   Paris\n"]}],"execution_count":28,"metadata":{"id":"mJu8r4-UmPV3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747139602021,"user_tz":-330,"elapsed":11,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"68ab1662-10d9-4c00-d14b-4dda4b8bffcd"}},{"cell_type":"markdown","source":["* **`.info()`:**\n","    * Provides a concise summary of a DataFrame, including:\n","        * The data type of the index.\n","        * The data type of each column (`Dtype`).\n","        * The number of non-null (non-missing) values in each column (`Non-Null Count`).\n","        * Memory usage of the DataFrame.\n","        * *[29]*\n","    * **Output Interpretation:** Extremely useful for quickly spotting columns with missing data or incorrect data types."],"metadata":{"id":"4NGTEbjEmPV4"}},{"cell_type":"code","source":["# Create a sample DataFrame for inspection with mixed types and missing values\n","data_info = {'ID': [1, 2, 3, 4, 5],\n","               'Product': ['Apple', 'Banana', 'Orange', np.nan, 'Apple'],\n","               'Price': [0.5, 0.25, np.nan, 1.0, 0.55],\n","               'Quantity': [10, 15, 8, 12, 20]}\n","df_for_info = pd.DataFrame(data_info)\n","df_for_info['OrderDate'] = pd.to_datetime(['2025-01-01', '2025-01-01', '2025-01-02', '2025-01-03', '2025-01-03'])\n","\n","print(\"\\nDataFrame info:\")\n","df_for_info.info()"],"outputs":[{"output_type":"stream","name":"stdout","text":["\n","DataFrame info:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 5 entries, 0 to 4\n","Data columns (total 5 columns):\n"," #   Column     Non-Null Count  Dtype         \n","---  ------     --------------  -----         \n"," 0   ID         5 non-null      int64         \n"," 1   Product    4 non-null      object        \n"," 2   Price      4 non-null      float64       \n"," 3   Quantity   5 non-null      int64         \n"," 4   OrderDate  5 non-null      datetime64[ns]\n","dtypes: datetime64[ns](1), float64(1), int64(2), object(1)\n","memory usage: 332.0+ bytes\n"]}],"execution_count":29,"metadata":{"id":"6rXhPlMMmPV4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747139646344,"user_tz":-330,"elapsed":42,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"179f8c2c-f51a-46a7-d93b-3af791ac6d40"}},{"cell_type":"markdown","source":["* **`.describe()`:**\n","    * Generates descriptive statistics of the DataFrame. *[31]*\n","    * For **numerical columns** (default): count, mean, standard deviation (std), min, 25th percentile (Q1), 50th percentile (median/Q2), 75th percentile (Q3), and max.\n","    * For **categorical/object columns** (if `include='object'` or `include='all'` is used): count, unique (number of distinct values), top (most frequent value/mode), and freq (frequency of the top value).\n","    * `include='all'` will show statistics for all columns, using NaN for metrics that don't apply to a particular data type."],"metadata":{"id":"gaI2Rz-ImPV4"}},{"cell_type":"code","source":["# Sample data creation (assuming df_for_info doesn't exist)\n","data_info = {\n","    'ID': [1, 2, 3, 4, 5],\n","    'Product': ['Apple', 'Banana', 'Orange', np.nan, 'Apple'],\n","    'Price': [0.5, 0.25, np.nan, 1.0, 0.55],\n","    'Quantity': [10, 15, 8, 12, 20],\n","    'OrderDate': pd.to_datetime(['2025-01-01', '2025-01-01', '2025-01-02', '2025-01-03', '2025-01-03'])\n","}\n","df_for_info = pd.DataFrame(data_info)"],"outputs":[],"execution_count":35,"metadata":{"id":"gWBlCXTtmPV4","executionInfo":{"status":"ok","timestamp":1747139794150,"user_tz":-330,"elapsed":16,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}}}},{"cell_type":"code","source":["# Descriptive statistics for numerical columns\n","print(\"\\nDescriptive statistics for numerical columns of df_for_info:\\n\", df_for_info.describe())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6uQ4uMDjpe_Q","executionInfo":{"status":"ok","timestamp":1747139818983,"user_tz":-330,"elapsed":10,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"f6dbfb5e-420f-427e-dcaa-f0bd6eeebe31"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Descriptive statistics for numerical columns of df_for_info:\n","              ID    Price   Quantity            OrderDate\n","count  5.000000  4.00000   5.000000                    5\n","mean   3.000000  0.57500  13.000000  2025-01-02 00:00:00\n","min    1.000000  0.25000   8.000000  2025-01-01 00:00:00\n","25%    2.000000  0.43750  10.000000  2025-01-01 00:00:00\n","50%    3.000000  0.52500  12.000000  2025-01-02 00:00:00\n","75%    4.000000  0.66250  15.000000  2025-01-03 00:00:00\n","max    5.000000  1.00000  20.000000  2025-01-03 00:00:00\n","std    1.581139  0.31225   4.690416                  NaN\n"]}]},{"cell_type":"code","source":["# Descriptive statistics for object columns\n","print(\"\\nDescriptive statistics for object columns of df_for_info:\\n\", df_for_info.describe(include=['object']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dfgqQFtjphiN","executionInfo":{"status":"ok","timestamp":1747139868123,"user_tz":-330,"elapsed":26,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"bc0fe039-5bc2-4b63-fb1a-fef343ceb1b0"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Descriptive statistics for object columns of df_for_info:\n","        Product\n","count        4\n","unique       3\n","top      Apple\n","freq         2\n"]}]},{"cell_type":"code","source":["# Descriptive statistics for all columns\n","print(\"\\nDescriptive statistics for all columns of df_for_info:\\n\", df_for_info.describe(include='all'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-g1dBPe6pjY9","executionInfo":{"status":"ok","timestamp":1747139893934,"user_tz":-330,"elapsed":57,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"219e6248-0041-4a87-8686-e053ce62868f"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Descriptive statistics for all columns of df_for_info:\n","               ID Product    Price   Quantity            OrderDate\n","count   5.000000       4  4.00000   5.000000                    5\n","unique       NaN       3      NaN        NaN                  NaN\n","top          NaN   Apple      NaN        NaN                  NaN\n","freq         NaN       2      NaN        NaN                  NaN\n","mean    3.000000     NaN  0.57500  13.000000  2025-01-02 00:00:00\n","min     1.000000     NaN  0.25000   8.000000  2025-01-01 00:00:00\n","25%     2.000000     NaN  0.43750  10.000000  2025-01-01 00:00:00\n","50%     3.000000     NaN  0.52500  12.000000  2025-01-02 00:00:00\n","75%     4.000000     NaN  0.66250  15.000000  2025-01-03 00:00:00\n","max     5.000000     NaN  1.00000  20.000000  2025-01-03 00:00:00\n","std     1.581139     NaN  0.31225   4.690416                  NaN\n"]}]},{"cell_type":"markdown","source":["* **Other Attributes for Basic Inspection:**\n","    * `df.shape`: Returns a tuple `(number_of_rows, number_of_columns)`.\n","    * `df.dtypes`: Returns a `Series` showing the data type of each column.\n","    * `df.columns`: Returns an `Index` object containing the column labels.\n","    * `df.index`: Returns an `Index` object containing the row labels."],"metadata":{"id":"HGr5nxhXmPV5"}},{"cell_type":"code","source":["df_for_info = pd.DataFrame(data_info)\n","\n","# Print DataFrame properties\n","print(\"\\nShape of df_for_info:\", df_for_info.shape)  # Output: (5, 5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-4LAjHpAo7ji","executionInfo":{"status":"ok","timestamp":1747140060609,"user_tz":-330,"elapsed":347,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"28473ea9-a975-4552-fd87-005311822763"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Shape of df_for_info: (5, 5)\n"]}]},{"cell_type":"code","source":["print(\"\\nData types of columns in df_for_info:\\n\", df_for_info.dtypes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WTXGJ6skqgB7","executionInfo":{"status":"ok","timestamp":1747140089260,"user_tz":-330,"elapsed":21,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"2029549c-c28b-4f0a-f1dd-696ddec62214"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Data types of columns in df_for_info:\n"," ID                    int64\n","Product              object\n","Price               float64\n","Quantity              int64\n","OrderDate    datetime64[ns]\n","dtype: object\n"]}]},{"cell_type":"code","source":["print(\"\\nColumn labels of df_for_info:\", df_for_info.columns)\n","# Expected Output: Index(['ID', 'Product', 'Price', 'Quantity', 'OrderDate'], dtype='object')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FxuAYk4OqjEp","executionInfo":{"status":"ok","timestamp":1747140098839,"user_tz":-330,"elapsed":11,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"2cbc2c60-ed27-4d79-d3ac-66f90e6f1174"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Column labels of df_for_info: Index(['ID', 'Product', 'Price', 'Quantity', 'OrderDate'], dtype='object')\n"]}]},{"cell_type":"code","source":["print(\"\\nIndex (row labels) of df_for_info:\", df_for_info.index)\n","# Expected Output: RangeIndex(start=0, stop=5, step=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gf1dC8ouqlb7","executionInfo":{"status":"ok","timestamp":1747140141116,"user_tz":-330,"elapsed":46,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"5c73751c-efcb-4cc9-96a5-35c132aee1a5"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Index (row labels) of df_for_info: RangeIndex(start=0, stop=5, step=1)\n"]}]},{"cell_type":"markdown","source":["**2.4 Data Selection and Indexing**\n","\n","Pandas offers powerful ways to select specific subsets of your data.\n","\n","* **Selecting Columns:**\n","    * Using square brackets `[]`:\n","        * `df['column_name']`: Selects a single column as a `Series`.\n","        * `df[['col1_name', 'col2_name']]`: Selects multiple columns as a new `DataFrame`. Note the inner list."],"metadata":{"id":"MvWLM-rnmPV5"}},{"cell_type":"code","source":["# Using df_for_info\n","selected_product_series = df_for_info['Product']\n","print(\"\\nSelecting column 'Product' (as Series):\\n\", selected_product_series)"],"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Selecting column 'Product' (as Series):\n"," 0     Apple\n","1    Banana\n","2    Orange\n","3       NaN\n","4     Apple\n","Name: Product, dtype: object\n"]}],"execution_count":53,"metadata":{"id":"Sr2UwVsnmPV5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747140165430,"user_tz":-330,"elapsed":11,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"779f90ee-0bc4-418c-d6d2-8189c643015e"}},{"cell_type":"code","source":["selected_cols_df = df_for_info[['Product', 'Price']]\n","print(\"\\nSelecting columns 'Product' and 'Price' (as DataFrame):\\n\", selected_cols_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dxJiesXZq11N","executionInfo":{"status":"ok","timestamp":1747140181937,"user_tz":-330,"elapsed":10,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"b10d0171-b471-4a85-9b8b-80f9395beab0"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Selecting columns 'Product' and 'Price' (as DataFrame):\n","   Product  Price\n","0   Apple   0.50\n","1  Banana   0.25\n","2  Orange    NaN\n","3     NaN   1.00\n","4   Apple   0.55\n"]}]},{"cell_type":"markdown","source":["* **Selecting Rows with `.loc[]` (Label-based):**\n","    * Accesses a group of rows and columns by **labels** (index names, column names).\n","    * `df.loc[row_label]`: Selects a single row (returns a `Series`).\n","    * `df.loc[[row_label1, row_label2]]`: Selects multiple rows by label (returns a `DataFrame`).\n","    * `df.loc[start_label:end_label]`: Slice of rows (inclusive of both start and end labels).\n","    * `df.loc[:, 'column_name']`: Selects all rows for a specific column (returns a `Series`).\n","    * `df.loc[row_label_slice, column_label_slice]`: Selects a specific block of data.\n","    * `df.loc[boolean_condition]`: Selects rows where the boolean condition is `True`."],"metadata":{"id":"sfBOlobImPV5"}},{"cell_type":"code","source":["# Set a custom index for df_for_info for better .loc demonstration\n","df_loc_demo = df_for_info.copy()\n","df_loc_demo.index = ['Order1', 'Order2', 'Order3', 'Order4', 'Order5']\n","print(\"\\nDataFrame with custom index for .loc demo:\\n\", df_loc_demo)"],"outputs":[{"output_type":"stream","name":"stdout","text":["\n","DataFrame with custom index for .loc demo:\n","         ID Product  Price  Quantity  OrderDate\n","Order1   1   Apple   0.50        10 2025-01-01\n","Order2   2  Banana   0.25        15 2025-01-01\n","Order3   3  Orange    NaN         8 2025-01-02\n","Order4   4     NaN   1.00        12 2025-01-03\n","Order5   5   Apple   0.55        20 2025-01-03\n"]}],"execution_count":57,"metadata":{"id":"WBuWr8mEmPV6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747140299929,"user_tz":-330,"elapsed":9,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"1396f49f-d9b2-4414-bb59-6ffd9e3559a8"}},{"cell_type":"code","source":["# Selecting row with index 'Order2'\n","print(\"\\nSelecting row with index 'Order2' using .loc:\\n\", df_loc_demo.loc['Order2'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbAvQhkQrXK8","executionInfo":{"status":"ok","timestamp":1747140311106,"user_tz":-330,"elapsed":9,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"5d6e9dcc-a0a4-4ebe-8f68-2d7facce15d3"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Selecting row with index 'Order2' using .loc:\n"," ID                             2\n","Product                   Banana\n","Price                       0.25\n","Quantity                      15\n","OrderDate    2025-01-01 00:00:00\n","Name: Order2, dtype: object\n"]}]},{"cell_type":"code","source":["# Selecting rows 'Order1' to 'Order3'\n","print(\"\\nSelecting rows 'Order1' to 'Order3' using .loc:\\n\", df_loc_demo.loc['Order1':'Order3'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RrCF5QXJraxH","executionInfo":{"status":"ok","timestamp":1747140325872,"user_tz":-330,"elapsed":9,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"114f1105-d9cf-4473-9198-2aa9c48e8f4b"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Selecting rows 'Order1' to 'Order3' using .loc:\n","         ID Product  Price  Quantity  OrderDate\n","Order1   1   Apple   0.50        10 2025-01-01\n","Order2   2  Banana   0.25        15 2025-01-01\n","Order3   3  Orange    NaN         8 2025-01-02\n"]}]},{"cell_type":"code","source":["# Selecting 'Product' and 'Quantity' for rows 'Order2' and 'Order4'\n","print(\"\\nSelecting 'Product' and 'Quantity' for rows 'Order2' and 'Order4':\\n\",\n","      df_loc_demo.loc[['Order2', 'Order4'], ['Product', 'Quantity']])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NhfJbDMBrcan","executionInfo":{"status":"ok","timestamp":1747140332459,"user_tz":-330,"elapsed":10,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"b6fb31c9-0c6a-493a-b328-30969ad46d21"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Selecting 'Product' and 'Quantity' for rows 'Order2' and 'Order4':\n","        Product  Quantity\n","Order2  Banana        15\n","Order4     NaN        12\n"]}]},{"cell_type":"code","source":["# Selecting rows where Quantity > 10\n","print(\"\\nSelecting rows where Quantity > 10 using .loc:\\n\",\n","      df_loc_demo.loc[df_loc_demo['Quantity'] > 10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"se7JBUbYreDc","executionInfo":{"status":"ok","timestamp":1747140339205,"user_tz":-330,"elapsed":6,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"840df80d-940e-48c3-f23a-991187ede6c3"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Selecting rows where Quantity > 10 using .loc:\n","         ID Product  Price  Quantity  OrderDate\n","Order2   2  Banana   0.25        15 2025-01-01\n","Order4   4     NaN   1.00        12 2025-01-03\n","Order5   5   Apple   0.55        20 2025-01-03\n"]}]},{"cell_type":"markdown","source":["* **Selecting Rows with `.iloc[]` (Integer Position-based):**\n","    * Accesses a group of rows and columns by **integer positions** (0-indexed).\n","    * `df.iloc[row_pos]` Selects a single row by its integer position (returns a `Series`).\n","    * `df.iloc[[row_pos1, row_pos2]]`: Selects multiple rows by integer position (returns a `DataFrame`).\n","    * `df.iloc[start_pos:end_pos]`: Slice of rows (exclusive of `end_pos`, like Python list slicing).\n","    * `df.iloc[:, col_pos]`: Selects all rows for a specific column by position (returns a `Series`).\n","    * `df.iloc[row_pos_slice, col_pos_slice]`: Selects a specific block of data by position."],"metadata":{"id":"iTVWTAFtmPV6"}},{"cell_type":"code","source":["# Using df_for_info (which has default integer index)\n","print(\"\\nSelecting row at position 0 using .iloc:\\n\", df_for_info.iloc[0])"],"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Selecting row at position 0 using .iloc:\n"," ID                             1\n","Product                    Apple\n","Price                        0.5\n","Quantity                      10\n","OrderDate    2025-01-01 00:00:00\n","Name: 0, dtype: object\n"]}],"execution_count":62,"metadata":{"id":"_Ks-O4ufmPV6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747140533700,"user_tz":-330,"elapsed":10,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"8b7f173d-48ee-45d9-87da-e5389f80a8ee"}},{"cell_type":"code","source":["print(\"\\nSelecting rows at positions 0 to 2 (exclusive of 2) using .iloc:\\n\", df_for_info.iloc[0:2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"THJh60lNsPRL","executionInfo":{"status":"ok","timestamp":1747140553037,"user_tz":-330,"elapsed":49,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"1f97967d-2cbe-4bbd-d661-ad02288adba1"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Selecting rows at positions 0 to 2 (exclusive of 2) using .iloc:\n","    ID Product  Price  Quantity  OrderDate\n","0   1   Apple   0.50        10 2025-01-01\n","1   2  Banana   0.25        15 2025-01-01\n"]}]},{"cell_type":"code","source":["print(\"\\nSelecting first 3 rows and first 2 columns (positions 0, 1) using .iloc:\\n\", df_for_info.iloc[0:3, 0:2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nu-pP0w2sT4L","executionInfo":{"status":"ok","timestamp":1747140559735,"user_tz":-330,"elapsed":8,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"f2ec3f5e-2935-434c-a44e-32d5ef045852"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Selecting first 3 rows and first 2 columns (positions 0, 1) using .iloc:\n","    ID Product\n","0   1   Apple\n","1   2  Banana\n","2   3  Orange\n"]}]},{"cell_type":"code","source":["print(\"\\nSelecting specific rows [0, 2, 4] and columns [1, 3] using .iloc:\\n\", df_for_info.iloc[[0, 2, 4], [1, 3]])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lAOv04tIsVRt","executionInfo":{"status":"ok","timestamp":1747140569135,"user_tz":-330,"elapsed":10,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"06073a87-639a-40ae-861f-74d6c65c1b97"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Selecting specific rows [0, 2, 4] and columns [1, 3] using .iloc:\n","   Product  Quantity\n","0   Apple        10\n","2  Orange         8\n","4   Apple        20\n"]}]},{"cell_type":"markdown","source":["## Difference Between `df.loc` and `df.iloc`\n","\n","| Feature       | `df.loc` (Label-based) | `df.iloc` (Position-based) |\n","|--------------|----------------------|--------------------------|\n","| **Row Selection** | Uses **index labels** | Uses **integer positions** |\n","| **Column Selection** | Uses **column names** | Uses **integer positions** |\n","| **Slicing Behavior** | Includes both endpoints | Excludes the endpoint (Python slicing behavior) |\n","| **Can use conditions?** | ✅ Yes | ❌ No |\n","\n","### Summary:\n","- `df.loc` is **label-based**, meaning it selects rows and columns using explicit **names**.\n","- `df.iloc` is **position-based**, meaning it selects rows and columns using numerical **indices**.\n"],"metadata":{"id":"H0JvrQjJsFz-"}},{"cell_type":"markdown","source":["**2.5 Basic Operations on DataFrames**\n","\n","* **Adding Columns:**\n","    * **Direct Assignment:** `df['new_column_name'] = values`\n","        * `values` can be a scalar (same value for all rows), a list or NumPy array (must match DataFrame length), or a Pandas `Series` (will align by index).\n","    * **Using `assign()`:** `df_new = df.assign(new_col_name = expression)`\n","        * Creates a *new* DataFrame with the added column(s), leaving the original DataFrame unchanged.\n","        * Allows creating multiple columns at once.\n","        * Can use lambda functions to define new columns based on existing ones.\n","        "],"metadata":{"id":"reOlBAJ0mPV6"}},{"cell_type":"code","source":["# Create DataFrame\n","\n","df_ops = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [10, 20, 30, 40]})\n","print(\"Original DataFrame for ops:\\n\", df_ops)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Original DataFrame for ops:\n","    A   B\n","0  1  10\n","1  2  20\n","2  3  30\n","3  4  40\n"]}],"execution_count":89,"metadata":{"id":"Bl_lXy8BmPV6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747141853452,"user_tz":-330,"elapsed":16,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"d7070af4-1773-4c3f-b52b-ddffdf32b151"}},{"cell_type":"code","source":["# Direct assignment - new column 'C' as sum of A and B\n","\n","df_ops['C'] = df_ops['A'] + df_ops['B']\n","print(\"\\nAfter adding 'C' (direct assignment):\\n\", df_ops)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mj03rOAjs1VT","executionInfo":{"status":"ok","timestamp":1747141853476,"user_tz":-330,"elapsed":23,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"94145f3d-20ae-48ff-a2c1-9a552a7dfa4f"},"execution_count":90,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","After adding 'C' (direct assignment):\n","    A   B   C\n","0  1  10  11\n","1  2  20  22\n","2  3  30  33\n","3  4  40  44\n"]}]},{"cell_type":"code","source":["# Using assign() - new column 'D' as A * 2\n","# Original df_ops is NOT modified by assign unless reassigned\n","\n","df_ops_assigned = df_ops.assign(D=df_ops['A'] * 2)\n","\n","print(\"\\nNew DataFrame with 'D' (using assign):\\n\", df_ops_assigned)\n","print(\"\\nOriginal df_ops after assign (unchanged from C addition):\\n\", df_ops)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TSR_JlQ-s4Zc","executionInfo":{"status":"ok","timestamp":1747141853496,"user_tz":-330,"elapsed":19,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"d9143eeb-73a1-43b2-88e8-fafc6acb32ad"},"execution_count":91,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","New DataFrame with 'D' (using assign):\n","    A   B   C  D\n","0  1  10  11  2\n","1  2  20  22  4\n","2  3  30  33  6\n","3  4  40  44  8\n","\n","Original df_ops after assign (unchanged from C addition):\n","    A   B   C\n","0  1  10  11\n","1  2  20  22\n","2  3  30  33\n","3  4  40  44\n"]}]},{"cell_type":"code","source":["# Using assign() with lambda functions to create new columns E & F\n","\n","df_ops_lambda = df_ops.assign(\n","    E=lambda x: x['A'] - x['B'],  # Column 'E': Computes the difference A - B\n","    F=lambda x: x['C'] * 0.1  # Column 'F': Computes 10% of column 'C'\n",")\n","\n","print(\"\\nNew DataFrame with 'E' and 'F' (assign with lambda):\\n\", df_ops_lambda)\n","\n","# Explanation: `assign()` is often preferred in method chains for a more functional style.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-pT3dvNgs8BA","executionInfo":{"status":"ok","timestamp":1747141853505,"user_tz":-330,"elapsed":7,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"6a83b703-9993-4cfa-a5dd-11026efabba3"},"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","New DataFrame with 'E' and 'F' (assign with lambda):\n","    A   B   C   E    F\n","0  1  10  11  -9  1.1\n","1  2  20  22 -18  2.2\n","2  3  30  33 -27  3.3\n","3  4  40  44 -36  4.4\n"]}]},{"cell_type":"markdown","source":["* **Modifying Columns:**\n","    * Simply assign new values to an existing column: `df['existing_column'] = new_values`."],"metadata":{"id":"UYalNAeHmPV6"}},{"cell_type":"code","source":["print(\"\\nBefore modifying column 'A' in df_ops:\\n\", df_ops)\n","\n","df_ops['A'] = df_ops['A'] * 10\n","print(\"\\nAfter modifying column 'A' in df_ops:\\n\", df_ops)"],"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Before modifying column 'A' in df_ops:\n","    A   B   C\n","0  1  10  11\n","1  2  20  22\n","2  3  30  33\n","3  4  40  44\n","\n","After modifying column 'A' in df_ops:\n","     A   B   C\n","0  10  10  11\n","1  20  20  22\n","2  30  30  33\n","3  40  40  44\n"]}],"execution_count":93,"metadata":{"id":"_2tNUVoGmPV6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747141853770,"user_tz":-330,"elapsed":266,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"1f00a571-90ed-4363-8333-006e12041009"}},{"cell_type":"markdown","source":["* **Deleting Columns:**\n","    * **Using `del` keyword:** `del df['column_name']`\n","        * Modifies the DataFrame in-place.\n","    * **Using `pop()` method:** `popped_series = df.pop('column_name')`\n","        * Removes the column and returns it as a `Series`.\n","        * Modifies the DataFrame in-place.\n","    * **Using `drop()` method:** `df_new = df.drop(columns=['col1', 'col2'], inplace=False)`\n","        * More flexible. Can drop rows or columns.\n","        * `columns=['col_name']` or `labels=['col_name'], axis=1` to specify column(s).\n","        * `labels=['row_index'], axis=0` to specify row(s).\n","        * `inplace=False` (default): Returns a new DataFrame with the column(s) dropped. Original is unchanged.\n","        * `inplace=True`: Modifies the DataFrame directly and returns `None`."],"metadata":{"id":"faPOcW83mPV6"}},{"cell_type":"code","source":["# Work on a copy to avoid modifying the original DataFrame\n","df_to_delete_from = df_ops_lambda.copy()\n","print(\"\\nDataFrame before deletions:\\n\", df_to_delete_from)"],"outputs":[{"output_type":"stream","name":"stdout","text":["\n","DataFrame before deletions:\n","    A   B   C   E    F\n","0  1  10  11  -9  1.1\n","1  2  20  22 -18  2.2\n","2  3  30  33 -27  3.3\n","3  4  40  44 -36  4.4\n"]}],"execution_count":94,"metadata":{"id":"HgKTDymmmPV6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747141853771,"user_tz":-330,"elapsed":229,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"ef5ade5b-f370-414c-ec34-90d15be70dd6"}},{"cell_type":"code","source":["# Using del (removes column permanently)\n","if 'F' in df_to_delete_from.columns:\n","    del df_to_delete_from['F']\n","    print(\"\\nAfter deleting 'F' using del:\\n\", df_to_delete_from)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zZwnw1DGuhj3","executionInfo":{"status":"ok","timestamp":1747141853775,"user_tz":-330,"elapsed":224,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"e6a3854f-7e8a-4c8f-ee56-41ae2c538eca"},"execution_count":95,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","After deleting 'F' using del:\n","    A   B   C   E\n","0  1  10  11  -9\n","1  2  20  22 -18\n","2  3  30  33 -27\n","3  4  40  44 -36\n"]}]},{"cell_type":"code","source":["# Using pop() (removes column and returns it as a Series)\n","if 'E' in df_to_delete_from.columns:\n","    e_series = df_to_delete_from.pop('E')\n","    print(\"\\nAfter popping 'E':\\n\", df_to_delete_from)\n","    print(\"Popped series 'E':\\n\", e_series)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EkXHO8Sxulx6","executionInfo":{"status":"ok","timestamp":1747141853784,"user_tz":-330,"elapsed":9,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"593ad3e6-7b3d-48b4-eeba-02901e090ea3"},"execution_count":96,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","After popping 'E':\n","    A   B   C\n","0  1  10  11\n","1  2  20  22\n","2  3  30  33\n","3  4  40  44\n","Popped series 'E':\n"," 0    -9\n","1   -18\n","2   -27\n","3   -36\n","Name: E, dtype: int64\n"]}]},{"cell_type":"code","source":["# Using drop() with inplace=False (returns new DataFrame without modifying original)\n","df_dropped_C = df_to_delete_from.drop(columns=['C'])\n","print(\"\\nNew DataFrame after dropping 'C' (original unchanged):\\n\", df_dropped_C)\n","print(\"\\nOriginal df_to_delete_from after non-inplace drop:\\n\", df_to_delete_from)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1M85eZ7FupJu","executionInfo":{"status":"ok","timestamp":1747141853793,"user_tz":-330,"elapsed":7,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"8e9f29f3-562a-4346-dab0-b3b943c96455"},"execution_count":97,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","New DataFrame after dropping 'C' (original unchanged):\n","    A   B\n","0  1  10\n","1  2  20\n","2  3  30\n","3  4  40\n","\n","Original df_to_delete_from after non-inplace drop:\n","    A   B   C\n","0  1  10  11\n","1  2  20  22\n","2  3  30  33\n","3  4  40  44\n"]}]},{"cell_type":"code","source":["# Using drop() with inplace=True (modifies original DataFrame)\n","if 'B' in df_to_delete_from.columns:\n","    df_to_delete_from.drop(columns=['B'], inplace=True)\n","    print(\"\\nOriginal df_to_delete_from after dropping 'B' (inplace=True):\\n\", df_to_delete_from)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PGK1yueUusF4","executionInfo":{"status":"ok","timestamp":1747141853801,"user_tz":-330,"elapsed":6,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"7a15bb16-0a3e-45ce-9cdb-e884b4b42807"},"execution_count":98,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Original df_to_delete_from after dropping 'B' (inplace=True):\n","    A   C\n","0  1  11\n","1  2  22\n","2  3  33\n","3  4  44\n"]}]},{"cell_type":"markdown","source":["\n","\n","\n","**2.6 Saving DataFrames**\n","\n","Pandas allows DataFrames to be saved to various file formats. `to_csv()` is very common.\n","\n","* **`df.to_csv()` Function:** Writes the DataFrame to a Comma-Separated Values (CSV) file.\n","    \n","    * **Common Parameters:**\n","        * `path_or_buf`: File path or object. If `None`, the result is returned as a string. E.g., `'output.csv'`.\n","        * `sep`: Delimiter to use in the output file (default is `','`).\n","        * `index=True/False`: Whether to write the DataFrame index as a column in the CSV. Default is `True`. Often set to `False` if the index is a default integer index (0, 1, 2...) and not meaningful data.\n","        * `header=True/False`: Whether to write the column names as the first line (header row). Default is `True`.\n","        * `mode='w'/'a'`: Write mode. `'w'` to overwrite the file if it exists (default), `'a'` to append to an existing file.\n","        * `encoding`: Specifies the character encoding to use, e.g., `'utf-8'` (common). *[35]*\n","        * `columns`: Optional list of column names to write. If specified, only these columns are saved."],"metadata":{"id":"SgOPt8cmmPV7"}},{"cell_type":"code","source":["# Create a DataFrame\n","df_to_save = pd.DataFrame({\n","    'Name': ['Laptop', 'Mouse', 'Keyboard'],\n","    'Price': [1200, 25, 75],\n","    'Quantity': [5, 10, 7]\n","})\n","\n","print(\"\\nDataFrame to be saved:\\n\", df_to_save)"],"outputs":[{"output_type":"stream","name":"stdout","text":["\n","DataFrame to be saved:\n","        Name  Price  Quantity\n","0    Laptop   1200         5\n","1     Mouse     25        10\n","2  Keyboard     75         7\n"]}],"execution_count":99,"metadata":{"id":"syZ0HzRQmPV7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747141853808,"user_tz":-330,"elapsed":6,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"688a3dfc-ad6e-4a7e-b377-6b4f9c7bfa70"}},{"cell_type":"code","source":["# Try saving the DataFrame in various formats\n","try:\n","    # Save with index and header (default)\n","    df_to_save.to_csv('output_with_index_header.csv')\n","    print(\"\\nSaved to output_with_index_header.csv (with index and header)\")\n","\n","    # Expected content in file:\n","    # ,Name,Price,Quantity\n","    # 0,Laptop,1200,5\n","    # 1,Mouse,25,10\n","    # 2,Keyboard,75,7\n","\n","    # Save without index, but with header\n","    df_to_save.to_csv('output_no_index.csv', index=False)\n","    print(\"Saved to output_no_index.csv (no index, with header)\")\n","\n","    # Expected content in file:\n","    # Name,Price,Quantity\n","    # Laptop,1200,5\n","    # Mouse,25,10\n","    # Keyboard,75,7\n","\n","    # Save without index and without header, using tab separator\n","    df_to_save.to_csv('output_custom.tsv', sep='\\t', index=False, header=False)\n","    print(\"Saved to output_custom.tsv (tab separated, no index, no header)\")\n","\n","    # Expected content in file:\n","    # Laptop    1200    5\n","    # Mouse     25      10\n","    # Keyboard  75      7\n","\n","except Exception as e:\n","    print(f\"Error saving files: {e}. This might be due to permissions or disk space issues.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VMXNriqGvWiv","executionInfo":{"status":"ok","timestamp":1747141853816,"user_tz":-330,"elapsed":6,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"840c4296-b4a5-4d82-fbba-d21c8b52f248"},"execution_count":100,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Saved to output_with_index_header.csv (with index and header)\n","Saved to output_no_index.csv (no index, with header)\n","Saved to output_custom.tsv (tab separated, no index, no header)\n"]}]},{"cell_type":"code","source":["# Explanation:\n","# - `index=False` avoids saving the default index when it's not meaningful.\n","# - `header=False` omits column headers if not needed.\n","# - Using `sep='\\t'` saves in tab-separated format for compatibility."],"metadata":{"id":"fCl6eRddvdfk","executionInfo":{"status":"ok","timestamp":1747141853928,"user_tz":-330,"elapsed":111,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}}},"execution_count":101,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","**Module 2: Practice Questions**\n","\n","16. **Coding:** Create a Pandas `Series` named `s_colors` from the list `['Red', 'Green', 'Blue']` with a custom index `['R', 'G', 'B']`. Print the `Series`.\n","17. **MCQ:** What is the primary data structure in Pandas for handling 2-dimensional tabular data?\n","    * A) Series\n","    * B) NumPy Array\n","    * C) DataFrame\n","    * D) Python List\n","18. **Coding:** Create a Pandas `DataFrame` named `df_students` from the following dictionary. Print the DataFrame."],"metadata":{"id":"j6DUWIYjmPV7"}},{"cell_type":"code","source":["student_data = {'StudentID': [101, 102, 103],\n","                    'Name': ['Alice', 'Bob', 'Carol'],\n","                    'Score': [85, 92, 78]}"],"outputs":[],"execution_count":102,"metadata":{"id":"ysYdQz-MmPV7","executionInfo":{"status":"ok","timestamp":1747141853931,"user_tz":-330,"elapsed":1,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}}}},{"cell_type":"code","source":["df_student = pd.DataFrame(student_data)\n","print(df_student)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0UbgRqn2wqTW","executionInfo":{"status":"ok","timestamp":1747141853936,"user_tz":-330,"elapsed":4,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"1c9076e9-93c3-4b8c-9d2e-e706420ca468"},"execution_count":103,"outputs":[{"output_type":"stream","name":"stdout","text":["   StudentID   Name  Score\n","0        101  Alice     85\n","1        102    Bob     92\n","2        103  Carol     78\n"]}]},{"cell_type":"markdown","source":["19. **Short Answer:** What is a \"header row\" in the context of a CSV file?\n","20. **Function Parameter:** In `pd.read_csv()`, what does the `sep` parameter control?\n","21. **Function Parameter:** If your CSV file does not have a header row, what value should you pass to the `header` parameter in `pd.read_csv()`?"],"metadata":{"id":"e_0XjZKUmPV7"}},{"cell_type":"markdown","source":["22. **Coding:** Assume you have a CSV file named `products.csv` that has no header and contains product names and prices. Write the Python code to read this file into a DataFrame, providing column names 'ProductName' and 'ProductPrice'.\n"],"metadata":{"id":"46bKP0-1vq_1"}},{"cell_type":"code","source":[],"metadata":{"id":"kjW17FDyvs9L","executionInfo":{"status":"ok","timestamp":1747141853938,"user_tz":-330,"elapsed":1,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}}},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":["23. **Pandas Method:** Which method would you use to view the first 7 rows of a DataFrame named `df_data`?\n","24. **Pandas Method:** Which method provides a quick statistical summary (mean, std, min, max, etc.) of the numerical columns in a DataFrame?\n","25. **Attribute:** Which DataFrame attribute returns a tuple representing its dimensions (rows, columns)?\n","26. **Short Answer:** What is the key difference between selecting data using `.loc[]` and `.iloc[]`?\n","\n"],"metadata":{"id":"2PjVHdk3vtJE"}},{"cell_type":"markdown","source":["27. **Coding:** Given `df_students` from question 18, write code to select only the 'Name' column."],"metadata":{"id":"7lha4YSdvzG-"}},{"cell_type":"code","source":["print(df_student['Name'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4BkKljnWv9w9","executionInfo":{"status":"ok","timestamp":1747141866940,"user_tz":-330,"elapsed":12,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"89736149-3e0e-43aa-cf75-8b1eb145baa5"},"execution_count":108,"outputs":[{"output_type":"stream","name":"stdout","text":["0    Alice\n","1      Bob\n","2    Carol\n","Name: Name, dtype: object\n"]}]},{"cell_type":"markdown","source":["28. **Coding:** Given `df_students` from question 18, write code to select the row for the student with `StudentID` 102 using `.loc[]` (assuming `StudentID` is set as the index first, or by boolean indexing if not)."],"metadata":{"id":"GSiFtuhvv-Ku"}},{"cell_type":"code","source":["df_students_indexed = df_student.set_index('StudentID')\n","print(\"\\nDataFrame with StudentID as index:\")\n","print(df_students_indexed)\n","\n","try:\n","    student_102_loc = df_students_indexed.loc[102]\n","    print(\"\\nRow for StudentID 102 using .loc[] on indexed DataFrame:\")\n","    print(student_102_loc)\n","except KeyError:\n","    print(\"\\nStudentID 102 not found in the index.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vAC-lInZv_h_","executionInfo":{"status":"ok","timestamp":1747142046722,"user_tz":-330,"elapsed":265,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"d1e4aafe-a59e-4e09-bfa5-19982007f02b"},"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","DataFrame with StudentID as index:\n","            Name  Score\n","StudentID              \n","101        Alice     85\n","102          Bob     92\n","103        Carol     78\n","\n","Row for StudentID 102 using .loc[] on indexed DataFrame:\n","Name     Bob\n","Score     92\n","Name: 102, dtype: object\n"]}]},{"cell_type":"markdown","source":["29. **Coding:** Given `df_students` from question 18, write code to select the first two rows using `.iloc[]`."],"metadata":{"id":"D8iKaJMGv_8V"}},{"cell_type":"code","source":["df_student.iloc[0:2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"Pzw-5Cy5wAKz","executionInfo":{"status":"ok","timestamp":1747142139627,"user_tz":-330,"elapsed":12,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"05d43caa-00af-4e06-b076-29fcebfea3df"},"execution_count":116,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   StudentID   Name  Score\n","0        101  Alice     85\n","1        102    Bob     92"],"text/html":["\n","  <div id=\"df-1e0fb9a8-b9e1-4291-b0d7-95a25ebf5226\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>StudentID</th>\n","      <th>Name</th>\n","      <th>Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>101</td>\n","      <td>Alice</td>\n","      <td>85</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>102</td>\n","      <td>Bob</td>\n","      <td>92</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e0fb9a8-b9e1-4291-b0d7-95a25ebf5226')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-1e0fb9a8-b9e1-4291-b0d7-95a25ebf5226 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-1e0fb9a8-b9e1-4291-b0d7-95a25ebf5226');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-caa602e4-12cb-4d71-bc1d-a4c842a230b7\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-caa602e4-12cb-4d71-bc1d-a4c842a230b7')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-caa602e4-12cb-4d71-bc1d-a4c842a230b7 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"df_student\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"StudentID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 101,\n        \"max\": 102,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          102,\n          101\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Bob\",\n          \"Alice\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 85,\n        \"max\": 92,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          92,\n          85\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":116}]},{"cell_type":"markdown","source":["30. **Coding:** Add a new column named 'Grade' to `df_students`. For simplicity, assign 'Pass' to all students."],"metadata":{"id":"OHQBs_YAwB7X"}},{"cell_type":"code","source":[],"metadata":{"id":"PPytaF2rwCdc","executionInfo":{"status":"aborted","timestamp":1747141853943,"user_tz":-330,"elapsed":727,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["31. **Pandas Method:** Which method can be used to add new columns to a DataFrame while returning a *new* DataFrame (leaving the original unchanged)?\n","32. **Coding:** Delete the 'Score' column from `df_students` *in-place*.\n"],"metadata":{"id":"vZzF8Y_xwEYc"}},{"cell_type":"code","source":[],"metadata":{"id":"fFHdMd9TwFC6","executionInfo":{"status":"aborted","timestamp":1747141853957,"user_tz":-330,"elapsed":741,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["33. **Pandas Method:** What is the purpose of the `index=False` parameter in the `df.to_csv()` method?\n","34. **Coding:** Save the `df_students` DataFrame (after any modifications) to a CSV file named `student_records.csv` without writing the index.\n"],"metadata":{"id":"glQTx42kwHV6"}},{"cell_type":"code","source":[],"metadata":{"id":"kX9S68RRwJMK","executionInfo":{"status":"aborted","timestamp":1747141854099,"user_tz":-330,"elapsed":883,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["35. **True/False:** When creating a DataFrame from a list of dictionaries, Pandas will raise an error if the dictionaries have different keys.\n","36. **Short Answer:** What information does `df.info()` provide about a DataFrame? List at least three items.\n","37. **Coding:** Create a Series with 5 random numbers and a custom character index ('v', 'w', 'x', 'y', 'z').\n"],"metadata":{"id":"_vQqUfiXwJhv"}},{"cell_type":"code","source":[],"metadata":{"id":"PpPQMK_nwLbl","executionInfo":{"status":"aborted","timestamp":1747141854100,"user_tz":-330,"elapsed":884,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["38. **Conceptual:** If you use `df.describe(include='object')`, what kind of statistics would you expect to see?\n","39. **Selection:** How would you select all rows and only the first and third columns of a DataFrame `df` using `.iloc[]`?\n","40. **Selection:** How would you select rows where a column 'Age' is greater than 30 in a DataFrame `df` using `.loc[]`?\n","\n","---\n"],"metadata":{"id":"exQWZdTNwLnz"}},{"cell_type":"markdown","source":["**Module 3: Data Cleaning and Preparation**\n","\n","Data cleaning is a critical step in the data analysis pipeline. It involves identifying and correcting (or removing) errors, inconsistencies, and inaccuracies in datasets to ensure data quality for analysis and modeling.\n"],"metadata":{"id":"jwi9thrKwNey"}},{"cell_type":"markdown","source":["**3.1 Handling Missing Data**\n","\n","Missing data, often represented as `NaN` (Not a Number) in Pandas, can significantly skew analysis results and negatively impact machine learning model performance if not handled appropriately. *[36]*\n","\n","* **Identifying Missing Values:**\n","    * `df.isnull()` or `df.isna()`: These methods return a DataFrame of the same shape as the input, with `True` where values are missing (`NaN`) and `False` otherwise. *[38]*\n","    * `df.notnull()`: The opposite of `isnull()`; returns `True` for non-missing values.\n","    * `df.isnull().sum()`: This is a very common and useful command. It returns a `Series` showing the count of missing values in each column. *[38]*\n","    * `df.isnull().sum().sum()`: Returns the total number of missing values in the entire DataFrame. *[39]*\n","    * `df.isnull().values.any()`: Returns `True` if there is at least one missing value anywhere in the DataFrame. Useful for a quick check. *[39]*"],"metadata":{"id":"wR7akGxSwP1D"}},{"cell_type":"code","source":["import pandas as pd\n","    import numpy as np\n","\n","    data_missing_raw = {'col1': [1, 2, np.nan, 4, np.nan, 7],\n","                        'col2': ['A', 'B', 'C', np.nan, 'E', 'F'],\n","                        'col3': [10.0, 11.5, 12.3, np.nan, 14.2, np.nan],\n","                        'col4': [True, False, True, True, False, False]}\n","    df_missing = pd.DataFrame(data_missing_raw)\n","    print(\"Original DataFrame with missing values:\\n\", df_missing)\n","\n","    print(\"\\nBoolean DataFrame of missing values (isnull()):\\n\", df_missing.isnull())\n","    # Output: Shows True where data is NaN\n","\n","    print(\"\\nCount of missing values per column:\\n\", df_missing.isnull().sum())\n","    # Output:\n","    # col1    2\n","    # col2    1\n","    # col3    2\n","    # col4    0\n","    # dtype: int64\n","\n","    print(\"\\nTotal missing values in DataFrame:\", df_missing.isnull().sum().sum()) # Output: 5\n","\n","    print(\"\\nAre there any missing values at all?\", df_missing.isnull().values.any()) # Output: True"],"outputs":[],"execution_count":null,"metadata":{"id":"64gb5WHomPV7","executionInfo":{"status":"aborted","timestamp":1747141854102,"user_tz":-330,"elapsed":886,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}}}},{"cell_type":"markdown","source":["* `[Image: Heatmap visualization of df_missing.isnull(). This would show a grid where cells are colored (e.g., yellow for True/missing, purple for False/not missing), giving a quick visual overview of missing data patterns.]`\n","        * You can create this with `sns.heatmap(df_missing.isnull(), cbar=False, cmap='viridis')`.\n","\n","* **Strategies for Handling Missing Values:**\n","    The choice of strategy depends on several factors: the amount of missing data, the nature of the variable, the mechanism causing data to be missing, and the goals of the analysis. The two primary approaches are **deletion** and **imputation**. *[36]*\n","\n","    1.  **Deletion: Removing rows or columns with missing values.**\n","        * **`df.dropna()`**: Removes rows or columns containing missing values. *[37]*\n","            * `axis=0` (default): Drops **rows** that have at least one `NaN` (or all `NaN`s if `how='all'`).\n","            * `axis=1`: Drops **columns** that have at least one `NaN` (or all `NaN`s if `how='all'`).\n","            * `how='any'` (default): Drops the row/column if **any** `NaN` values are present.\n","            * `how='all'`: Drops the row/column only if **all** its values are `NaN`.\n","            * `thresh=n`: Keeps only rows/columns that have at least `n` non-`NaN` values. For example, `thresh=len(df.columns)-2` would keep rows with at most 2 NaNs.\n","            * `subset=['col_name1', 'col_name2']`: Restricts the `NaN` check to only the specified columns when deciding to drop rows (or specified index labels when dropping columns). *[37]*\n","            * `inplace=True/False` (default `False`): Modifies the DataFrame directly if `True`, or returns a new DataFrame if `False`."],"metadata":{"id":"sujzDoWpmPV7"}},{"cell_type":"code","source":["print(\"\\nOriginal df_missing before dropna operations:\\n\", df_missing)\n","\n","        # Drop rows with ANY missing values (default behavior)\n","        df_dropped_rows_any = df_missing.dropna() # axis=0, how='any' by default\n","        print(\"\\nDataFrame after dropping rows with any NaN:\\n\", df_dropped_rows_any) # Row 2, 3, 4, 5 would be dropped\n","\n","        # Drop columns with ANY missing values\n","        df_dropped_cols_any = df_missing.dropna(axis=1) # how='any' by default\n","        print(\"\\nDataFrame after dropping columns with any NaN:\\n\", df_dropped_cols_any) # col1, col2, col3 would be dropped\n","\n","        # Drop rows where 'col1' OR 'col3' is NaN\n","        df_dropped_subset = df_missing.dropna(subset=['col1', 'col3'], how='any') # how='any' is default for subset too\n","        print(\"\\nDataFrame after dropping rows if NaN in 'col1' or 'col3':\\n\", df_dropped_subset) # Row 2, 3, 5 would be dropped\n","\n","        # Keep rows that have at least 3 non-NaN values (out of 4 columns)\n","        df_thresh = df_missing.dropna(thresh=3)\n","        print(\"\\nDataFrame after keeping rows with at least 3 non-NaNs:\\n\", df_thresh)\n","\n","        # Original df_missing is unchanged if inplace=False\n","        # print(\"\\nOriginal df_missing after non-inplace dropna:\\n\", df_missing)"],"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"unexpected indent (<ipython-input-105-e748b47def71>, line 4)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-105-e748b47def71>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    df_dropped_rows_any = df_missing.dropna() # axis=0, how='any' by default\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"]}],"execution_count":105,"metadata":{"id":"eMOPsgQJmPV7","colab":{"base_uri":"https://localhost:8080/","height":110},"executionInfo":{"status":"error","timestamp":1747141854110,"user_tz":-330,"elapsed":6,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"6872b93f-69e8-4bcb-8fa4-83aa2a962543"}},{"cell_type":"markdown","source":["* **Pros & Cons of Deletion:**\n","            * **Pros:** Simple to implement.\n","            * **Cons:** Can lead to significant loss of data if missing values are widespread, potentially biasing the results or reducing statistical power. *[36]* Only suitable if data is missing completely at random and the proportion is small.\n","\n","    2.  **Imputation: Replacing missing values with estimated or calculated values.** *[36]*\n","        * **`df.fillna(value, method=None, inplace=False)`**: Fills `NaN` values using a specified method or value.\n","            * **Filling with a Constant:**\n","                * `df.fillna(0)`: Replaces all `NaN`s with 0.\n","                * `df['column_name'].fillna('Unknown')`: Replaces `NaN`s in a specific column with \"Unknown\".\n","            * **Filling with Statistical Measures (Mean, Median, Mode):**\n","                * **Mean:** `df['numerical_col'].fillna(df['numerical_col'].mean())`\n","                    * Good for numerical data that is somewhat normally distributed. Sensitive to outliers. *[41]*\n","                * **Median:** `df['numerical_col'].fillna(df['numerical_col'].median())`\n","                    * Good for numerical data, especially if it's skewed or has outliers, as the median is more robust.\n","                * **Mode:** `df['categorical_col'].fillna(df['categorical_col'].mode()[0])`\n","                    * Suitable for categorical data. `mode()` returns a Series (as there can be multiple modes), so `[0]` is used to get the first mode. *[43]*\n","            * **Filling with Forward Fill (`ffill`) or Backward Fill (`bfill`):**\n","                * `method='ffill'`: Propagates the last valid (non-`NaN`) observation forward to fill the `NaN`.\n","                * `method='bfill'` (or `backfill`): Uses the next valid observation to fill the `NaN`.\n","                * Useful for time series data or when the order of data matters.\n","            * **Grouped Imputation:** Filling missing values based on statistics from a specific group within the data.\n","                * Example: Impute missing 'Income' with the mean 'Income' of the corresponding 'JobType'.\n","                * Requires using `groupby()` and `transform()`."],"metadata":{"id":"Nfv8KndYmPV8"}},{"cell_type":"code","source":["df_impute = df_missing.copy() # Work on a copy\n","        print(\"\\nOriginal df_missing for imputation:\\n\", df_impute)\n","\n","        # Impute 'col1' (numerical) with its mean\n","        mean_col1 = df_impute['col1'].mean()\n","        df_impute['col1_imputed_mean'] = df_impute['col1'].fillna(mean_col1)\n","        print(\"\\nImputing 'col1' with mean:\\n\", df_impute[['col1', 'col1_imputed_mean']])\n","\n","        # Impute 'col2' (categorical) with its mode\n","        mode_col2 = df_impute['col2'].mode()[0] # Get the first mode\n","        df_impute['col2_imputed_mode'] = df_impute['col2'].fillna(mode_col2)\n","        print(\"\\nImputing 'col2' with mode ('%s'):\\n\" % mode_col2, df_impute[['col2', 'col2_imputed_mode']])\n","\n","        # Impute 'col3' using forward fill\n","        df_impute['col3_imputed_ffill'] = df_impute['col3'].fillna(method='ffill')\n","        print(\"\\nImputing 'col3' with ffill:\\n\", df_impute[['col3', 'col3_imputed_ffill']])\n","\n","        # Impute 'col3' using backward fill (on a fresh copy for clarity)\n","        df_impute_bfill = df_missing.copy()\n","        df_impute_bfill['col3_imputed_bfill'] = df_impute_bfill['col3'].fillna(method='bfill')\n","        print(\"\\nImputing 'col3' with bfill:\\n\", df_impute_bfill[['col3', 'col3_imputed_bfill']])\n","\n","        # Grouped Imputation Example\n","        # Suppose we want to fill NaN in col1 based on the mean of col1 for each distinct value in col4 (True/False)\n","        # df_missing['col1_imputed_groupmean'] = df_missing.groupby('col4')['col1'].transform(lambda x: x.fillna(x.mean()))\n","        # print(\"\\nDataFrame after grouped mean imputation for 'col1' based on 'col4':\\n\", df_missing[['col4','col1','col1_imputed_groupmean']])\n","        # The `transform` method applies a function to each group and returns a Series with the same index as the original DataFrame, making assignment easy. *[42]*"],"outputs":[],"execution_count":null,"metadata":{"id":"4y2AkNRemPV8","executionInfo":{"status":"aborted","timestamp":1747141854104,"user_tz":-330,"elapsed":887,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}}}},{"cell_type":"markdown","source":["* **Pros & Cons of Imputation:**\n","            * **Pros:** Preserves sample size. Can be more sophisticated than deletion.\n","            * **Cons:** Can introduce bias if the imputation method is not chosen carefully. May reduce variance in the data or distort relationships between variables. The imputed values are artificial.\n","\n","* **Flowchart for Handling Missing Data Strategy:**\n","    ```\n","    [Start: Missing Data Identified]\n","        |\n","        V\n","    [1. Assess Missingness]\n","        - Quantify: df.isnull().sum() (count per column), percentage per column.\n","        - Visualize: Heatmap of missing values.\n","        - Understand Pattern (if possible): MCAR, MAR, MNAR? *[36]*\n","            - MCAR (Missing Completely At Random): Missingness is independent of any variable.\n","            - MAR (Missing At Random): Missingness depends on other OBSERVED variables.\n","            - MNAR (Missing Not At Random): Missingness depends on the UNSEEN missing values themselves (hardest to deal with).\n","        |\n","        V\n","    [2. Decision Point: Based on extent and nature]\n","        |\n","        +--------------------------------+---------------------------------+\n","        |                                |                                 |\n","        V                                V                                 V\n","    [IF: Column has VERY HIGH % missing (e.g., >60-70%)] [IF: Small % missing (e.g., <5%) AND seems MCAR] [ELSE: Moderate missingness OR important feature]\n","        |                                |                                 |\n","        V                                V                                 V\n","    [Strategy: Consider Dropping Column] [Strategy: Deletion (Row-wise)]  [Strategy: Imputation]\n","    `df.dropna(axis=1, thresh=...)`      `df.dropna(subset=[...])`        |\n","                                                                          |\n","                                                                          V\n","                                                                    [Choose Imputation Method]\n","                                                                          |\n","                                                 +------------------------+------------------------+\n","                                                 |                        |                        |\n","                                                 V                        V                        V\n","                                           [Numerical Data]        [Categorical Data]       [Time Series/Ordered]\n","                                                 |                        |                        |\n","                                                 V                        V                        V\n","                                           - Mean (if normal, no outliers)  - Mode (most common)     - Forward Fill (ffill)\n","                                           - Median (skewed, outliers)    - New Category (\"Missing\") - Backward Fill (bfill)\n","                                           - Model-based (e.g., KNNImputer, Regression) *[36]*\n","        |                                |                                 |\n","        V                                V                                 V\n","    [3. Apply Chosen Strategy]----------------------------------------------+\n","        |\n","        V\n","    [4. Evaluate & Verify]\n","        - Check `df.isnull().sum()` again.\n","        - Assess if distributions are overly distorted.\n","        - Consider impact on downstream analysis/modeling.\n","        |\n","        V\n","    [End: Missing Data Handled]\n","    ```\n","    * `[Diagram: The above text formatted as a visual flowchart with boxes and arrows.]`\n","\n","**3.2 Handling Duplicates**\n","\n","Duplicate rows can skew analysis (e.g., by over-representing certain observations) and affect model training.\n","\n","* **`df.duplicated(subset=None, keep='first')`**: Returns a boolean `Series` indicating whether each row is a duplicate.\n","    * `subset`: A list of column labels to consider when identifying duplicates. By default, it uses all columns. If `subset=['colA', 'colB']`, a row is a duplicate if its values in 'colA' and 'colB' are identical to those in a prior row.\n","    * `keep='first'` (default): Marks all occurrences of duplicates as `True` except for the *first* one.\n","    * `keep='last'`: Marks all occurrences of duplicates as `True` except for the *last* one.\n","    * `keep=False`: Marks *all* occurrences of duplicate rows as `True`.\n","* **`df.drop_duplicates(subset=None, keep='first', inplace=False)`**: Returns a DataFrame with duplicate rows removed. The parameters `subset`, `keep`, and `inplace` work the same way as in `df.duplicated()`."],"metadata":{"id":"Bcujnxu8mPV8"}},{"cell_type":"code","source":["data_duplicates_raw = {'colA': [1, 1, 2, 3, 2, 1, 4],\n","                       'colB': ['x', 'x', 'y', 'z', 'y', 'x', 'w'],\n","                       'colC': [100, 100, 200, 300, 200, 100, 400]}\n","df_duplicates = pd.DataFrame(data_duplicates_raw)\n","print(\"Original DataFrame with duplicates:\\n\", df_duplicates)\n","# Row 0: 1, x, 100\n","# Row 1: 1, x, 100 (Duplicate of row 0)\n","# Row 4: 2, y, 200 (Duplicate of row 2)\n","# Row 5: 1, x, 100 (Duplicate of row 0 & 1)\n","\n","print(\"\\nBoolean Series of duplicated rows (first occurrence NOT marked as duplicate):\\n\",\n","      df_duplicates.duplicated())\n","# Output:\n","# 0    False\n","# 1     True  (duplicate of row 0)\n","# 2    False\n","# 3    False\n","# 4     True  (duplicate of row 2)\n","# 5     True  (duplicate of row 0/1)\n","# 6    False\n","# dtype: bool\n","\n","print(\"\\nBoolean Series of duplicated rows (last occurrence NOT marked as duplicate):\\n\",\n","      df_duplicates.duplicated(keep='last'))\n","# Output:\n","# 0     True\n","# 1     True\n","# 2     True\n","# 3    False\n","# 4    False\n","# 5    False\n","# 6    False\n","# dtype: bool\n","\n","print(\"\\nBoolean Series of duplicated rows (ALL duplicates marked as True):\\n\",\n","      df_duplicates.duplicated(keep=False))\n","# Output:\n","# 0     True\n","# 1     True\n","# 2     True\n","# 3    False\n","# 4     True\n","# 5     True\n","# 6    False\n","# dtype: bool\n","\n","# Drop duplicates, keeping the first occurrence by default\n","df_no_duplicates_first = df_duplicates.drop_duplicates() # keep='first' is default\n","print(\"\\nDataFrame after dropping all-column duplicates (keeping first):\\n\", df_no_duplicates_first)\n","# Output: Rows 1, 4, 5 will be dropped.\n","\n","# Drop duplicates based on 'colA' and 'colB' only, keeping the last occurrence\n","df_no_duplicates_subset_last = df_duplicates.drop_duplicates(subset=['colA', 'colB'], keep='last')\n","print(\"\\nDataFrame after dropping duplicates based on 'colA' & 'colB' (keeping last):\\n\", df_no_duplicates_subset_last)\n","# Considering (colA, colB):\n","# (1,x) appears at 0, 1, 5. Keeps row 5.\n","# (2,y) appears at 2, 4. Keeps row 4.\n","# (3,z) appears at 3. Keeps row 3.\n","# (4,w) appears at 6. Keeps row 6."],"outputs":[{"output_type":"stream","name":"stdout","text":["Original DataFrame with duplicates:\n","    colA colB  colC\n","0     1    x   100\n","1     1    x   100\n","2     2    y   200\n","3     3    z   300\n","4     2    y   200\n","5     1    x   100\n","6     4    w   400\n","\n","Boolean Series of duplicated rows (first occurrence NOT marked as duplicate):\n"," 0    False\n","1     True\n","2    False\n","3    False\n","4     True\n","5     True\n","6    False\n","dtype: bool\n","\n","Boolean Series of duplicated rows (last occurrence NOT marked as duplicate):\n"," 0     True\n","1     True\n","2     True\n","3    False\n","4    False\n","5    False\n","6    False\n","dtype: bool\n","\n","Boolean Series of duplicated rows (ALL duplicates marked as True):\n"," 0     True\n","1     True\n","2     True\n","3    False\n","4     True\n","5     True\n","6    False\n","dtype: bool\n","\n","DataFrame after dropping all-column duplicates (keeping first):\n","    colA colB  colC\n","0     1    x   100\n","2     2    y   200\n","3     3    z   300\n","6     4    w   400\n","\n","DataFrame after dropping duplicates based on 'colA' & 'colB' (keeping last):\n","    colA colB  colC\n","3     3    z   300\n","4     2    y   200\n","5     1    x   100\n","6     4    w   400\n"]}],"execution_count":106,"metadata":{"id":"USuCCW0LmPWK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747141854130,"user_tz":-330,"elapsed":17,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"3a33e386-7a88-45d4-8827-1f2f179f5990"}},{"cell_type":"markdown","source":["**3.3 Data Type Conversion**\n","\n","Ensuring columns have the correct data types is essential for accurate computations, analysis, and memory efficiency. Pandas often infers data types during loading (`read_csv`), but sometimes explicit conversion is needed. *[46]*\n","\n","* **`df['column'].astype(new_type)`** or `df.astype({'column1': new_type1, 'column2': new_type2})`\n","    * Converts the data type of one or more columns. *[46]*\n","    * `new_type` can be:\n","        * Python types: `int`, `float`, `str`, `bool`.\n","        * NumPy dtypes: `np.int64`, `np.float64`, `np.bool_`, etc.\n","        * Special Pandas type: `'category'` (for columns with a limited number of unique string values; can save memory and speed up some operations). *[48]*\n","        * Datetime: `'datetime64[ns]'`.\n","    * **Common Conversions & Potential Issues:**\n","        * **Object to Numeric (`int`, `float`):**\n","            * `df['col'].astype(float)` or `df['col'].astype(int)`.\n","            * Will raise a `ValueError` if the column contains non-numeric strings (e.g., '$15,000.00', 'Unknown'). *[46]*\n","            * Requires pre-cleaning (e.g., removing currency symbols, commas, handling non-numeric placeholders) before conversion.\n","        * **Numeric to String:** `df['col'].astype(str)`.\n","        * **Object to Category:** `df['col'].astype('category')`.\n","        * **Object/String to Datetime:** `df['col'].astype('datetime64[ns]')` or, more robustly, `pd.to_datetime(df['col'])`."],"metadata":{"id":"3EUUaUSjmPWK"}},{"cell_type":"code","source":["df_types_raw = pd.DataFrame({'A_str_int': ['10', '25', '30', '15'],\n","                                 'B_str_float': ['100.5', '200.0', '350.75', '99.9'],\n","                                 'C_category_like': ['Type1', 'Type2', 'Type1', 'Type3'],\n","                                 'D_mixed': ['50', '60.5', 'Error', '70']})\n","    print(\"Original DataFrame dtypes:\\n\", df_types_raw.dtypes)\n","    # All will likely be 'object'\n","\n","    df_types_converted = df_types_raw.copy()\n","\n","    # Convert column 'A_str_int' to integer\n","    df_types_converted['A_str_int'] = df_types_converted['A_str_int'].astype(int)\n","\n","    # Convert column 'B_str_float' to float\n","    df_types_converted['B_str_float'] = df_types_converted['B_str_float'].astype(float)\n","\n","    # Convert column 'C_category_like' to category\n","    df_types_converted['C_category_like'] = df_types_converted['C_category_like'].astype('category')\n","\n","    print(\"\\nDataFrame dtypes after astype():\\n\", df_types_converted.dtypes)\n","    print(\"\\nDataFrame after astype():\\n\", df_types_converted.head())\n","    # Note: df_types_converted['D_mixed'].astype(float) would raise a ValueError due to 'Error'"],"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"unexpected indent (<ipython-input-107-65bab160f04e>, line 5)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-107-65bab160f04e>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    print(\"Original DataFrame dtypes:\\n\", df_types_raw.dtypes)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"]}],"execution_count":107,"metadata":{"id":"VXFRyajCmPWK","colab":{"base_uri":"https://localhost:8080/","height":110},"executionInfo":{"status":"error","timestamp":1747141854134,"user_tz":-330,"elapsed":3,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"4885971d-d0ea-4aa4-83fa-8c4e6550e1b3"}},{"cell_type":"markdown","source":["* **`pd.to_numeric(arg, errors='raise')`**: Specialized function to convert an argument (Series, list, scalar) to a numeric type. More flexible in handling errors than `astype()`.\n","    * `arg`: The Series or column to convert.\n","    * `errors`: Defines how to handle values that cannot be converted to numeric.\n","        * `'raise'` (default): Raises a `ValueError` if an unparseable value is encountered.\n","        * `'coerce'`: Replaces unparseable values with `NaN`. This is very useful for cleaning. *[48]*\n","        * `'ignore'`: If invalid parsing occurs, returns the input object unchanged (so the column might remain of object type or mixed type).\n","* **`pd.to_datetime(arg, errors='raise', format=None)`**: Converts an argument to datetime objects.\n","    * `arg`: The Series or column to convert.\n","    * `errors`:\n","        * `'raise'` (default): Raises a `ValueError` for unparseable dates.\n","        * `'coerce'`: Replaces unparseable dates with `NaT` (Not a Time, the datetime equivalent of `NaN`).\n","        * `'ignore'`: Returns the input if unparseable.\n","    * `format`: A string representing the expected date format (e.g., `'%Y-%m-%d'`, `'%d/%m/%Y %H:%M:%S'`). If not provided, Pandas tries to infer the format, which is flexible but can be slower or ambiguous for mixed formats. Providing a format string improves performance and reliability if your dates are consistent."],"metadata":{"id":"3YAYZKBumPWL"}},{"cell_type":"code","source":["df_convert_robust = pd.DataFrame({'price_str': ['$100.50', '€50', 'NotANumber', ' $75.20 '],\n","                                      'date_str': ['2025-01-15', '20/02/2025', 'Invalid Date', 'Mar 3, 2025'],\n","                                      'value_str': ['123', '45.6', '789', 'non_numeric']})\n","\n","    # Clean and convert price_str to numeric, coercing errors\n","    # 1. Remove currency symbols and spaces\n","    df_convert_robust['price_cleaned'] = df_convert_robust['price_str'].str.replace(r'[$,€\\s]', '', regex=True)\n","    # 2. Convert to numeric\n","    df_convert_robust['price_numeric'] = pd.to_numeric(df_convert_robust['price_cleaned'], errors='coerce')\n","    print(\"\\nAfter pd.to_numeric on price_str (with cleaning):\\n\", df_convert_robust[['price_str', 'price_numeric']])\n","\n","    # Convert date_str to datetime, coercing errors (Pandas tries to infer format)\n","    df_convert_robust['date_dt_auto'] = pd.to_datetime(df_convert_robust['date_str'], errors='coerce')\n","    print(\"\\nAfter pd.to_datetime on date_str (auto-infer, coercing errors):\\n\", df_convert_robust[['date_str', 'date_dt_auto']])\n","\n","    # Example of converting specific format (if all were like '20/02/2025')\n","    # df_convert_robust['date_dt_specific'] = pd.to_datetime(df_convert_robust['date_str'], format='%d/%m/%Y', errors='coerce')\n","    # print(\"\\nAfter pd.to_datetime with specific format '%d/%m/%Y':\\n\", df_convert_robust[['date_str', 'date_dt_specific']])\n","\n","    # Using errors='coerce' with pd.to_numeric is a common and robust strategy.\n","    df_convert_robust['value_numeric'] = pd.to_numeric(df_convert_robust['value_str'], errors='coerce')\n","    print(\"\\nConverting 'value_str' to numeric with errors='coerce':\\n\", df_convert_robust[['value_str', 'value_numeric']])"],"outputs":[],"execution_count":null,"metadata":{"id":"EhbFTorYmPWL","executionInfo":{"status":"aborted","timestamp":1747141854136,"user_tz":-330,"elapsed":3,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}}}},{"cell_type":"markdown","source":["**3.4 Renaming Columns**\n","\n","Column names may need to be changed for clarity, consistency (e.g., standardizing case, removing spaces), or to make them valid Python identifiers if they contain special characters.\n","\n","* **`df.rename(columns={'old_name1': 'new_name1', 'old_name2': 'new_name2'}, inplace=False)`**: Renames one or more columns using a dictionary that maps old names to new names. *[49]*\n","    * Can also rename index labels using the `index` parameter (e.g., `index={'old_idx': 'new_idx'}`).\n","    * `inplace=False` (default): Returns a new DataFrame with renamed columns/index. Original is unchanged.\n","    * `inplace=True`: Modifies the DataFrame directly.\n","* **`df.columns = ['new_name1', 'new_name2', ...]`**: Assigns a new list of column names directly.\n","    * The list provided **must** have the same length as the number of existing columns.\n","    * This method is suitable for renaming all columns at once or if you have a programmatic way to generate all new names. *[50]*"],"metadata":{"id":"1VpUAWLmmPWL"}},{"cell_type":"code","source":["df_rename_raw = pd.DataFrame({'First Name': ['John', 'Jane', 'Mike'],\n","                              'Last Name': ['Doe', 'Smith', 'Ross'],\n","                              'Age_ yrs': [30, 28, 35],\n","                              'email address': ['j.doe@mail.com', 'jane@mail.com', 'm.ross@mail.com']})\n","print(\"Original DataFrame for renaming:\\n\", df_rename_raw)\n","\n","# Rename specific columns using df.rename() (returns a new DataFrame)\n","df_renamed_specific = df_rename_raw.rename(columns={'First Name': 'FirstName',\n","                                                    'Last Name': 'LastName',\n","                                                    'Age_ yrs': 'Age',\n","                                                    'email address': 'EmailAddress'})\n","print(\"\\nAfter renaming specific columns (new DataFrame returned):\\n\", df_renamed_specific)\n","print(\"\\nOriginal DataFrame (should be unchanged):\\n\", df_rename_raw) # Verifies original is unchanged\n","\n","# Rename all columns by assigning a new list to df.columns\n","# This modifies the DataFrame in-place. Let's work on a copy.\n","df_rename_all = df_rename_raw.copy()\n","new_column_names = ['FName', 'LName', 'CurrentAge', 'Email']\n","if len(new_column_names) == len(df_rename_all.columns):\n","    df_rename_all.columns = new_column_names\n","    print(\"\\nAfter renaming all columns by direct assignment:\\n\", df_rename_all)\n","else:\n","    print(\"\\nError: Length of new column names list does not match number of columns.\")\n","\n","# Tip: Clean column names (e.g., lowercase, replace spaces with underscores)\n","df_clean_cols = df_rename_raw.copy()\n","df_clean_cols.columns = df_clean_cols.columns.str.lower().str.replace(' ', '_').str.replace('[^A-Za-z0-9_]+', '', regex=True)\n","print(\"\\nAfter cleaning all column names systematically:\\n\", df_clean_cols)\n","# This uses string methods on the Index object of column names."],"outputs":[],"execution_count":null,"metadata":{"id":"hIfQlkBzmPWL","executionInfo":{"status":"aborted","timestamp":1747141854137,"user_tz":-330,"elapsed":3,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}}}},{"cell_type":"markdown","source":["* **Choosing a Method:**\n","    * `rename()`: More flexible for renaming a subset of columns, or renaming based on a function. Safer as it returns a new DataFrame by default. *[49]*\n","    * Direct assignment to `df.columns`: Quicker for a complete overhaul of all column names if you have the full list of new names. Modifies in-place.\n","\n","---\n","\n","**Module 3: Practice Questions**\n","\n","41. **True/False:** In Pandas, `NaN` stands for \"Not a Number\" and is used to represent missing data.\n","42. **Pandas Method:** Which method would you use to get a count of missing values for each column in a DataFrame `df_data`?\n","43. **MCQ:** If you want to drop rows from `df_data` only if *all* values in that row are missing, what parameters would you use with `dropna()`?\n","    * A) `axis=0, how='any'`\n","    * B) `axis=1, how='all'`\n","    * C) `axis=0, how='all'`\n","    * D) `axis=1, how='any'`\n","44. **Short Answer:** What is \"imputation\" in the context of handling missing data? Name two common imputation techniques for numerical data.\n","45. **Pandas Method:** How can you fill all `NaN` values in a numerical column 'Age' with the median age using `fillna()`? Write the code. (Assume `df_data` is your DataFrame).\n","46. **Explain:** What is the difference between `method='ffill'` and `method='bfill'` in `fillna()`? When might you use them?\n","47. **Coding:** You have a DataFrame `df_sales` with columns 'Product_ID' and 'Region'. Some 'Region' values are missing. You want to impute these missing 'Region' values with the mode of the 'Region' column. Write the code.\n","48. **Pandas Method:** Which method is used to identify duplicate rows in a DataFrame?\n","49. **Parameter:** In `df.drop_duplicates()`, what does the `keep` parameter control? What are its possible values?\n","50. **Coding:** Given a DataFrame `df_orders`, remove duplicate rows based on all columns, keeping the *last* occurrence. Modify the DataFrame in-place.\n","51. **Pandas Method:** Which method is primarily used to convert the data type of a Pandas Series or DataFrame column?\n","52. **Scenario:** You have a column 'Price_Text' with string values like \"€25.99\", \"$10.50\". You want to convert this to a numeric (float) column named 'Price_Numeric'. Outline the steps you would take, including handling potential errors.\n","53. **Function:** Which Pandas function is more robust for converting a column to numeric types and allows you to coerce errors into `NaN`s?\n","54. **Function:** Which Pandas function is used to convert a column of string dates into datetime objects? What parameter helps if the dates are in non-standard formats?\n","55. **Coding:** Rename a column ' old name with spaces ' to 'new_concise_name' in `df_data`. Do this without modifying `df_data` in-place.\n","56. **Coding:** You have a DataFrame `df_raw` with columns `['User ID', 'Transaction Date', 'Amount (USD)']`. Provide the code to change these column names to `['user_id', 'transaction_date', 'amount_usd']`.\n","57. **Critical Thinking:** Why is it generally better to use `errors='coerce'` with `pd.to_numeric` or `pd.to_datetime` during initial data cleaning rather than letting the operation fail?\n","58. **Pandas Method:** How can you check if *any* value in an entire DataFrame `df` is `NaN` with a single boolean output?\n","59. **Imputation Strategy:** If a numerical column has many outliers, would mean imputation or median imputation be a more robust choice? Why?\n","60. **Code:** Given `df_missing` from the module notes, drop any column that has more than 1 missing value. (Hint: You might need to iterate or use a comprehension with `df.isnull().sum()`).\n","\n","---\n","*(Continued in next response due to length limitations)*"],"metadata":{"id":"xspAUqjBmPWL"}},{"cell_type":"markdown","source":["<div class=\"md-recitation\">\n","  Sources\n","  <ol>\n","  <li><a href=\"https://www.scribd.com/document/681281393/210130107085-PDS-Practical\">https://www.scribd.com/document/681281393/210130107085-PDS-Practical</a></li>\n","  <li><a href=\"https://medium.com/@tim.po.developer/must-know-python-libraries-for-dominating-finance-your-essential-toolkit-f4c25a0f6d30\">https://medium.com/@tim.po.developer/must-know-python-libraries-for-dominating-finance-your-essential-toolkit-f4c25a0f6d30</a></li>\n","  <li><a href=\"https://medium.com/coders-mojo/implemented-scikit-learn-projects-c0e65f70e54e\">https://medium.com/coders-mojo/implemented-scikit-learn-projects-c0e65f70e54e</a></li>\n","  <li><a href=\"https://github.com/BladimirBL/Talento-Tech\">https://github.com/BladimirBL/Talento-Tech</a></li>\n","  </ol>\n","</div>"],"metadata":{"id":"UxE4MtoTmPWM"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}