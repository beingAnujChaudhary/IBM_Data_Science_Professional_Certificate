{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Comprehensive Notes for Data Analysis Using Python**\n"
      ],
      "metadata": {
        "id": "-9SF2ZAGmPVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 1: Introduction to Data Analysis and Python Environment**\n",
        "\n"
      ],
      "metadata": {
        "id": "ghGMPqRb7e7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1 What is Data Analysis?**\n",
        "\n",
        "* **Definition:** Data analysis is the systematic process of examining, cleaning, transforming, and interpreting raw data to extract meaningful insights, identify patterns and trends, answer questions, and support informed decision-making. Think of it as being a detective for data, looking for clues (insights) within a pile of evidence (raw data).\n",
        "* **Goal:** To turn raw data into actionable knowledge.\n",
        "* **Process:** It involves several stages:\n",
        "    1.  **Asking the right questions:** What do you want to find out?\n",
        "    2.  **Data Collection:** Gathering raw data from various sources.\n",
        "    3.  **Data Cleaning (Inspection & Cleansing):** Identifying and handling errors, inconsistencies, missing values, and outliers in the data. This is like preparing your ingredients before cooking.\n",
        "    4.  **Data Transformation:** Converting data into a suitable format for analysis and modeling. This might involve changing data types, creating new features, or restructuring data.\n",
        "    5.  **Data Modeling & Analysis:** Applying statistical techniques, algorithms, and computational tools to explore the data, find patterns, test hypotheses, and build predictive models.\n",
        "    6.  **Interpretation & Communication:** Understanding the results of the analysis and communicating the findings effectively, often through reports, visualizations, and dashboards.\n",
        "    \n",
        "* **Importance:** Data analysis is vital across many fields like business (to understand customer behavior, optimize marketing), science (to analyze experimental results), finance (for risk assessment, fraud detection), healthcare (to improve patient outcomes), and social sciences (to study societal trends)."
      ],
      "metadata": {
        "id": "DkAMEJPL7i6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mPmxXwxz8ldL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2 Why Python for Data Analysis?**\n",
        "\n",
        "Python has become a dominant language for data analysis for several compelling reasons:\n",
        "\n",
        "* **Simplicity and Readability:**\n",
        "    * **Understandable Definition:** Python's syntax is designed to be clear, concise, and human-readable, resembling plain English. This lowers the learning curve, especially for those new to programming.\n",
        "    * **Example:** Compare `print(\"Hello\")` in Python to more verbose syntax in other languages.\n",
        "* **Extensive Libraries:**\n",
        "    * **Understandable Definition:** Python offers a vast collection of specialized toolkits (libraries) built specifically for data analysis tasks. These libraries provide pre-written, optimized code, so you don't have to build everything from scratch.\n",
        "    * **Key Libraries:** Pandas (data manipulation), NumPy (numerical computing), Matplotlib/Seaborn (visualization), Scikit-learn (machine learning), Statsmodels (statistical modeling).\n",
        "* **Large Community Support:**\n",
        "    * **Understandable Definition:** A massive global community of Python users and developers means abundant learning resources (tutorials, blogs, courses), quick help when you're stuck (forums like Stack Overflow), and continuous improvement of the language and its libraries.\n",
        "* **Integration Capabilities:**\n",
        "    * **Understandable Definition:** Python plays well with other technologies. It can be easily integrated with other programming languages (like C++, Java), databases, web frameworks, and big data tools.\n",
        "* **Scalability:**\n",
        "    * **Understandable Definition:** Python can effectively handle datasets of varying sizes, from small spreadsheets to large enterprise databases. Performance for very large datasets can be enhanced using optimized libraries and techniques for parallel processing.\n",
        "* **Versatility:** Python is a general-purpose language, meaning it's not just for data analysis. You can use it for web development, automation, AI, and more, making it a valuable skill overall."
      ],
      "metadata": {
        "id": "jVvxh4CG8ua6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.3 Core Python Libraries for Data Analysis**\n",
        "\n",
        "These are the workhorse libraries you'll encounter frequently:"
      ],
      "metadata": {
        "id": "RWea0Pxs802F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Pandas:**\n",
        "    * **Definition:** Pandas (Python Data Analysis Library) is an open-source library that provides high-performance, flexible, and intuitive data structures (like tables) and tools for data manipulation and analysis. It's the cornerstone for most data wrangling tasks in Python.\n",
        "        \n",
        "    * **Key Features:**\n",
        "        * **DataFrame Object:** A 2-dimensional, size-mutable, tabular data structure with labeled rows and columns (like a spreadsheet or SQL table). It can hold heterogeneous data types (numbers, strings, booleans, etc.).\n",
        "        * **Series Object:** A 1-dimensional labeled array, capable of holding any data type. Think of it as a single column in a DataFrame.\n",
        "        * **Data Input/Output (I/O):** Comprehensive tools for reading data from and writing data to various file formats, including CSV, Excel, SQL databases, JSON, HTML, HDF5, and more.\n",
        "        * **Data Alignment and Missing Data Handling:** Smart alignment of data based on labels and robust tools for detecting, filtering, and filling missing data (e.g., `NaN`).\n",
        "        * **Reshaping and Pivoting:** Powerful functions for restructuring datasets, such as pivoting, stacking, unstacking, and melting.\n",
        "        * **Label-based Slicing, Indexing, and Subsetting:** Flexible ways to select, filter, and retrieve specific parts of large datasets using labels or positions.\n",
        "        * **Group By Engine:** Efficient mechanism for splitting data into groups based on some criteria, applying a function to each group (e.g., sum, mean), and then combining the results.\n",
        "        * **Merging and Joining:** SQL-like operations for combining datasets based on common columns or indices.\n",
        "        * **Time Series Functionality:** Specialized tools for working with time-stamped data, including date range generation, frequency conversion, moving window statistics, and date shifting.\n",
        "        * **Performance:** Critical internal operations are often written in Cython or C for speed. *\n",
        "    * **Official Documentation:** [Pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/) (Link based on common knowledge, as specific URL for  isn't provided beyond context)"
      ],
      "metadata": {
        "id": "E1PNIKK983cg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **NumPy:**\n",
        "    * **Definition:** NumPy (Numerical Python) is the foundational package for numerical computation in Python. It provides powerful support for large, multi-dimensional arrays and matrices, along with a vast collection of mathematical functions to operate on them efficiently. Many other data science libraries, including Pandas, are built on top of NumPy.\n",
        "        \n",
        "    * **Key Features:**\n",
        "        * **N-dimensional Array Object (`ndarray`):** An efficient, powerful data structure for storing and manipulating homogeneous numerical data (all elements of the same type).\n",
        "        * **Broadcasting Functions:** A mechanism that allows NumPy to perform arithmetic operations on arrays of different shapes, simplifying code and avoiding explicit loops.\n",
        "        * **Tools for Integrating C/C++ and Fortran Code:** Allows for leveraging code written in these languages for performance-critical tasks.\n",
        "        * **Linear Algebra, Fourier Transform, and Random Number Capabilities:** Comprehensive mathematical functions for advanced computations.\n",
        "    * **Official Documentation:** [NumPy Documentation](https://numpy.org/doc/)"
      ],
      "metadata": {
        "id": "D42A2VIO9YJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qcfZApC39oYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* **Matplotlib:**\n",
        "    * **Definition:** Matplotlib is a comprehensive and widely-used Python library for creating static, animated, and interactive visualizations in a variety of formats. It provides fine-grained control over every aspect of a plot.\n",
        "        \n",
        "    * **Key Features:**\n",
        "        * **Variety of Plots:** Can generate line plots, scatter plots, bar charts, histograms, pie charts, error charts, contour plots, and much more.\n",
        "        * **Figures and Axes:** Plots are typically created within a `Figure` object, which can contain one or more `Axes` objects. An `Axes` represents an individual plot or subplot with its own coordinate system.\n",
        "        * **Artist Objects:** Almost everything you see on a Matplotlib figure is an `Artist` (e.g., text objects, lines, patches). This object-oriented approach allows for detailed customization.\n",
        "        * **Pyplot Module (`matplotlib.pyplot`):** A collection of functions that provide a MATLAB-like interface, making it easy to create plots quickly.\n",
        "        * **Customization:** Offers extensive control over colors, line styles, fonts, labels, legends, and annotations.\n",
        "    * **Official Documentation:** [Matplotlib Documentation](https://matplotlib.org/stable/contents.html) *[7]*\n",
        "\n"
      ],
      "metadata": {
        "id": "MCwXvvjI9qAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* **Seaborn:**\n",
        "    * **Definition:** Seaborn is a Python data visualization library built on top of Matplotlib. It provides a higher-level interface for creating attractive and informative statistical graphics, often requiring less code than Matplotlib for common statistical plots.\n",
        "        * *Reference: [9]*\n",
        "    * **Key Features:**\n",
        "        * **Statistical Plotting:** Specialized functions for visualizing statistical relationships, distributions, and categorical data.\n",
        "        * **Integration with Pandas:** Works seamlessly with Pandas DataFrames, making it easy to map DataFrame columns to plot aesthetics. *[9]*\n",
        "        * **Aesthetic Defaults:** Comes with visually appealing default styles, themes, and color palettes. *[9]*\n",
        "        * **High-Level Functions:** Simplifies the creation of complex plots like heatmaps, violin plots, pair plots, joint plots, and regression plots with built-in statistical estimation. *[11]*\n",
        "    * **Official Documentation:** [Seaborn Documentation](https://seaborn.pydata.org/) *[10]*\n",
        "    "
      ],
      "metadata": {
        "id": "P6gej-EQ96vz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Scikit-learn:**\n",
        "    * **Definition:** Scikit-learn is a comprehensive and robust open-source machine learning library for Python. It provides a wide range of tools for supervised and unsupervised learning, model selection, preprocessing, and evaluation.\n",
        "        * *Reference: [13], [14]*\n",
        "    * **Key Features:**\n",
        "        * **Supervised Learning Algorithms:** Includes algorithms for classification (e.g., Logistic Regression, SVM, Decision Trees, Random Forests) and regression (e.g., Linear Regression, Ridge Regression). *[14]*\n",
        "        * **Unsupervised Learning Algorithms:** Offers algorithms for clustering (e.g., K-Means, DBSCAN), dimensionality reduction (e.g., PCA, t-SNE), and anomaly detection. *[14]*\n",
        "        * **Data Preprocessing:** Provides tools for feature scaling, normalization, encoding categorical data, handling missing values, and feature extraction. *[13]*\n",
        "        * **Model Selection:** Includes methods for splitting data into training and testing sets, cross-validation, and hyperparameter tuning (e.g., GridSearchCV). *[13]*\n",
        "        * **Model Evaluation:** Offers various metrics for assessing the performance of classification, regression, and clustering models. *[13]*\n",
        "        * **Interoperability:** Designed to work well with NumPy and SciPy.\n",
        "    * **Official Documentation:** [Scikit-learn Documentation](https://scikit-learn.org/stable/) *[14]*."
      ],
      "metadata": {
        "id": "wS8uMlu298VO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* **Statsmodels:**\n",
        "    * **Definition:** Statsmodels is a Python library that focuses on providing classes and functions for estimating many different statistical models, conducting statistical tests, and performing statistical data exploration. It often provides more detailed statistical output and diagnostics compared to Scikit-learn for classical statistical models.\n",
        "        * *Reference: [16]*\n",
        "    * **Key Features:**\n",
        "        * **Statistical Models:** Extensive support for linear models (OLS), generalized linear models (GLM), time series analysis (ARIMA, VAR), survival analysis, and more. *[17]*\n",
        "        * **Statistical Tests:** A wide array of hypothesis tests, goodness-of-fit tests, and diagnostic tests.\n",
        "        * **R-style Formulas:** Allows model specification using R-like formulas in conjunction with Pandas DataFrames (e.g., `response ~ predictor1 + predictor2`). *[17]*\n",
        "        * **Detailed Results:** Provides comprehensive summary outputs for model estimators, including standard errors, p-values, confidence intervals, and various statistical metrics. *[17]*\n",
        "    * **Official Documentation:** [Statsmodels Documentation](https://www.statsmodels.org/stable/index.html) *[17]*\n",
        "\n"
      ],
      "metadata": {
        "id": "Xf6-6xkP9-A6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**1.4 Setting Up the Python Environment**\n",
        "\n",
        "A correctly configured Python environment is essential for a smooth data analysis workflow.\n",
        "\n",
        "* **Recommended Method: Anaconda Distribution**\n",
        "    * **What it is:** Anaconda is a free and open-source distribution of Python and R, specifically tailored for scientific computing and data science.\n",
        "    * **Benefits:** It comes with Python, the `conda` package and environment manager, and many of the core data analysis libraries (Pandas, NumPy, Matplotlib, Scikit-learn, etc.) pre-installed. This simplifies setup significantly.\n",
        "    * **Installation:** Download from [anaconda.com](https://www.anaconda.com/products/distribution) and follow the installation instructions for your operating system.\n",
        "* **Alternative: Standard Python + `pip`**\n",
        "    1.  **Install Python:** Download Python directly from [python.org](https://www.python.org/downloads/).\n",
        "    2.  **Use `pip` (Python Package Installer):** `pip` is usually included with Python. You can install libraries individually from the command line/terminal:\n",
        "        ```bash\n",
        "        pip install pandas numpy matplotlib seaborn scikit-learn statsmodels\n",
        "        ```\n",
        "        "
      ],
      "metadata": {
        "id": "uzILyF_B-CrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Virtual Environments (Highly Recommended):**\n",
        "    * **Why?** To manage dependencies for different projects separately. This prevents conflicts where one project needs a specific version of a library, and another project needs a different version.\n",
        "    * **How?**\n",
        "        * **Using `venv` (built-in with Python):**\n",
        "            ```bash\n",
        "            # Create a virtual environment (e.g., named 'myenv')\n",
        "            python -m venv myenv\n",
        "            # Activate it\n",
        "            # On Windows:\n",
        "            # myenv\\Scripts\\activate\n",
        "            # On macOS/Linux:\n",
        "            # source myenv/bin/activate\n",
        "            # Install packages within the activated environment\n",
        "            pip install ...\n",
        "            # Deactivate when done\n",
        "            # deactivate\n",
        "            ```\n",
        "        * **Using `conda` (with Anaconda):**\n",
        "            ```bash\n",
        "            # Create a virtual environment (e.g., named 'my_conda_env') with specific Python version\n",
        "            conda create --name my_conda_env python=3.9\n",
        "            # Activate it\n",
        "            conda activate my_conda_env\n",
        "            # Install packages (can specify conda or pip install)\n",
        "            conda install pandas\n",
        "            # or\n",
        "            # pip install pandas\n",
        "            # Deactivate when done\n",
        "            conda deactivate\n",
        "            ```\n",
        "    * `[Diagram: Flowchart showing decision: New Project? -> Yes -> Create Virtual Environment -> Activate -> Install Libraries. If Existing Project -> Activate Environment.]`\n",
        "    "
      ],
      "metadata": {
        "id": "E8tpqghK-EyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 1: Practice Questions**\n"
      ],
      "metadata": {
        "id": "lZDGeE3B-HmI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  **Short Answer:** In your own words, what is the primary goal of data analysis?\n",
        "2.  **MCQ:** Which of the following is NOT a typical stage in the data analysis process?\n",
        "    * A) Data Collection\n",
        "    * B) Software Development\n",
        "    * C) Data Cleaning\n",
        "    * D) Interpretation & Communication\n",
        "3.  **List:** Name three key advantages of using Python for data analysis.\n",
        "4.  **Fill-in-the-Blanks:** The Pandas library's primary 2D data structure is called a \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_, while its 1D data structure is called a \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_.\n",
        "5.  **MCQ:** Which Python library is fundamental for numerical computations and provides the `ndarray` object?\n",
        "    * A) Pandas\n",
        "    * B) Matplotlib\n",
        "    * C) NumPy\n",
        "    * D) Scikit-learn\n",
        "6.  **Matching:** Match the library with its primary purpose:\n",
        "    * Matplotlib         -> A) Machine Learning\n",
        "    * Seaborn            -> B) Statistical Modeling, Detailed Output\n",
        "    * Scikit-learn       -> C) Foundational Plotting\n",
        "    * Statsmodels        -> D) High-level Statistical Visualization\n",
        "7.  **Short Answer:** Why are virtual environments recommended for Python projects?\n",
        "8.  **Command:** Write the `pip` command to install the Pandas and NumPy libraries.\n",
        "9.  **Command:** Write the `conda` command to create a new environment named `data_analysis_env` with Python version 3.10.\n",
        "10. **True/False:** Python's syntax is generally considered more verbose and harder to read than languages like Java or C++.\n",
        "11. **Short Answer:** What is the role of the `pyplot` module in Matplotlib?\n",
        "12. **MCQ:** Which library would you primarily use if you wanted to create an interactive heatmap of a correlation matrix with minimal code?\n",
        "    * A) NumPy\n",
        "    * B) Seaborn\n",
        "    * C) Statsmodels\n",
        "    * D) Base Python\n",
        "13. **Explain:** What does it mean when it's said that critical code paths in Pandas are written in \"Cython or C\"?\n",
        "14. **Short Answer:** Give an example of how Pandas' \"Group By Engine\" might be used.\n",
        "15. **True/False:** Anaconda is a Python library, similar to Pandas or NumPy.\n"
      ],
      "metadata": {
        "id": "T_Iyr6y6-Nbr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vdTYMnjP-OK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Module 2: Pandas Fundamentals - Data Structures and Basic Operations**\n",
        "\n",
        "\n",
        "This module introduces the core data structures in Pandas, `DataFrame` and `Series`, and covers essential operations for loading, inspecting, selecting, and saving data."
      ],
      "metadata": {
        "id": "PW8hgvcQ-QQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**2.1 Pandas Data Structures: `DataFrame` and `Series`**\n",
        "\n",
        "* **`Series`:**\n",
        "    * **Definition:** A Pandas `Series` is a one-dimensional labeled array capable of holding data of any type (integers, strings, floating-point numbers, Python objects, etc.). The axis labels are collectively referred to as the **index**. You can think of a `Series` as a single column in a spreadsheet or a single column from a SQL table.\n",
        "    * **Key Characteristics:**\n",
        "        * One-dimensional.\n",
        "        * Homogeneous data (typically, though can hold mixed types if `dtype=object`).\n",
        "        * Labeled index (can be numbers, strings, dates, etc.).\n",
        "        * Size-immutable (length cannot be changed once created, but values can be).\n",
        "    * **Creation:**"
      ],
      "metadata": {
        "id": "6IAX4W_u-T8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# From a Python list\n",
        "my_list = [10, 20, 30, 40, 50]\n",
        "\n",
        "s_from_list = pd.Series(my_list, dtype='int64')  # Explicit dtype for clarity\n",
        "print(\"Series from list:\\n\", s_from_list)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Series from list:\n",
            " 0    10\n",
            "1    20\n",
            "2    30\n",
            "3    40\n",
            "4    50\n",
            "dtype: int64\n"
          ]
        }
      ],
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsUtJvT_mPVz",
        "outputId": "5297147e-8d22-464e-f11a-7f8e1ec29551"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From a Python list with a custom index\n",
        "s_list_custom_index = pd.Series([1, 3, 5, np.nan, 6, 8], index=['a', 'b', 'c', 'd', 'e', 'f'])\n",
        "print(\"\\nSeries from list with custom index:\\n\", s_list_custom_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaWsjbv__jei",
        "outputId": "f94900eb-89b4-4bc0-813a-a265deeb8564"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Series from list with custom index:\n",
            " a    1.0\n",
            "b    3.0\n",
            "c    5.0\n",
            "d    NaN\n",
            "e    6.0\n",
            "f    8.0\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# From a NumPy array with a specified index\n",
        "s_numpy = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e'])\n",
        "print(\"\\nSeries from NumPy array with index:\\n\", s_numpy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POCCnqNa_vYb",
        "outputId": "f0853853-2a76-41c3-8d27-f2ce8095bf95"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Series from NumPy array with index:\n",
            " a    0.176519\n",
            "b   -0.743293\n",
            "c    1.672523\n",
            "d   -0.108207\n",
            "e   -0.650336\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# From a Python dictionary\n",
        "s_dict_data = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}\n",
        "s_dict = pd.Series(s_dict_data, dtype='int64')  # Explicit dtype for clarity\n",
        "print(\"\\nSeries from dictionary:\\n\", s_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCAV-YHp_yMt",
        "outputId": "cbba7941-0937-429e-8375-a6f5edb23c54"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Series from dictionary:\n",
            " Ohio      35000\n",
            "Texas     71000\n",
            "Oregon    16000\n",
            "Utah       5000\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Explanation:**\n",
        "  * The first example creates a `Series` from a list. Pandas automatically assigns a default integer index starting from 0.\n",
        "  * The second example shows creating a `Series` with a custom index provided by a list of labels. `np.nan` represents a missing value.\n",
        "  * The third example uses a NumPy array for data and specifies an index.\n",
        "  * The fourth example demonstrates creating a `Series` from a Python dictionary. The dictionary keys are used as the index labels, and the dictionary values become the data for the `Series`."
      ],
      "metadata": {
        "id": "zEMaY4NEAmy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **`DataFrame`:**\n",
        "    * **Definition:** A Pandas `DataFrame` is a two-dimensional, size-mutable (can add/remove columns/rows), and potentially heterogeneous (columns can have different data types) tabular data structure with labeled axes (rows and columns). It's the most commonly used Pandas object and can be thought of as a dictionary of `Series` objects, an in-memory spreadsheet, a SQL table, or a structured collection of data.\n",
        "        \n",
        "    * **Key Characteristics:**\n",
        "        * Two-dimensional (rows and columns).\n",
        "        * Can hold columns of different data types.\n",
        "        * Labeled rows (index) and labeled columns.\n",
        "        * Size-mutable (can change shape).\n",
        "    * **Creation:**"
      ],
      "metadata": {
        "id": "D91On71jAcZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From a dictionary of lists or Series\n",
        "data_dict = {'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
        "             'Age': [25, 30, 35, 28],\n",
        "             'City': ['New York', 'Paris', 'London', 'Tokyo']}\n",
        "\n",
        "df_from_dict = pd.DataFrame(data_dict)\n",
        "\n",
        "print(\"DataFrame from dictionary of lists:\\n\", df_from_dict)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame from dictionary of lists:\n",
            "       Name  Age      City\n",
            "0    Alice   25  New York\n",
            "1      Bob   30     Paris\n",
            "2  Charlie   35    London\n",
            "3    David   28     Tokyo\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "id": "0WhgFwrJmPV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7a24b44-9b5a-4db2-f5a3-62dd73d484f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From a list of dictionaries\n",
        "data_list_dict = [{'Name': 'Eve', 'Age': 22, 'City': 'Berlin'},\n",
        "                  {'Name': 'Frank', 'Age': 29, 'City': 'Madrid', 'Occupation': 'Engineer'}]\n",
        "\n",
        "df_from_list_dict = pd.DataFrame(data_list_dict)\n",
        "\n",
        "print(\"\\nDataFrame from list of dictionaries:\\n\", df_from_list_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVRSH9PkA98Y",
        "outputId": "3aed9a31-a2be-4c3c-f6f5-03d4075f3862"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame from list of dictionaries:\n",
            "     Name  Age    City Occupation\n",
            "0    Eve   22  Berlin        NaN\n",
            "1  Frank   29  Madrid   Engineer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# From a NumPy array, with a datetime index and labeled columns\n",
        "dates = pd.date_range('20250101', periods=6)\n",
        "random_data = np.random.randn(6, 4)\n",
        "\n",
        "df_from_numpy = pd.DataFrame(random_data, index=dates, columns=['A', 'B', 'C', 'D'])\n",
        "\n",
        "print(\"\\nDataFrame from NumPy array with datetime index and labeled columns:\\n\", df_from_numpy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6P72zKsA_K8",
        "outputId": "8180d52b-4efd-4964-f7f0-c056b5a4056f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame from NumPy array with datetime index and labeled columns:\n",
            "                    A         B         C         D\n",
            "2025-01-01 -0.164754  0.482505 -2.091973 -0.827540\n",
            "2025-01-02 -0.406569 -0.079819  0.010626 -2.145920\n",
            "2025-01-03 -0.774114  0.575282  1.863914  0.943362\n",
            "2025-01-04 -1.140393 -0.262792 -0.435426 -0.505387\n",
            "2025-01-05 -0.441544 -0.006994  0.981240 -1.136845\n",
            "2025-01-06  0.376163 -0.212764  1.590295 -0.516188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Explanation:**\n",
        "\t* The first example constructs a `DataFrame` from a dictionary where keys are column names and values are lists representing column data.\n",
        "\t* The second example uses a list of dictionaries, where each dictionary forms a row. Pandas intelligently handles missing keys by filling with `NaN`.\n",
        "\t* The third example shows creating a `DataFrame` from a 2D NumPy array, explicitly providing row index labels (here, dates) and column labels."
      ],
      "metadata": {
        "id": "yRsCMJX8mPV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 Loading Data into Pandas DataFrames**\n",
        "\n",
        "Pandas excels at reading data from various file formats into DataFrames."
      ],
      "metadata": {
        "id": "xQLW_GG4BVeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Definition of a Dataset:**\n",
        "    * **Understandable Definition:** A dataset is simply a collection of data, usually organized in a structured way (like a table) so that it can be easily accessed, managed, and analyzed. Think of it as the raw material for your data analysis project.\n",
        "        "
      ],
      "metadata": {
        "id": "rqBZyFsWBqrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* **CSV File Format:**\n",
        "    * **Understandable Definition:** A CSV (Comma-Separated Values) file is a plain text file where data is stored in a tabular format. Each line in the file represents a row of data, and the values within each row are separated by commas (or sometimes other characters like semicolons or tabs). It's a very common and simple format for exchanging tabular data."
      ],
      "metadata": {
        "id": "j8esbOLmBsE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Header Row:**\n",
        "    * **Understandable Definition:** In a tabular data file (like a CSV or spreadsheet), the header row is the first row that contains labels or names for each column. These labels make the data understandable by providing context for what each column represents.\n"
      ],
      "metadata": {
        "id": "GlHEy7ZyB5n_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **`pd.read_csv()` Function:**\n",
        "    * **Purpose:** This is the primary Pandas function for reading data from a CSV file into a DataFrame.\n",
        "        \n",
        "    * **Common Parameters:**\n",
        "        * `filepath_or_buffer`: The path to the CSV file (as a string, e.g., `'data.csv'`, `'C:/Users/Me/Documents/data.csv'`) or a URL.\n",
        "        * `sep` (or `delimiter`): The character used to separate values in the file. Default is `','`. Use `sep='\\t'` for tab-separated files, or `sep=';'` for semicolon-separated files. Pandas often auto-detects this if not `','`.\n",
        "        * `header`: Specifies which row number to use as column names (0-indexed). Default is `0` (first row). If your file has no header, use `header=None`.\n",
        "        * `names`: A list of column names to use. If you provide `names` and the file has a header, you should also specify `header=0` to overwrite the file's header or `header=None` if the file has no header and you're providing names.\n",
        "        * `index_col`: Column(s) to use as the row labels (index) of the DataFrame. Can be an integer (column position) or a string (column name).\n",
        "        * `usecols`: Allows you to read only a subset of columns, specified by name or position. E.g., `usecols=['Name', 'Age']`.\n",
        "        * `skiprows`: Number of lines to skip at the beginning of the file, or a list of specific row numbers (0-indexed) to skip.\n",
        "        * `nrows`: Number of rows to read from the file. Useful for reading just a sample of a very large file.\n"
      ],
      "metadata": {
        "id": "m7swZYYrB_ev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    * **Reading CSV with Header (Default Behavior):**\n",
        "        * If your CSV file has a header row (column names in the first line), `pd.read_csv()` will usually detect and use it automatically."
      ],
      "metadata": {
        "id": "BTyXrDbICUyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a dummy CSV file with a header for demonstration\n",
        "csv_data_with_header = \"\"\"Name,Age,City\n",
        "Alice,25,New York\n",
        "Bob,30,London\n",
        "Carol,22,Paris\n",
        "\"\"\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with header (inferred):\n",
            "     Name  Age      City\n",
            "0  Alice   25  New York\n",
            "1    Bob   30    London\n",
            "2  Carol   22     Paris\n"
          ]
        }
      ],
      "execution_count": 13,
      "metadata": {
        "id": "lPvVRWMGmPV1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb8ebcde-3d7b-4834-9156-0cf8386dd03e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write CSV file\n",
        "with open('data_with_header.csv', 'w', encoding='utf-8') as f:\n",
        "    f.write(csv_data_with_header)"
      ],
      "metadata": {
        "id": "iGQy06kYCzJt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try reading the CSV file\n",
        "try:\n",
        "    df_with_header = pd.read_csv('data_with_header.csv', encoding='utf-8')\n",
        "    print(\"DataFrame with header (inferred):\\n\", df_with_header)\n",
        "\n",
        "    # Expected Output:\n",
        "    #     Name  Age      City\n",
        "    # 0  Alice   25  New York\n",
        "    # 1    Bob   30    London\n",
        "    # 2  Carol   22     Paris\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'data_with_header\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQbGK-5RC9Yi",
        "outputId": "1a8a3f0a-cff7-4844-9783-f8af83218911"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with header (inferred):\n",
            "     Name  Age      City\n",
            "0  Alice   25  New York\n",
            "1    Bob   30    London\n",
            "2  Carol   22     Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write CSV file\n",
        "with open('data_with_header.csv', 'w', encoding='utf-8') as f:\n",
        "    f.write(csv_data_with_header)"
      ],
      "metadata": {
        "id": "wmuCkX_6Cuf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Reading CSV without Header:**\n",
        "        * If the CSV file lacks a header row, you must tell Pandas by setting `header=None`. You can then let Pandas assign default integer column names (0, 1, 2, ...) or provide your own using the `names` parameter."
      ],
      "metadata": {
        "id": "PRudIXdOmPV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dummy CSV file without a header\n",
        "csv_data_no_header = \"\"\"Dave,35,Berlin\n",
        "Eve,29,Madrid\n",
        "Frank,40,Rome\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "id": "47gP64JCmPV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write CSV file\n",
        "with open('data_no_header.csv', 'w', encoding='utf-8') as f:\n",
        "    f.write(csv_data_no_header)"
      ],
      "metadata": {
        "id": "7eXC5nUeDI40"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Write CSV file\n",
        "with open('data_no_header.csv', 'w', encoding='utf-8') as f:\n",
        "    f.write(csv_data_no_header)\n",
        "\n",
        "# Try reading the CSV file\n",
        "try:\n",
        "    # Method 1: Let Pandas assign default integer column names\n",
        "    df_no_header_default_names = pd.read_csv('data_no_header.csv', header=None)\n",
        "    print(\"\\nDataFrame without header (default names):\\n\", df_no_header_default_names)\n",
        "\n",
        "    # Expected Output:\n",
        "    #       0   1       2\n",
        "    # 0  Dave  35  Berlin\n",
        "    # 1   Eve  29  Madrid\n",
        "    # 2 Frank  40    Rome\n",
        "\n",
        "    # Method 2: Assign custom column names\n",
        "    column_names = ['FullName', 'UserAge', 'UserCity']\n",
        "    df_no_header_custom_names = pd.read_csv('data_no_header.csv', header=None, names=column_names)\n",
        "    print(\"\\nDataFrame without header (custom names):\\n\", df_no_header_custom_names)\n",
        "\n",
        "    # Expected Output:\n",
        "    #   FullName  UserAge UserCity\n",
        "    # 0     Dave       35   Berlin\n",
        "    # 1      Eve       29   Madrid\n",
        "    # 2    Frank       40     Rome\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'data_no_header.csv' not found. Ensure it's created in the same directory or provide the full path.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ilC5AyYDLU9",
        "outputId": "e94b671c-0d5f-4724-d397-c6b12b232585"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame without header (default names):\n",
            "        0   1       2\n",
            "0   Dave  35  Berlin\n",
            "1    Eve  29  Madrid\n",
            "2  Frank  40    Rome\n",
            "\n",
            "DataFrame without header (custom names):\n",
            "   FullName  UserAge UserCity\n",
            "0     Dave       35   Berlin\n",
            "1      Eve       29   Madrid\n",
            "2    Frank       40     Rome\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3 Inspecting DataFrames**\n",
        "\n",
        "Once data is loaded, the first step is to get a feel for its structure and content.\n",
        "\n",
        "* **`.head(n=5)` and `.tail(n=5)`:**\n",
        "    * `df.head(n)`: Returns the first `n` rows of the DataFrame. Default is 5. Excellent for a quick peek at the beginning of your data. *[27]*\n",
        "    * `df.tail(n)`: Returns the last `n` rows of the DataFrame. Default is 5. Useful for checking the end of your data, especially after sorting or appending. *[27], [28]*"
      ],
      "metadata": {
        "id": "THbIbwZCmPV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check if df_with_header exists\n",
        "\n",
        "if 'df_with_header' in globals():\n",
        "\n",
        "  print(\"First 2 rows (head):\\n\", df_with_header.head(2))\n",
        "\n",
        "  print(\"\\nlast 2 rows (tail):\\n\", df_with_header.tail(2))\n",
        "\n",
        "else:\n",
        "    # Create a sample DataFrame for inspection if previous load failed\n",
        "    data_inspect = {\n",
        "        'colA': range(10),"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 2 rows (head):\n",
            "     Name  Age      City\n",
            "0  Alice   25  New York\n",
            "1    Bob   30    London\n",
            "\n",
            "last 2 rows (tail):\n",
            "     Name  Age    City\n",
            "1    Bob   30  London\n",
            "2  Carol   22   Paris\n"
          ]
        }
      ],
      "execution_count": 24,
      "metadata": {
        "id": "mJu8r4-UmPV3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8add7295-33ad-4994-b468-c3802ee376e6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **`.info()`:**\n",
        "    * Provides a concise summary of a DataFrame, including:\n",
        "        * The data type of the index.\n",
        "        * The data type of each column (`Dtype`).\n",
        "        * The number of non-null (non-missing) values in each column (`Non-Null Count`).\n",
        "        * Memory usage of the DataFrame.\n",
        "        \n",
        "    * **Output Interpretation:** Extremely useful for quickly spotting columns with missing data or incorrect data types."
      ],
      "metadata": {
        "id": "4NGTEbjEmPV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample DataFrame for inspection with mixed types and missing values\n",
        "\n",
        "data_info = {\n",
        "    'ID': [1, 2, 3, 4, 5],\n",
        "    'Product': ['Apple', 'Banana', 'Orange', np.nan, 'Apple'],\n",
        "    'Price': [0.5, 0.25, np.nan, 1.0, 0.55],\n",
        "    'Quantity': [10, 15, 8, 12, 20]\n",
        "}\n",
        "\n",
        "df_for_info = pd.DataFrame(data_info)\n",
        "\n",
        "print(df_for_info)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ID Product  Price  Quantity\n",
            "0   1   Apple   0.50        10\n",
            "1   2  Banana   0.25        15\n",
            "2   3  Orange    NaN         8\n",
            "3   4     NaN   1.00        12\n",
            "4   5   Apple   0.55        20\n"
          ]
        }
      ],
      "execution_count": 29,
      "metadata": {
        "id": "6rXhPlMMmPV4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a80882de-8bef-4fcc-9217-9a74e3847b60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert OrderDate column to datetime format\n",
        "\n",
        "df_for_info['OrderDate'] = pd.to_datetime(\n",
        "    ['2025-01-01', '2025-01-01', '2025-01-02', '2025-01-03', '2025-01-03'])\n",
        "\n",
        "# Try-except for robustness\n",
        "try:\n",
        "    print(\"\\nDataFrame info:\")\n",
        "    df_for_info.info()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mhomyE9KNPt",
        "outputId": "0debea69-1eb7-4b9f-8a18-31f1ffbe63c4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5 entries, 0 to 4\n",
            "Data columns (total 5 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   ID         5 non-null      int64         \n",
            " 1   Product    4 non-null      object        \n",
            " 2   Price      4 non-null      float64       \n",
            " 3   Quantity   5 non-null      int64         \n",
            " 4   OrderDate  5 non-null      datetime64[ns]\n",
            "dtypes: datetime64[ns](1), float64(1), int64(2), object(1)\n",
            "memory usage: 332.0+ bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **`.describe()`:**\n",
        "    * Generates descriptive statistics of the DataFrame.\n",
        "    * For **numerical columns** (default): count, mean, standard deviation (std), min, 25th percentile (Q1), 50th percentile (median/Q2), 75th percentile (Q3), and max.\n",
        "    * For **categorical/object columns** (if `include='object'` or `include='all'` is used): count, unique (number of distinct values), top (most frequent value/mode), and freq (frequency of the top value).\n",
        "    * `include='all'` will show statistics for all columns, using NaN for metrics that don't apply to a particular data type."
      ],
      "metadata": {
        "id": "gaI2Rz-ImPV4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "gWBlCXTtmPV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Other Attributes for Basic Inspection:**\n",
        "    * `df.shape`: Returns a tuple `(number_of_rows, number_of_columns)`.\n",
        "    * `df.dtypes`: Returns a `Series` showing the data type of each column.\n",
        "    * `df.columns`: Returns an `Index` object containing the column labels.\n",
        "    * `df.index`: Returns an `Index` object containing the row labels."
      ],
      "metadata": {
        "id": "HGr5nxhXmPV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert OrderDate column to datetime format\n",
        "df_for_info['OrderDate'] = pd.to_datetime(\n",
        "    ['2025-01-01', '2025-01-01', '2025-01-02', '2025-01-03', '2025-01-03'])\n",
        "\n",
        "print(df_for_info)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ID Product  Price  Quantity  OrderDate\n",
            "0   1   Apple   0.50        10 2025-01-01\n",
            "1   2  Banana   0.25        15 2025-01-01\n",
            "2   3  Orange    NaN         8 2025-01-02\n",
            "3   4     NaN   1.00        12 2025-01-03\n",
            "4   5   Apple   0.55        20 2025-01-03\n"
          ]
        }
      ],
      "execution_count": 37,
      "metadata": {
        "id": "qPUUGIRKmPV5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b8b913-6910-4302-9e62-e008c5e7678c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive statistics for numerical columns\n",
        "print(\"\\nDescriptive statistics for numerical columns:\\n\", df_for_info.describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhdXzfKFLrt7",
        "outputId": "5a129c4c-f47b-42bd-88d1-038efed2ee71"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Descriptive statistics for numerical columns:\n",
            "              ID    Price   Quantity            OrderDate\n",
            "count  5.000000  4.00000   5.000000                    5\n",
            "mean   3.000000  0.57500  13.000000  2025-01-02 00:00:00\n",
            "min    1.000000  0.25000   8.000000  2025-01-01 00:00:00\n",
            "25%    2.000000  0.43750  10.000000  2025-01-01 00:00:00\n",
            "50%    3.000000  0.52500  12.000000  2025-01-02 00:00:00\n",
            "75%    4.000000  0.66250  15.000000  2025-01-03 00:00:00\n",
            "max    5.000000  1.00000  20.000000  2025-01-03 00:00:00\n",
            "std    1.581139  0.31225   4.690416                  NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive statistics for categorical (object) columns\n",
        "print(\"\\nDescriptive statistics for object columns:\\n\", df_for_info.describe(include=['object']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdjIHdqBLvXH",
        "outputId": "6a571879-29e9-421c-85e3-c36616c886de"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Descriptive statistics for object columns:\n",
            "        Product\n",
            "count        4\n",
            "unique       3\n",
            "top      Apple\n",
            "freq         2\n",
            "\n",
            "Descriptive statistics for all columns (datetime treated separately):\n",
            "               ID Product    Price   Quantity            OrderDate\n",
            "count   5.000000       4  4.00000   5.000000                    5\n",
            "unique       NaN       3      NaN        NaN                  NaN\n",
            "top          NaN   Apple      NaN        NaN                  NaN\n",
            "freq         NaN       2      NaN        NaN                  NaN\n",
            "mean    3.000000     NaN  0.57500  13.000000  2025-01-02 00:00:00\n",
            "min     1.000000     NaN  0.25000   8.000000  2025-01-01 00:00:00\n",
            "25%     2.000000     NaN  0.43750  10.000000  2025-01-01 00:00:00\n",
            "50%     3.000000     NaN  0.52500  12.000000  2025-01-02 00:00:00\n",
            "75%     4.000000     NaN  0.66250  15.000000  2025-01-03 00:00:00\n",
            "max     5.000000     NaN  1.00000  20.000000  2025-01-03 00:00:00\n",
            "std     1.581139     NaN  0.31225   4.690416                  NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive statistics for all columns (handling datetime appropriately)\n",
        "try:\n",
        "    print(\"\\nDescriptive statistics for all columns:\\n\", df_for_info.describe(include='all', datetime_is_numeric=True))\n",
        "except TypeError:\n",
        "    print(\"\\nDescriptive statistics for all columns (datetime treated separately):\\n\", df_for_info.describe(include='all'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2giE6-tRLzIM",
        "outputId": "d36b75a0-c971-47f8-8b01-8ca1da7b4fc6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Descriptive statistics for all columns (datetime treated separately):\n",
            "               ID Product    Price   Quantity            OrderDate\n",
            "count   5.000000       4  4.00000   5.000000                    5\n",
            "unique       NaN       3      NaN        NaN                  NaN\n",
            "top          NaN   Apple      NaN        NaN                  NaN\n",
            "freq         NaN       2      NaN        NaN                  NaN\n",
            "mean    3.000000     NaN  0.57500  13.000000  2025-01-02 00:00:00\n",
            "min     1.000000     NaN  0.25000   8.000000  2025-01-01 00:00:00\n",
            "25%     2.000000     NaN  0.43750  10.000000  2025-01-01 00:00:00\n",
            "50%     3.000000     NaN  0.52500  12.000000  2025-01-02 00:00:00\n",
            "75%     4.000000     NaN  0.66250  15.000000  2025-01-03 00:00:00\n",
            "max     5.000000     NaN  1.00000  20.000000  2025-01-03 00:00:00\n",
            "std     1.581139     NaN  0.31225   4.690416                  NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4 Data Selection and Indexing**\n",
        "\n",
        "Pandas offers powerful ways to select specific subsets of your data.\n",
        "\n",
        "* **Selecting Columns:**\n",
        "    * Using square brackets `[]`:\n",
        "        * `df['column_name']`: Selects a single column as a `Series`.\n",
        "        * `df[['col1_name', 'col2_name']]`: Selects multiple columns as a new `DataFrame`. Note the inner list."
      ],
      "metadata": {
        "id": "MvWLM-rnmPV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using df_for_info\n",
        "\n",
        "selected_product_series = df_for_info['Product']\n",
        "print(\"\\nSelecting column 'Product' (as Sereies):\\n\", selected_product_series)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selecting column 'Product' (as Sereies):\n",
            " 0     Apple\n",
            "1    Banana\n",
            "2    Orange\n",
            "3       NaN\n",
            "4     Apple\n",
            "Name: Product, dtype: object\n"
          ]
        }
      ],
      "execution_count": 43,
      "metadata": {
        "id": "Sr2UwVsnmPV5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e5931f7-65af-4cf5-daec-a4d356b1a1c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_cols_df = df_for_info[['Product', 'Price']]\n",
        "print(\"\\nSelecting columns 'Product' and 'Price' (as DataFrame):\\n\", selected_cols_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIXAkyfnMYU3",
        "outputId": "b16fb09b-4ad2-450f-e09b-19a433b1c2e1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selecting columns 'Product' and 'Price' (as DataFrame):\n",
            "   Product  Price\n",
            "0   Apple   0.50\n",
            "1  Banana   0.25\n",
            "2  Orange    NaN\n",
            "3     NaN   1.00\n",
            "4   Apple   0.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Selecting Rows with `.loc[]` (Label-based):**\n",
        "    * Accesses a group of rows and columns by **labels** (index names, column names).\n",
        "    * `df.loc[row_label]`: Selects a single row (returns a `Series`).\n",
        "    * `df.loc[[row_label1, row_label2]]`: Selects multiple rows by label (returns a `DataFrame`).\n",
        "    * `df.loc[start_label:end_label]`: Slice of rows (inclusive of both start and end labels).\n",
        "    * `df.loc[:, 'column_name']`: Selects all rows for a specific column (returns a `Series`).\n",
        "    * `df.loc[row_label_slice, column_label_slice]`: Selects a specific block of data.\n",
        "    * `df.loc[boolean_condition]`: Selects rows where the boolean condition is `True`."
      ],
      "metadata": {
        "id": "sfBOlobImPV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's set a custom index for df_for_info for better .loc demonstration\n",
        "df_loc_demo = df_for_info.copy()\n",
        "\n",
        "df_loc_demo.index = ['Order1', 'Order2', 'Order3', 'Order4', 'Order5']\n",
        "print(\"\\nDataFrame with custom index for .loc demo:\\n\", df_loc_demo)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame with custom index for .loc demo:\n",
            "         ID Product  Price  Quantity  OrderDate\n",
            "Order1   1   Apple   0.50        10 2025-01-01\n",
            "Order2   2  Banana   0.25        15 2025-01-01\n",
            "Order3   3  Orange    NaN         8 2025-01-02\n",
            "Order4   4     NaN   1.00        12 2025-01-03\n",
            "Order5   5   Apple   0.55        20 2025-01-03\n"
          ]
        }
      ],
      "execution_count": 46,
      "metadata": {
        "id": "WBuWr8mEmPV6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b4ecaf9-a14f-4734-d322-75fc9bc5a9cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSelecting row with index 'Order2' using .loc:\\n\", df_loc_demo.loc['Order2'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE4t5w4ANFEb",
        "outputId": "55db6068-1ecc-4ac0-cd03-5e28eb029041"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selecting row with index 'Order2' using .loc:\n",
            " ID                             2\n",
            "Product                   Banana\n",
            "Price                       0.25\n",
            "Quantity                      15\n",
            "OrderDate    2025-01-01 00:00:00\n",
            "Name: Order2, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSelecting row with index 'Order1' to 'Order3' using .loc:\\n\", df_loc_demo.loc['Order1': 'Order3'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTk_rCjENbgk",
        "outputId": "9cdbf116-47e9-45b8-b73b-9284ba3a0be3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selecting row with index 'Order1' to 'Order3' using .loc:\n",
            "         ID Product  Price  Quantity  OrderDate\n",
            "Order1   1   Apple   0.50        10 2025-01-01\n",
            "Order2   2  Banana   0.25        15 2025-01-01\n",
            "Order3   3  Orange    NaN         8 2025-01-02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSelecting 'Product' and 'Quantity' for rows 'Order2' and 'Order4':\\n\",\n",
        "          df_loc_demo.loc[['Order2', 'Order4'], ['Product', 'Quantity']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1837RJpgNj29",
        "outputId": "3f1ada52-d440-4d5a-c710-f5d514f3ab86"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selecting 'Product' and 'Quantity' for rows 'Order2' and 'Order4':\n",
            "        Product  Quantity\n",
            "Order2  Banana        15\n",
            "Order4     NaN        12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSelecting rows where Quantity>10 using .loc:\\n\", df_loc_demo.loc[df_loc_demo['Quantity']>10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRBTjFcZN2O9",
        "outputId": "cf36895e-0f80-4d08-aa45-a34c8a5ba340"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selecting rows where Quantity>10 using .loc:\n",
            "         ID Product  Price  Quantity  OrderDate\n",
            "Order2   2  Banana   0.25        15 2025-01-01\n",
            "Order4   4     NaN   1.00        12 2025-01-03\n",
            "Order5   5   Apple   0.55        20 2025-01-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Selecting Rows with `.iloc[]` (Integer Position-based):**\n",
        "    * Accesses a group of rows and columns by **integer positions** (0-indexed).\n",
        "    * `df.iloc[row_pos]` Selects a single row by its integer position (returns a `Series`).\n",
        "    * `df.iloc[[row_pos1, row_pos2]]`: Selects multiple rows by integer position (returns a `DataFrame`).\n",
        "    * `df.iloc[start_pos:end_pos]`: Slice of rows (exclusive of `end_pos`, like Python list slicing).\n",
        "    * `df.iloc[:, col_pos]`: Selects all rows for a specific column by position (returns a `Series`).\n",
        "    * `df.iloc[row_pos_slice, col_pos_slice]`: Selects a specific block of data by position."
      ],
      "metadata": {
        "id": "iTVWTAFtmPV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using df_for_info (which has default integer index)\n",
        "print(\"\\nSelecting row at position 0 using .iloc:\\n\", df_for_info.iloc[0])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selecting row at position 0 using .iloc:\n",
            " ID                             1\n",
            "Product                    Apple\n",
            "Price                        0.5\n",
            "Quantity                      10\n",
            "OrderDate    2025-01-01 00:00:00\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ],
      "execution_count": 56,
      "metadata": {
        "id": "_Ks-O4ufmPV6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff63e588-1f62-4e97-ceb7-edd0a23c08dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSelecting rows at positions 0 to 2 (exclusive of 2) using .iloc:\\n\", df_for_info.iloc[0:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQhsqnWDQAvc",
        "outputId": "9ae5b484-ae94-454f-cc43-6af118ac190a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selecting rows at positions 0 to 2 (exclusive of 2) using .iloc:\n",
            "    ID Product  Price  Quantity  OrderDate\n",
            "0   1   Apple   0.50        10 2025-01-01\n",
            "1   2  Banana   0.25        15 2025-01-01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSelecting first 3 rows and first 2 columns (positions 0, 1) using .iloc:\\n\", df_for_info.iloc[0:3, 0:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbClWMtxQEej",
        "outputId": "788dd897-c267-4ead-d389-915dc0ce9417"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selecting first 3 rows and first 2 columns (positions 0, 1) using .iloc:\n",
            "    ID Product\n",
            "0   1   Apple\n",
            "1   2  Banana\n",
            "2   3  Orange\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSelecting specific rows [0, 2, 4] and columns [1, 3] using .iloc:\\n\", df_for_info.iloc[[0, 2, 4], [1, 3]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bvq4mTXaQGy6",
        "outputId": "d58480ec-9b55-425b-9a40-35d50deb0d2f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selecting specific rows [0, 2, 4] and columns [1, 3] using .iloc:\n",
            "   Product  Quantity\n",
            "0   Apple        10\n",
            "2  Orange         8\n",
            "4   Apple        20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Key Difference:** `.loc` uses labels, `.iloc` uses integer positions. Mistaking one for the other is a common source of errors.\n",
        "\n",
        "**2.5 Basic Operations on DataFrames**\n",
        "\n",
        "* **Adding Columns:**\n",
        "    * **Direct Assignment:** `df['new_column_name'] = values`\n",
        "        * `values` can be a scalar (same value for all rows), a list or NumPy array (must match DataFrame length), or a Pandas `Series` (will align by index).\n",
        "    * **Using `assign()`:** `df_new = df.assign(new_col_name = expression)`\n",
        "        * Creates a *new* DataFrame with the added column(s), leaving the original DataFrame unchanged.\n",
        "        * Allows creating multiple columns at once.\n",
        "        * Can use lambda functions to define new columns based on existing ones.\n",
        "        * *Reference: [18]*"
      ],
      "metadata": {
        "id": "reOlBAJ0mPV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ops = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [10, 20, 30, 40]})\n",
        "\n",
        "print(\"Original DataFrame for ops:\\n\", df_ops)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame for ops:\n",
            "    A   B\n",
            "0  1  10\n",
            "1  2  20\n",
            "2  3  30\n",
            "3  4  40\n"
          ]
        }
      ],
      "execution_count": 60,
      "metadata": {
        "id": "Bl_lXy8BmPV6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb5314b-c01f-4b47-d2eb-f577217d958d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Direct assignment - new column 'C' as sum of A and B\n",
        "df_ops['C'] = df_ops['A'] + df_ops['B']\n",
        "\n",
        "print(\"\\nAfter adding 'C' (direct assignment):\\n\", df_ops)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvywgxHAQW8r",
        "outputId": "fef25652-a11f-4a2c-9ed8-8d8baf692426"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After adding 'C' (direct assignment):\n",
            "    A   B   C\n",
            "0  1  10  11\n",
            "1  2  20  22\n",
            "2  3  30  33\n",
            "3  4  40  44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using assign() - new column 'D' as A * 2\n",
        "# Original df_ops is NOT modified by assign unless reassigned: df_ops = df_ops.assign(...)\n",
        "\n",
        "df_ops_assigned = df_ops.assign(D = df_ops['A'] * 2)\n",
        "\n",
        "print(\"\\nNew DataFrame with 'D' (using assign):\\n\", df_ops_assigned)\n",
        "\n",
        "print(\"\\nOriginal df_ops after assign (unchanged from C addition):\\n\", df_ops)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAvTdMnEQkzT",
        "outputId": "bfc1e578-193d-4e2c-cbbf-1afcfc6d21f3"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "New DataFrame with 'D' (using assign):\n",
            "    A   B   C  D\n",
            "0  1  10  11  2\n",
            "1  2  20  22  4\n",
            "2  3  30  33  6\n",
            "3  4  40  44  8\n",
            "\n",
            "Original df_ops after assign (unchanged from C addition):\n",
            "    A   B   C\n",
            "0  1  10  11\n",
            "1  2  20  22\n",
            "2  3  30  33\n",
            "3  4  40  44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_ops_lambda = df_ops.assign(E = lambda x: x['A'] - x['B'], F = lambda x: x['C'] * 0.1)\n",
        "\n",
        "print(\"\\nNew DataFrame with 'E' and 'F' (assign with lambda):\\n\", df_ops_lambda)\n",
        "\n",
        "# Explanation: `assign()` is often preferred in method chains for a more functional style.\n",
        "df_ops_assigned = df_ops.assign(D = df_ops['A'] * 2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZoLmJKgRJ6u",
        "outputId": "e4fc1188-690d-437b-88b6-e75ff931b0b7"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "New DataFrame with 'E' and 'F' (assign with lambda):\n",
            "    A   B   C   E    F\n",
            "0  1  10  11  -9  1.1\n",
            "1  2  20  22 -18  2.2\n",
            "2  3  30  33 -27  3.3\n",
            "3  4  40  44 -36  4.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Modifying Columns:**\n",
        "    * Simply assign new values to an existing column: `df['existing_column'] = new_values`."
      ],
      "metadata": {
        "id": "UYalNAeHmPV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBefore modifying column 'A' in df_ops:\\n\", df_ops)\n",
        "df_ops['A'] = df_ops['A'] * 10\n",
        "print(\"\\nAfter modifying column 'A' in df_ops:\\n\", df_ops)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Before modifying column 'A' in df_ops:\n",
            "    A   B   C\n",
            "0  1  10  11\n",
            "1  2  20  22\n",
            "2  3  30  33\n",
            "3  4  40  44\n",
            "\n",
            "After modifying column 'A' in df_ops:\n",
            "     A   B   C\n",
            "0  10  10  11\n",
            "1  20  20  22\n",
            "2  30  30  33\n",
            "3  40  40  44\n"
          ]
        }
      ],
      "execution_count": 69,
      "metadata": {
        "id": "_2tNUVoGmPV6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "037bf581-b433-4386-cfdb-e29c10ff0ee4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Deleting Columns:**\n",
        "    * **Using `del` keyword:** `del df['column_name']`\n",
        "        * Modifies the DataFrame in-place.\n",
        "    * **Using `pop()` method:** `popped_series = df.pop('column_name')`\n",
        "        * Removes the column and returns it as a `Series`.\n",
        "        * Modifies the DataFrame in-place.\n",
        "    * **Using `drop()` method:** `df_new = df.drop(columns=['col1', 'col2'], inplace=False)`\n",
        "        * More flexible. Can drop rows or columns.\n",
        "        * `columns=['col_name']` or `labels=['col_name'], axis=1` to specify column(s).\n",
        "        * `labels=['row_index'], axis=0` to specify row(s).\n",
        "        * `inplace=False` (default): Returns a new DataFrame with the column(s) dropped. Original is unchanged.\n",
        "        * `inplace=True`: Modifies the DataFrame directly and returns `None`."
      ],
      "metadata": {
        "id": "faPOcW83mPV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using df_ops_lambda from previous example for deletion\n",
        "df_to_delete_from = df_ops_lambda.copy() # Work on a copy\n",
        "\n",
        "print(\"\\nDataFrame before deletions:\\n\", df_to_delete_from)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame before deletions:\n",
            "    A   B   C   E    F\n",
            "0  1  10  11  -9  1.1\n",
            "1  2  20  22 -18  2.2\n",
            "2  3  30  33 -27  3.3\n",
            "3  4  40  44 -36  4.4\n"
          ]
        }
      ],
      "execution_count": 79,
      "metadata": {
        "id": "HgKTDymmmPV6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32941bd8-7436-427f-82f6-8d23504e4b9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using del\n",
        "\n",
        "if 'F' in df_to_delete_from.columns:\n",
        "  del df_to_delete_from['F']\n",
        "  print(\"\\After deleting 'F' using del\\\", df_to_delete_from\")\n",
        "\n",
        "print(df_to_delete_from)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQm-_t5NSONp",
        "outputId": "1b34cfc4-f4f3-4a67-ab8f-382acea44990"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\After deleting 'F' using del\", df_to_delete_from\n",
            "   A   B   C   E\n",
            "0  1  10  11  -9\n",
            "1  2  20  22 -18\n",
            "2  3  30  33 -27\n",
            "3  4  40  44 -36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using pop()\n",
        "\n",
        "if 'E' in df_to_delete_from.columns:\n",
        "  e_series = df_to_delete_from.pop('E')\n",
        "  print(\"\\nAfter popping 'E':\\n\", df_to_delete_from)\n",
        "  print(\"Popped series 'E':\\n\", e_series)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tQEGteITTda",
        "outputId": "b8574d3d-2568-46e9-b88d-c920e9e37fb9"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After popping 'E':\n",
            "    A   B   C\n",
            "0  1  10  11\n",
            "1  2  20  22\n",
            "2  3  30  33\n",
            "3  4  40  44\n",
            "Popped series 'E':\n",
            " 0    -9\n",
            "1   -18\n",
            "2   -27\n",
            "3   -36\n",
            "Name: E, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using drop() (inplace=False by default)\n",
        "\n",
        "df_dropped_C = df_to_delete_from.drop(columns=['C'])\n",
        "print(\"\\nNew DataFrame after dropping 'C' (original unchanged):\\n\", df_dropped_C)\n",
        "print(\"\\nOriginal df_to_delete_from after non-inplace drop:\\n\", df_to_delete_from)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQe6M-bUTYro",
        "outputId": "1aafabc8-381c-4663-d18c-83a7b7a73179"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "New DataFrame after dropping 'C' (original unchanged):\n",
            "    A   B\n",
            "0  1  10\n",
            "1  2  20\n",
            "2  3  30\n",
            "3  4  40\n",
            "\n",
            "Original df_to_delete_from after non-inplace drop:\n",
            "    A   B   C\n",
            "0  1  10  11\n",
            "1  2  20  22\n",
            "2  3  30  33\n",
            "3  4  40  44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using drop() (inplace=True)\n",
        "if 'B' in df_to_delete_from.columns:\n",
        "  df_to_delete_from.drop(columns=['B'], inplace=True)\n",
        "  print(\"\\nOriginal df_to_delete_from after dropping 'B' (inplace=True):\\n\", df_to_delete_from)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJdZd6WdTfO2",
        "outputId": "1e75b14c-5a61-4bfa-db9d-3459d56b4987"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original df_to_delete_from after dropping 'B' (inplace=True):\n",
            "    A   C\n",
            "0  1  11\n",
            "1  2  22\n",
            "2  3  33\n",
            "3  4  44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.6 Saving DataFrames**\n",
        "\n",
        "Pandas allows DataFrames to be saved to various file formats. `to_csv()` is very common.\n",
        "\n",
        "* **`df.to_csv()` Function:** Writes the DataFrame to a Comma-Separated Values (CSV) file.\n",
        "    \n",
        "    * **Common Parameters:**\n",
        "        * `path_or_buf`: File path or object. If `None`, the result is returned as a string. E.g., `'output.csv'`. *[34]*\n",
        "        * `sep`: Delimiter to use in the output file (default is `','`).\n",
        "        * `index=True/False`: Whether to write the DataFrame index as a column in the CSV. Default is `True`. Often set to `False` if the index is a default integer index (0, 1, 2...) and not meaningful data.\n",
        "        * `header=True/False`: Whether to write the column names as the first line (header row). Default is `True`.\n",
        "        * `mode='w'/'a'`: Write mode. `'w'` to overwrite the file if it exists (default), `'a'` to append to an existing file.\n",
        "        * `encoding`: Specifies the character encoding to use, e.g., `'utf-8'` (common).\n",
        "        * `columns`: Optional list of column names to write. If specified, only these columns are saved."
      ],
      "metadata": {
        "id": "SgOPt8cmmPV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "df_to_save = pd.DataFrame({\n",
        "    'Name': ['Laptop', 'Mouse', 'Keyboard'],\n",
        "    'Price': [1200, 25, 75],\n",
        "    'Quantity': [5, 10, 7]\n",
        "})\n",
        "\n",
        "print(\"\\nDataFrame to be saved:\\n\", df_to_save)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame to be saved:\n",
            "        Name  Price  Quantity\n",
            "0    Laptop   1200         5\n",
            "1     Mouse     25        10\n",
            "2  Keyboard     75         7\n",
            "\n",
            "Saved to output_with_index_header.csv (with index and header)\n",
            "Saved to output_no_index.csv (no index, with header)\n",
            "Saved to output_custom.tsv (tab-separated, no index, no header)\n"
          ]
        }
      ],
      "execution_count": 85,
      "metadata": {
        "id": "syZ0HzRQmPV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba23dc79-7a28-401b-f594-65da2bfe5919"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save with index and header (default)\n",
        "try:\n",
        "    df_to_save.to_csv('output_with_index_header.csv', encoding='utf-8')\n",
        "    print(\"\\nSaved to output_with_index_header.csv (with index and header)\")\n",
        "        # Check file content:\n",
        "        # ,Name,Price,Quantity\n",
        "        # 0,Laptop,1200,5\n",
        "        # 1,Mouse,25,10\n",
        "        # 2,Keyboard,75,7\n",
        "\n",
        "    # Save without index, but with header\n",
        "    df_to_save.to_csv('output_no_index.csv', index=False, encoding='utf-8')\n",
        "    print(\"Saved to output_no_index.csv (no index, with header)\")\n",
        "        # Check file content:\n",
        "        # Name,Price,Quantity\n",
        "        # Laptop,1200,5\n",
        "        # Mouse,25,10\n",
        "        # Keyboard,75,7\n",
        "\n",
        "    # Save without index and without header, using tab separator\n",
        "    df_to_save.to_csv('output_custom.tsv', sep='\\t', index=False, header=False, encoding='utf-8')\n",
        "    print(\"Saved to output_custom.tsv (tab-separated, no index, no header)\")\n",
        "        # Check file content:\n",
        "        # Laptop    1200    5\n",
        "        # Mouse     25      10\n",
        "        # Keyboard  75      7\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error saving files: {e}. This might be a permissions issue.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw80B0udUMWP",
        "outputId": "88a53ba3-d053-4d73-c0db-197525eb1ede"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved to output_with_index_header.csv (with index and header)\n",
            "Saved to output_no_index.csv (no index, with header)\n",
            "Saved to output_custom.tsv (tab-separated, no index, no header)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explanation: `index=False` is commonly used when the DataFrame's index is just a default RangeIndex\n",
        "# and doesn't represent actual data you need to save. *[34, 35]*"
      ],
      "metadata": {
        "id": "TTRiEOWSUlC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Module 2: Practice Questions**"
      ],
      "metadata": {
        "id": "j6DUWIYjmPV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. **Coding:** Create a Pandas `Series` named `s_colors` from the list `['Red', 'Green', 'Blue']` with a custom index `['R', 'G', 'B']`. Print the `Series`."
      ],
      "metadata": {
        "id": "1Ka4ELEIUwIt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vs1pvYGTUxsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. **MCQ:** What is the primary data structure in Pandas for handling 2-dimensional tabular data?\n",
        "    * A) Series\n",
        "    * B) NumPy Array\n",
        "    * C) DataFrame\n",
        "    * D) Python List"
      ],
      "metadata": {
        "id": "zdrMRpKzUzZX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M7SacpEOU0j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. **Coding:** Create a Pandas `DataFrame` named `df_students` from the following dictionary. Print the DataFrame."
      ],
      "metadata": {
        "id": "euXhIeEfU0wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student_data = {'StudentID': [101, 102, 103],\n",
        "                    'Name': ['Alice', 'Bob', 'Carol'],\n",
        "                    'Score': [85, 92, 78]}"
      ],
      "outputs": [],
      "execution_count": 88,
      "metadata": {
        "id": "ysYdQz-MmPV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. **Short Answer:** What is a \"header row\" in the context of a CSV file?\n"
      ],
      "metadata": {
        "id": "e_0XjZKUmPV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PsYdN-_SU60Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. **Function Parameter:** In `pd.read_csv()`, what does the `sep` parameter control?\n"
      ],
      "metadata": {
        "id": "BmMBA5WJU7yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. **Function Parameter:** If your CSV file does not have a header row, what value shoul you pass to the `header` parameter in `pd.read_csv()`?\n"
      ],
      "metadata": {
        "id": "lg5S24kOU9LK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. **Coding:** Assume you have a CSV file named `products.csv` that has no header and contains product names and prices. Write the Python code to read this file into a DataFrame, providing column names 'ProductName' and 'ProductPrice'.\n"
      ],
      "metadata": {
        "id": "8ydhFULmVB-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G_RiXTmXVIXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. **Pandas Method:** Which method would you use to view the first 7 rows of a DataFrame named `df_data`?"
      ],
      "metadata": {
        "id": "4oCwrazcVIx_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iCmMlbFxVJO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. **Pandas Method:** Which method provides a quick statistical summary (mean, std, min, max, etc.) of the numerical columns in a DataFrame?"
      ],
      "metadata": {
        "id": "yznO013HVLL7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "geRnnKdmVNMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. **Attribute:** Which DataFrame attribute returns a tuple representing its dimensions (rows, columns)?\n"
      ],
      "metadata": {
        "id": "UfidUm5EVNgp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JSH0mpDVVO16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. **Short Answer:** What is the key difference between selecting data using `.loc[]` and `.iloc[]`?"
      ],
      "metadata": {
        "id": "y8NNPiUIVPDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "27. **Coding:** Given `df_students` from question 18, write code to select only the 'Name' column."
      ],
      "metadata": {
        "id": "jEVxVopGVQkz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZipcWH7VVSQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. **Coding:** Given `df_students` from question 18, write code to select the row for the student with `StudentID` 102 using `.loc[]` (assuming `StudentID` is set as the index first, or by boolean indexing if not)."
      ],
      "metadata": {
        "id": "Z_icasz2VSed"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L8Ocnn4GVTst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "29. **Coding:** Given `df_students` from question 18, write code to select the first two rows using `.iloc[]`."
      ],
      "metadata": {
        "id": "lZslUTXiVT7u"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lQUhQm3uVVXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. **Coding:** Add a new column named 'Grade' to `df_students`. For simplicity, assign 'Pass' to all students."
      ],
      "metadata": {
        "id": "kkV0h-7FVVkD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Am1HbeO8VXe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. **Pandas Method:** Which method can be used to add new columns to a DataFrame while returning a *new* DataFrame (leaving the original unchanged)?"
      ],
      "metadata": {
        "id": "Y_lwZtOsVXti"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aNUaStvHVgbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. **Coding:** Delete the 'Score' column from `df_students` *in-place*.\n"
      ],
      "metadata": {
        "id": "hweS6K_AVZde"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PU4zKixQVboB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. **Pandas Method:** What is the purpose of the `index=False` parameter in the `df.to_csv()` method?\n"
      ],
      "metadata": {
        "id": "Fu6w5ttdVb2d"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FRjfsx1JVht0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. **Coding:** Save the `df_students` DataFrame (after any modifications) to a CSV file named `student_records.csv` without writing the index.\n"
      ],
      "metadata": {
        "id": "NMlZJmEmVFFA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m4N6n4q2VlDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. **True/False:** When creating a DataFrame from a list of dictionaries, Pandas will raise an error if the dictionaries have different keys."
      ],
      "metadata": {
        "id": "6YBVJoa2VlOa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u7Fh7r-LVoUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. **Short Answer:** What information does `df.info()` provide about a DataFrame? List at least three items."
      ],
      "metadata": {
        "id": "BLlFNc00Vogr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kSNTFTPpVpnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. **Coding:** Create a Series with 5 random numbers and a custom character index ('v', 'w', 'x', 'y', 'z')."
      ],
      "metadata": {
        "id": "dqE3U7y9Vp1k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1SF6wBv-Vq9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. **Conceptual:** If you use `df.describe(include='object')`, what kind of statistics would you expect to see?"
      ],
      "metadata": {
        "id": "spTklNLeVrN3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s-lTuBvOVsdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. **Selection:** How would you select all rows and only the first and third columns of a DataFrame `df` using `.iloc[]`?"
      ],
      "metadata": {
        "id": "zFt8JNF3Vs7N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Gil7eSPVuCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. **Selection:** How would you select rows where a column 'Age' is greater than 30 in a DataFrame `df` using `.loc[]`?"
      ],
      "metadata": {
        "id": "7bRPgJN9VuQA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OEDzsUDCVvvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Module 3: Data Cleaning and Preparation**"
      ],
      "metadata": {
        "id": "FkMO3BeIVv96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data cleaning is a critical step in the data analysis pipeline. It involves identifying and correcting (or removing) errors, inconsistencies, and inaccuracies in datasets to ensure data quality for analysis and modeling.\n"
      ],
      "metadata": {
        "id": "pVq9Efh-VxNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1 Handling Missing Data**\n",
        "\n",
        "Missing data, often represented as `NaN` (Not a Number) in Pandas, can significantly skew analysis results and negatively impact machine learning model performance if not handled appropriately. *[36]*"
      ],
      "metadata": {
        "id": "e63e3A8YVzBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Identifying Missing Values:**\n",
        "    * `df.isnull()` or `df.isna()`: These methods return a DataFrame of the same shape as the input, with `True` where values are missing (`NaN`) and `False` otherwise. *[38]*\n",
        "    * `df.notnull()`: The opposite of `isnull()`; returns `True` for non-missing values.\n",
        "    * `df.isnull().sum()`: This is a very common and useful command. It returns a `Series` showing the count of missing values in each column. *[38]*\n",
        "    * `df.isnull().sum().sum()`: Returns the total number of missing values in the entire DataFrame. *[39]*\n",
        "    * `df.isnull().values.any()`: Returns `True` if there is at least one missing value anywhere in the DataFrame. Useful for a quick check. *[39]*"
      ],
      "metadata": {
        "id": "QUb7dmQxV0Zh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with missing values\n",
        "data_missing_raw = {\n",
        "    'col1': [1, 2, np.nan, 4, np.nan, 7],\n",
        "    'col2': ['A', 'B', 'C', np.nan, 'E', 'F'],\n",
        "    'col3': [10.0, 11.5, 12.3, np.nan, 14.2, np.nan],\n",
        "    'col4': [True, False, True, True, False, False]\n",
        "}"
      ],
      "outputs": [],
      "execution_count": 89,
      "metadata": {
        "id": "64gb5WHomPV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_missing = pd.DataFrame(data_missing_raw)\n",
        "print(\"Original DataFrame with missing values:\\n\", df_missing)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owmLqisOWk98",
        "outputId": "903f9fae-1b8a-454c-f0c9-2f71c0c74692"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame with missing values:\n",
            "    col1 col2  col3   col4\n",
            "0   1.0    A  10.0   True\n",
            "1   2.0    B  11.5  False\n",
            "2   NaN    C  12.3   True\n",
            "3   4.0  NaN   NaN   True\n",
            "4   NaN    E  14.2  False\n",
            "5   7.0    F   NaN  False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check missing values\n",
        "print(\"\\nBoolean DataFrame of missing values (isnull()):\\n\", df_missing.isnull())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4OCbCU5Wpyn",
        "outputId": "aee07688-5a86-4e33-cc61-b9047aca9c21"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Boolean DataFrame of missing values (isnull()):\n",
            "     col1   col2   col3   col4\n",
            "0  False  False  False  False\n",
            "1  False  False  False  False\n",
            "2   True  False  False  False\n",
            "3  False   True   True  False\n",
            "4   True  False  False  False\n",
            "5  False  False   True  False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count missing values per column\n",
        "print(\"\\nCount of missing values per column:\\n\", df_missing.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qguCFJRkWsAc",
        "outputId": "92649300-31a9-4715-bd0f-4096afc70c3a"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Count of missing values per column:\n",
            " col1    2\n",
            "col2    1\n",
            "col3    2\n",
            "col4    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Total missing values in DataFrame\n",
        "print(\"\\nTotal missing values in DataFrame:\", df_missing.isnull().sum().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OU8iG4rWt8A",
        "outputId": "02c0f77f-76e3-4006-f4e5-bf15522b1e62"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total missing values in DataFrame: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if any missing values exist\n",
        "print(\"\\nAre there any missing values at all?\", df_missing.isnull().values.any())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1gMJFv5Wvjw",
        "outputId": "a2f304ad-414e-4c35-9586-e19281e07d3b"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Are there any missing values at all? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 4))  # Set figure size\n",
        "sns.heatmap(df_missing.isnull(), cbar=False, cmap='viridis')\n",
        "\n",
        "plt.title(\"Heatmap of Missing Values\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "WcE2v5yzXASH",
        "outputId": "4f604951-db41-4f2a-9025-7271089834aa"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `[Image: Heatmap visualization of df_missing.isnull(). This would show a grid where cells are colored (e.g., yellow for True/missing, purple for False/not missing), giving a quick visual overview of missing data patterns.]`\n",
        "        * You can create this with `sns.heatmap(df_missing.isnull(), cbar=False, cmap='viridis')`."
      ],
      "metadata": {
        "id": "sujzDoWpmPV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Strategies for Handling Missing Values:**\n",
        "    The choice of strategy depends on several factors: the amount of missing data, the nature of the variable, the mechanism causing data to be missing, and the goals of the analysis. The two primary approaches are **deletion** and **imputation**.\n"
      ],
      "metadata": {
        "id": "O7fuYVJGW7-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "    1.  **Deletion: Removing rows or columns with missing values.**\n",
        "        * **`df.dropna()`**: Removes rows or columns containing missing values. *[37]*\n",
        "            * `axis=0` (default): Drops **rows** that have at least one `NaN` (or all `NaN`s if `how='all'`).\n",
        "            * `axis=1`: Drops **columns** that have at least one `NaN` (or all `NaN`s if `how='all'`).\n",
        "            * `how='any'` (default): Drops the row/column if **any** `NaN` values are present.\n",
        "            * `how='all'`: Drops the row/column only if **all** its values are `NaN`.\n",
        "            * `thresh=n`: Keeps only rows/columns that have at least `n` non-`NaN` values. For example, `thresh=len(df.columns)-2` would keep rows with at most 2 NaNs.\n",
        "            * `subset=['col_name1', 'col_name2']`: Restricts the `NaN` check to only the specified columns when deciding to drop rows (or specified index labels when dropping columns). *[37]*\n",
        "            * `inplace=True/False` (default `False`): Modifies the DataFrame directly if `True`, or returns a new DataFrame if `False`."
      ],
      "metadata": {
        "id": "4TpRHjt_X20F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nOriginal df_missing before dropna operations:\\n\", df_missing)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original df_missing before dropna operations:\n",
            "    col1 col2  col3   col4\n",
            "0   1.0    A  10.0   True\n",
            "1   2.0    B  11.5  False\n",
            "2   NaN    C  12.3   True\n",
            "3   4.0  NaN   NaN   True\n",
            "4   NaN    E  14.2  False\n",
            "5   7.0    F   NaN  False\n"
          ]
        }
      ],
      "execution_count": 99,
      "metadata": {
        "id": "eMOPsgQJmPV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "738bd7cb-5f2f-49ba-d77f-820858e00fb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with ANY missing values\n",
        "df_dropped_rows_any = df_missing.dropna()  # Default axis=0 (drop rows)\n",
        "print(\"\\nDataFrame after dropping rows with any NaN:\\n\", df_dropped_rows_any)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR31obX1YOAW",
        "outputId": "e828b498-7fce-4a9f-a5c6-27f6286b80ed"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame after dropping rows with any NaN:\n",
            "    col1 col2  col3   col4\n",
            "0   1.0    A  10.0   True\n",
            "1   2.0    B  11.5  False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns with ANY missing values\n",
        "df_dropped_cols_any = df_missing.dropna(axis=1)  # Default is how='any'\n",
        "print(\"\\nDataFrame after dropping columns with any NaN:\\n\", df_dropped_cols_any)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTkdp6qgYRIx",
        "outputId": "05cb0a59-5397-4c32-bbd6-0a3fd93f0a90"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame after dropping columns with any NaN:\n",
            "     col4\n",
            "0   True\n",
            "1  False\n",
            "2   True\n",
            "3   True\n",
            "4  False\n",
            "5  False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where 'col1' OR 'col3' is NaN\n",
        "df_dropped_subset = df_missing.dropna(subset=['col1', 'col3'])  # Default is how='any'\n",
        "print(\"\\nDataFrame after dropping rows if NaN in 'col1' or 'col3':\\n\", df_dropped_subset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejNycWx0YSvf",
        "outputId": "018f85f1-58c1-41d2-ca36-6e9a898f5de9"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame after dropping rows if NaN in 'col1' or 'col3':\n",
            "    col1 col2  col3   col4\n",
            "0   1.0    A  10.0   True\n",
            "1   2.0    B  11.5  False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep rows that have at least 3 non-NaN values (out of 4 columns)\n",
        "df_thresh = df_missing.dropna(thresh=3)\n",
        "print(\"\\nDataFrame after keeping rows with at least 3 non-NaNs:\\n\", df_thresh)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEPks5uNYULX",
        "outputId": "8af02d13-452a-4810-a9ae-58cba780a844"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame after keeping rows with at least 3 non-NaNs:\n",
            "    col1 col2  col3   col4\n",
            "0   1.0    A  10.0   True\n",
            "1   2.0    B  11.5  False\n",
            "2   NaN    C  12.3   True\n",
            "4   NaN    E  14.2  False\n",
            "5   7.0    F   NaN  False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify original df_missing remains unchanged\n",
        "print(\"\\nOriginal df_missing after non-inplace dropna:\\n\", df_missing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36qAcWDqYYVl",
        "outputId": "6452e435-d0c0-4fbc-f4a1-c0b6cf95afb2"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original df_missing after non-inplace dropna:\n",
            "    col1 col2  col3   col4\n",
            "0   1.0    A  10.0   True\n",
            "1   2.0    B  11.5  False\n",
            "2   NaN    C  12.3   True\n",
            "3   4.0  NaN   NaN   True\n",
            "4   NaN    E  14.2  False\n",
            "5   7.0    F   NaN  False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Pros & Cons of Deletion:**\n",
        "            * **Pros:** Simple to implement.\n",
        "            * **Cons:** Can lead to significant loss of data if missing values are widespread, potentially biasing the results or reducing statistical power.\n",
        "             * Only suitable if data is missing completely at random and the proportion is small.\n"
      ],
      "metadata": {
        "id": "Nfv8KndYmPV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    2.  **Imputation: Replacing missing values with estimated or calculated values.** *[36]*\n",
        "        * **`df.fillna(value, method=None, inplace=False)`**: Fills `NaN` values using a specified method or value.\n",
        "            * **Filling with a Constant:**\n",
        "                * `df.fillna(0)`: Replaces all `NaN`s with 0.\n",
        "                * `df['column_name'].fillna('Unknown')`: Replaces `NaN`s in a specific column with \"Unknown\".\n",
        "            * **Filling with Statistical Measures (Mean, Median, Mode):**\n",
        "                * **Mean:** `df['numerical_col'].fillna(df['numerical_col'].mean())`\n",
        "                    * Good for numerical data that is somewhat normally distributed. Sensitive to outliers. *[41]*\n",
        "                * **Median:** `df['numerical_col'].fillna(df['numerical_col'].median())`\n",
        "                    * Good for numerical data, especially if it's skewed or has outliers, as the median is more robust.\n",
        "                * **Mode:** `df['categorical_col'].fillna(df['categorical_col'].mode()[0])`\n",
        "                    * Suitable for categorical data. `mode()` returns a Series (as there can be multiple modes), so `[0]` is used to get the first mode. *[43]*\n",
        "            * **Filling with Forward Fill (`ffill`) or Backward Fill (`bfill`):**\n",
        "                * `method='ffill'`: Propagates the last valid (non-`NaN`) observation forward to fill the `NaN`.\n",
        "                * `method='bfill'` (or `backfill`): Uses the next valid observation to fill the `NaN`.\n",
        "                * Useful for time series data or when the order of data matters.\n",
        "            * **Grouped Imputation:** Filling missing values based on statistics from a specific group within the data.\n",
        "                * Example: Impute missing 'Income' with the mean 'Income' of the corresponding 'JobType'.\n",
        "                * Requires using `groupby()` and `transform()`."
      ],
      "metadata": {
        "id": "-2OmybYrYwcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d7Ixpoc_Yh5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_impute = df_missing.copy() # Work on a copy\n",
        "print(\"\\nOriginal df_missing for imputation:\\n\", df_impute)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original df_missing for imputation:\n",
            "    col1 col2  col3   col4\n",
            "0   1.0    A  10.0   True\n",
            "1   2.0    B  11.5  False\n",
            "2   NaN    C  12.3   True\n",
            "3   4.0  NaN   NaN   True\n",
            "4   NaN    E  14.2  False\n",
            "5   7.0    F   NaN  False\n"
          ]
        }
      ],
      "execution_count": 105,
      "metadata": {
        "id": "4y2AkNRemPV8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15954910-79f8-4748-cbd7-38680217ac4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute 'col1' (numerical) with its mean\n",
        "mean_col1 = df_impute['col1'].mean()\n",
        "df_impute['col1_imputed_mean'] = df_impute['col1'].fillna(mean_col1)\n",
        "print(\"\\nImputing 'col1' with mean:\\n\", df_impute[['col1', 'col1_imputed_mean']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhldrfYTY9W6",
        "outputId": "f2476268-853e-4eb5-f721-ac5319a853f0"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Imputing 'col1' with mean:\n",
            "    col1  col1_imputed_mean\n",
            "0   1.0                1.0\n",
            "1   2.0                2.0\n",
            "2   NaN                3.5\n",
            "3   4.0                4.0\n",
            "4   NaN                3.5\n",
            "5   7.0                7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute 'col2' (categorical) with its mode\n",
        "mode_col2 = df_impute['col2'].mode()[0] # Get the first mode\n",
        "df_impute['col2_imputed_mode'] = df_impute['col2'].fillna(mode_col2)\n",
        "print(\"\\nImputing 'col2' with mode ('%s'):\\n\" % mode_col2, df_impute[['col2', 'col2_imputed_mode']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRLyQoZtZCDi",
        "outputId": "da75e059-e932-491f-cc84-ff8b9589740a"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Imputing 'col2' with mode ('A'):\n",
            "   col2 col2_imputed_mode\n",
            "0    A                 A\n",
            "1    B                 B\n",
            "2    C                 C\n",
            "3  NaN                 A\n",
            "4    E                 E\n",
            "5    F                 F\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute 'col3' using forward fill\n",
        "df_impute['col3_imputed_ffill'] = df_impute['col3'].fillna(method='ffill')\n",
        "print(\"\\nImputing 'col3' with ffill:\\n\", df_impute[['col3', 'col3_imputed_ffill']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfj52gDJZGgb",
        "outputId": "f0bc6e79-2a65-4345-aea8-48a841c7085b"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Imputing 'col3' with ffill:\n",
            "    col3  col3_imputed_ffill\n",
            "0  10.0                10.0\n",
            "1  11.5                11.5\n",
            "2  12.3                12.3\n",
            "3   NaN                12.3\n",
            "4  14.2                14.2\n",
            "5   NaN                14.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-109-3f810f5caf1d>:2: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df_impute['col3_imputed_ffill'] = df_impute['col3'].fillna(method='ffill')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute 'col3' using backward fill (on a fresh copy for clarity)\n",
        "df_impute_bfill = df_missing.copy()\n",
        "df_impute_bfill['col3_imputed_bfill'] = df_impute_bfill['col3'].fillna(method='bfill')\n",
        "print(\"\\nImputing 'col3' with bfill:\\n\", df_impute_bfill[['col3', 'col3_imputed_bfill']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6O-fY1tZJ-T",
        "outputId": "4af5b9eb-e575-4df3-aa97-87804eee935e"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Imputing 'col3' with bfill:\n",
            "    col3  col3_imputed_bfill\n",
            "0  10.0                10.0\n",
            "1  11.5                11.5\n",
            "2  12.3                12.3\n",
            "3   NaN                14.2\n",
            "4  14.2                14.2\n",
            "5   NaN                 NaN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-110-4c307ff8bec8>:3: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df_impute_bfill['col3_imputed_bfill'] = df_impute_bfill['col3'].fillna(method='bfill')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouped Imputation Example\n",
        "# Suppose we want to fill NaN in col1 based on the mean of col1 for each distinct value in col4 (True/False)\n",
        "# df_missing['col1_imputed_groupmean'] = df_missing.groupby('col4')['col1'].transform(lambda x: x.fillna(x.mean()))\n",
        "# print(\"\\nDataFrame after grouped mean imputation for 'col1' based on 'col4':\\n\", df_missing[['col4','col1','col1_imputed_groupmean']])\n",
        "# The `transform` method applies a function to each group and returns a Series with the same index as the original DataFrame, making assignment easy. *[42]*"
      ],
      "metadata": {
        "id": "fuxY1DyZZPe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Pros & Cons of Imputation:**\n",
        "            * **Pros:** Preserves sample size. Can be more sophisticated than deletion.\n",
        "            * **Cons:** Can introduce bias if the imputation method is not chosen carefully.\n",
        "             * May reduce variance in the data or distort relationships between variables. The imputed values are artificial.\n"
      ],
      "metadata": {
        "id": "Bcujnxu8mPV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* **Flowchart for Handling Missing Data Strategy:**\n",
        "    ```\n",
        "    [Start: Missing Data Identified]\n",
        "        |\n",
        "        V\n",
        "    [1. Assess Missingness]\n",
        "        - Quantify: df.isnull().sum() (count per column), percentage per column.\n",
        "        - Visualize: Heatmap of missing values.\n",
        "        - Understand Pattern (if possible): MCAR, MAR, MNAR? *[36]*\n",
        "            - MCAR (Missing Completely At Random): Missingness is independent of any variable.\n",
        "            - MAR (Missing At Random): Missingness depends on other OBSERVED variables.\n",
        "            - MNAR (Missing Not At Random): Missingness depends on the UNSEEN missing values themselves (hardest to deal with).\n",
        "        |\n",
        "        V\n",
        "    [2. Decision Point: Based on extent and nature]\n",
        "        |\n",
        "        +--------------------------------+---------------------------------+\n",
        "        |                                |                                 |\n",
        "        V                                V                                 V\n",
        "    [IF: Column has VERY HIGH % missing (e.g., >60-70%)] [IF: Small % missing (e.g., <5%) AND seems MCAR] [ELSE: Moderate missingness OR important feature]\n",
        "        |                                |                                 |\n",
        "        V                                V                                 V\n",
        "    [Strategy: Consider Dropping Column] [Strategy: Deletion (Row-wise)]  [Strategy: Imputation]\n",
        "    `df.dropna(axis=1, thresh=...)`      `df.dropna(subset=[...])`        |\n",
        "                                                                          |\n",
        "                                                                          V\n",
        "                                                                    [Choose Imputation Method]\n",
        "                                                                          |\n",
        "                                                 +------------------------+------------------------+\n",
        "                                                 |                        |                        |\n",
        "                                                 V                        V                        V\n",
        "                                           [Numerical Data]        [Categorical Data]       [Time Series/Ordered]\n",
        "                                                 |                        |                        |\n",
        "                                                 V                        V                        V\n",
        "                                           - Mean (if normal, no outliers)  - Mode (most common)     - Forward Fill (ffill)\n",
        "                                           - Median (skewed, outliers)    - New Category (\"Missing\") - Backward Fill (bfill)\n",
        "                                           - Model-based (e.g., KNNImputer, Regression) *[36]*\n",
        "        |                                |                                 |\n",
        "        V                                V                                 V\n",
        "    [3. Apply Chosen Strategy]----------------------------------------------+\n",
        "        |\n",
        "        V\n",
        "    [4. Evaluate & Verify]\n",
        "        - Check `df.isnull().sum()` again.\n",
        "        - Assess if distributions are overly distorted.\n",
        "        - Consider impact on downstream analysis/modeling.\n",
        "        |\n",
        "        V\n",
        "    [End: Missing Data Handled]\n",
        "    ```\n",
        "    * `[Diagram: The above text formatted as a visual flowchart with boxes and arrows.]`\n"
      ],
      "metadata": {
        "id": "FYXPOcqwZiKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2 Handling Duplicates**\n",
        "\n",
        "Duplicate rows can skew analysis (e.g., by over-representing certain observations) and affect model training.\n",
        "\n",
        "* **`df.duplicated(subset=None, keep='first')`**: Returns a boolean `Series` indicating whether each row is a duplicate.\n",
        "    * `subset`: A list of column labels to consider when identifying duplicates. By default, it uses all columns. If `subset=['colA', 'colB']`, a row is a duplicate if its values in 'colA' and 'colB' are identical to those in a prior row.\n",
        "    * `keep='first'` (default): Marks all occurrences of duplicates as `True` except for the *first* one.\n",
        "    * `keep='last'`: Marks all occurrences of duplicates as `True` except for the *last* one.\n",
        "    * `keep=False`: Marks *all* occurrences of duplicate rows as `True`.\n",
        "* **`df.drop_duplicates(subset=None, keep='first', inplace=False)`**: Returns a DataFrame with duplicate rows removed. The parameters `subset`, `keep`, and `inplace` work the same way as in `df.duplicated()`."
      ],
      "metadata": {
        "id": "USuCCW0LmPWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_duplicates_raw = {'colA': [1, 1, 2, 3, 2, 1, 4],\n",
        "                       'colB': ['x', 'x', 'y', 'z', 'y', 'x', 'w'],\n",
        "                       'colC': [100, 100, 200, 300, 200, 100, 400]}\n",
        "df_duplicates = pd.DataFrame(data_duplicates_raw)\n",
        "\n",
        "print(\"Original DataFrame with duplicates:\\n\", df_duplicates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSTCre9saFPk",
        "outputId": "28d5dded-cb8b-4c11-eff0-55e56c4ac2be"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame with duplicates:\n",
            "    colA colB  colC\n",
            "0     1    x   100\n",
            "1     1    x   100\n",
            "2     2    y   200\n",
            "3     3    z   300\n",
            "4     2    y   200\n",
            "5     1    x   100\n",
            "6     4    w   400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBoolean Series of duplicated rows (first occurrence NOT marked as duplicate):\\n\",\n",
        "      df_duplicates.duplicated())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSFGdGIJaNjs",
        "outputId": "c72d8abd-091a-41ad-f4ba-c0dc695d90b3"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Boolean Series of duplicated rows (first occurrence NOT marked as duplicate):\n",
            " 0    False\n",
            "1     True\n",
            "2    False\n",
            "3    False\n",
            "4     True\n",
            "5     True\n",
            "6    False\n",
            "dtype: bool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBoolean Series of duplicated rows (last occurrence NOT marked as duplicate):\\n\",\n",
        "      df_duplicates.duplicated(keep='last'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHYlsmyVaP18",
        "outputId": "da2458ac-736d-4129-eb19-690aab581ec0"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Boolean Series of duplicated rows (last occurrence NOT marked as duplicate):\n",
            " 0     True\n",
            "1     True\n",
            "2     True\n",
            "3    False\n",
            "4    False\n",
            "5    False\n",
            "6    False\n",
            "dtype: bool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBoolean Series of duplicated rows (ALL duplicates marked as True):\\n\",\n",
        "      df_duplicates.duplicated(keep=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bywusoNDaRpb",
        "outputId": "4d747687-1f39-45ff-b200-a8e4665ef1a8"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Boolean Series of duplicated rows (ALL duplicates marked as True):\n",
            " 0     True\n",
            "1     True\n",
            "2     True\n",
            "3    False\n",
            "4     True\n",
            "5     True\n",
            "6    False\n",
            "dtype: bool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicates, keeping the first occurrence by default\n",
        "\n",
        "df_no_duplicates_first = df_duplicates.drop_duplicates() # keep='first' is default\n",
        "print(\"\\nDataFrame after dropping all-column duplicates (keeping first):\\n\", df_no_duplicates_first)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQTrlkhxaUm1",
        "outputId": "6c14a966-5ec1-41be-e25c-5410ea714d38"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame after dropping all-column duplicates (keeping first):\n",
            "    colA colB  colC\n",
            "0     1    x   100\n",
            "2     2    y   200\n",
            "3     3    z   300\n",
            "6     4    w   400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicates based on 'colA' and 'colB' only, keeping the last occurrence\n",
        "df_no_duplicates_subset_last = df_duplicates.drop_duplicates(subset=['colA', 'colB'], keep='last')\n",
        "print(\"\\nDataFrame after dropping duplicates based on 'colA' & 'colB' (keeping last):\\n\", df_no_duplicates_subset_last)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPRKKfmGaX8z",
        "outputId": "e59143b1-2aff-41d4-c0ba-1a19f7d1f936"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame after dropping duplicates based on 'colA' & 'colB' (keeping last):\n",
            "    colA colB  colC\n",
            "3     3    z   300\n",
            "4     2    y   200\n",
            "5     1    x   100\n",
            "6     4    w   400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3 Data Type Conversion**\n",
        "\n",
        "Ensuring columns have the correct data types is essential for accurate computations, analysis, and memory efficiency. Pandas often infers data types during loading (`read_csv`), but sometimes explicit conversion is needed. *[46]*\n",
        "\n",
        "* **`df['column'].astype(new_type)`** or `df.astype({'column1': new_type1, 'column2': new_type2})`\n",
        "    * Converts the data type of one or more columns. *[46]*\n",
        "    * `new_type` can be:\n",
        "        * Python types: `int`, `float`, `str`, `bool`.\n",
        "        * NumPy dtypes: `np.int64`, `np.float64`, `np.bool_`, etc.\n",
        "        * Special Pandas type: `'category'` (for columns with a limited number of unique string values; can save memory and speed up some operations). *[48]*\n",
        "        * Datetime: `'datetime64[ns]'`.\n",
        "    * **Common Conversions & Potential Issues:**\n",
        "        * **Object to Numeric (`int`, `float`):**\n",
        "            * `df['col'].astype(float)` or `df['col'].astype(int)`.\n",
        "            * Will raise a `ValueError` if the column contains non-numeric strings (e.g., '$15,000.00', 'Unknown'). *[46]*\n",
        "            * Requires pre-cleaning (e.g., removing currency symbols, commas, handling non-numeric placeholders) before conversion.\n",
        "        * **Numeric to String:** `df['col'].astype(str)`.\n",
        "        * **Object to Category:** `df['col'].astype('category')`.\n",
        "        * **Object/String to Datetime:** `df['col'].astype('datetime64[ns]')` or, more robustly, `pd.to_datetime(df['col'])`."
      ],
      "metadata": {
        "id": "3EUUaUSjmPWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_types_raw = pd.DataFrame({'A_str_int': ['10', '25', '30', '15'],\n",
        "                                 'B_str_float': ['100.5', '200.0', '350.75', '99.9'],\n",
        "                                 'C_category_like': ['Type1', 'Type2', 'Type1', 'Type3'],\n",
        "                                 'D_mixed': ['50', '60.5', 'Error', '70']})\n",
        "\n",
        "print(\"Original DataFrame dtypes:\\n\", df_types_raw.dtypes)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame dtypes:\n",
            " A_str_int          object\n",
            "B_str_float        object\n",
            "C_category_like    object\n",
            "D_mixed            object\n",
            "dtype: object\n"
          ]
        }
      ],
      "execution_count": 122,
      "metadata": {
        "id": "VXFRyajCmPWK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c39dbdf-6765-4035-e651-a44d50e82fb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All will likely be 'object'\n",
        "\n",
        "df_types_converted = df_types_raw.copy()"
      ],
      "metadata": {
        "id": "O1n_h8_gaslT"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert column 'A_str_int' to integer\n",
        "df_types_converted['A_str_int'] = df_types_converted['A_str_int'].astype(int)\n"
      ],
      "metadata": {
        "id": "CHklvuiha5Ik"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert column 'B_str_float' to float\n",
        "df_types_converted['B_str_float'] = df_types_converted['B_str_float'].astype(float)\n"
      ],
      "metadata": {
        "id": "5IMNMtyma9Fw"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert column 'C_category_like' to category\n",
        "df_types_converted['C_category_like'] = df_types_converted['C_category_like'].astype('category')"
      ],
      "metadata": {
        "id": "GEqvxbFIa-cY"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nDataFrame dtypes after astype():\\n\", df_types_converted.dtypes)\n",
        "print(\"\\nDataFrame after astype():\\n\", df_types_converted.head())# Note: df_types_converted['D_mixed'].astype(float) would raise a ValueError due to 'Error'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOoJglTQbA2X",
        "outputId": "e84a77a1-cbc5-4f2b-ab71-530ad6387dd5"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame dtypes after astype():\n",
            " A_str_int             int64\n",
            "B_str_float         float64\n",
            "C_category_like    category\n",
            "D_mixed              object\n",
            "dtype: object\n",
            "\n",
            "DataFrame after astype():\n",
            "    A_str_int  B_str_float C_category_like D_mixed\n",
            "0         10       100.50           Type1      50\n",
            "1         25       200.00           Type2    60.5\n",
            "2         30       350.75           Type1   Error\n",
            "3         15        99.90           Type3      70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **`pd.to_numeric(arg, errors='raise')`**: Specialized function to convert an argument (Series, list, scalar) to a numeric type. More flexible in handling errors than `astype()`.\n",
        "    * `arg`: The Series or column to convert.\n",
        "    * `errors`: Defines how to handle values that cannot be converted to numeric.\n",
        "        * `'raise'` (default): Raises a `ValueError` if an unparseable value is encountered.\n",
        "        * `'coerce'`: Replaces unparseable values with `NaN`. This is very useful for cleaning. *[48]*\n",
        "        * `'ignore'`: If invalid parsing occurs, returns the input object unchanged (so the column might remain of object type or mixed type).\n",
        "* **`pd.to_datetime(arg, errors='raise', format=None)`**: Converts an argument to datetime objects.\n",
        "    * `arg`: The Series or column to convert.\n",
        "    * `errors`:\n",
        "        * `'raise'` (default): Raises a `ValueError` for unparseable dates.\n",
        "        * `'coerce'`: Replaces unparseable dates with `NaT` (Not a Time, the datetime equivalent of `NaN`).\n",
        "        * `'ignore'`: Returns the input if unparseable.\n",
        "    * `format`: A string representing the expected date format (e.g., `'%Y-%m-%d'`, `'%d/%m/%Y %H:%M:%S'`). If not provided, Pandas tries to infer the format, which is flexible but can be slower or ambiguous for mixed formats. Providing a format string improves performance and reliability if your dates are consistent."
      ],
      "metadata": {
        "id": "3YAYZKBumPWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample DataFrame\n",
        "df_convert_robust = pd.DataFrame({\n",
        "    'price_str': ['$100.50', '50', 'NotANumber', ' $75.20 '],\n",
        "    'date_str': ['2025-01-15', '20/02/2025', 'Invalid Date', 'Mar 3, 2025'],\n",
        "    'value_str': ['123', '45.6', '789', 'non_numeric']\n",
        "})\n"
      ],
      "outputs": [],
      "execution_count": 128,
      "metadata": {
        "id": "EhbFTorYmPWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean and convert price_str to numeric, coercing errors\n",
        "df_convert_robust['price_cleaned'] = df_convert_robust['price_str'].str.strip()  # Remove extra spaces\n",
        "df_convert_robust['price_cleaned'] = df_convert_robust['price_cleaned'].str.replace(r'[$,]', '', regex=True)\n",
        "df_convert_robust['price_numeric'] = pd.to_numeric(df_convert_robust['price_cleaned'], errors='coerce')\n"
      ],
      "metadata": {
        "id": "ffaSO_mEbNjt"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nAfter pd.to_numeric on price_str (with cleaning):\\n\", df_convert_robust[['price_str', 'price_numeric']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WhU1i9pbQYq",
        "outputId": "69bedd6a-1e67-4189-ba92-c58307d57557"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After pd.to_numeric on price_str (with cleaning):\n",
            "     price_str  price_numeric\n",
            "0     $100.50          100.5\n",
            "1         50           50.0\n",
            "2  NotANumber            NaN\n",
            "3     $75.20            75.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert date_str to datetime, coercing errors (Pandas tries to infer format)\n",
        "df_convert_robust['date_dt_auto'] = pd.to_datetime(df_convert_robust['date_str'], errors='coerce')\n"
      ],
      "metadata": {
        "id": "6hxhHTFJbTWk"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nAfter pd.to_datetime on date_str (auto-infer, coercing errors):\\n\", df_convert_robust[['date_str', 'date_dt_auto']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ElynT4mbVNp",
        "outputId": "d10e646e-d650-47d6-8218-152af9b50928"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After pd.to_datetime on date_str (auto-infer, coercing errors):\n",
            "        date_str date_dt_auto\n",
            "0    2025-01-15   2025-01-15\n",
            "1    20/02/2025          NaT\n",
            "2  Invalid Date          NaT\n",
            "3   Mar 3, 2025          NaT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of converting specific format (if all were like '20/02/2025')\n",
        "# df_convert_robust['date_dt_specific'] = pd.to_datetime(df_convert_robust['date_str'], format='%d/%m/%Y', errors='coerce')\n",
        "# print(\"\\nAfter pd.to_datetime with specific format '%d/%m/%Y':\\n\", df_convert_robust[['date_str', 'date_dt_specific']])"
      ],
      "metadata": {
        "id": "-M9cisJcbXAN"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert value_str to numeric with coercion\n",
        "df_convert_robust['value_numeric'] = pd.to_numeric(df_convert_robust['value_str'], errors='coerce')"
      ],
      "metadata": {
        "id": "2eMOjxycbYUS"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nConverting 'value_str' to numeric with errors='coerce':\\n\", df_convert_robust[['value_str', 'value_numeric']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpriFP_PbZzq",
        "outputId": "79767d92-c1e1-4cfe-8395-fd5663507ef9"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Converting 'value_str' to numeric with errors='coerce':\n",
            "      value_str  value_numeric\n",
            "0          123          123.0\n",
            "1         45.6           45.6\n",
            "2          789          789.0\n",
            "3  non_numeric            NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.4 Renaming Columns**\n",
        "\n",
        "Column names may need to be changed for clarity, consistency (e.g., standardizing case, removing spaces), or to make them valid Python identifiers if they contain special characters.\n",
        "\n",
        "* **`df.rename(columns={'old_name1': 'new_name1', 'old_name2': 'new_name2'}, inplace=False)`**: Renames one or more columns using a dictionary that maps old names to new names. *[49]*\n",
        "    * Can also rename index labels using the `index` parameter (e.g., `index={'old_idx': 'new_idx'}`).\n",
        "    * `inplace=False` (default): Returns a new DataFrame with renamed columns/index. Original is unchanged.\n",
        "    * `inplace=True`: Modifies the DataFrame directly.\n",
        "* **`df.columns = ['new_name1', 'new_name2', ...]`**: Assigns a new list of column names directly.\n",
        "    * The list provided **must** have the same length as the number of existing columns.\n",
        "    * This method is suitable for renaming all columns at once or if you have a programmatic way to generate all new names. *[50]*"
      ],
      "metadata": {
        "id": "1VpUAWLmmPWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "df_rename_raw = pd.DataFrame({\n",
        "    'First Name': ['John', 'Jane', 'Mike'],\n",
        "    'Last Name': ['Doe', 'Smith', 'Ross'],\n",
        "    'Age_ yrs': [30, 28, 35],\n",
        "    'email address': ['j.doe@mail.com', 'jane@mail.com', 'm.ross@mail.com']\n",
        "})"
      ],
      "outputs": [],
      "execution_count": 136,
      "metadata": {
        "id": "hIfQlkBzmPWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original DataFrame for renaming:\\n\", df_rename_raw)\n",
        "\n",
        "# Rename specific columns using df.rename() (returns a new DataFrame)\n",
        "df_renamed_specific = df_rename_raw.rename(columns={\n",
        "    'First Name': 'FirstName',\n",
        "    'Last Name': 'LastName',\n",
        "    'Age_ yrs': 'Age',\n",
        "    'email address': 'EmailAddress'\n",
        "})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uvg29-M3blK1",
        "outputId": "97ebbae7-d0ac-4ef1-e2e3-96e76717cc2f"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame for renaming:\n",
            "   First Name Last Name  Age_ yrs    email address\n",
            "0       John       Doe        30   j.doe@mail.com\n",
            "1       Jane     Smith        28    jane@mail.com\n",
            "2       Mike      Ross        35  m.ross@mail.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nAfter renaming specific columns (new DataFrame returned):\\n\", df_renamed_specific)\n",
        "print(\"\\nOriginal DataFrame (should be unchanged):\\n\", df_rename_raw)  # Verify original is unchanged\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6urdcAabm_S",
        "outputId": "e4b73b18-fc4b-43b3-d48a-c46bba5a21df"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After renaming specific columns (new DataFrame returned):\n",
            "   FirstName LastName  Age     EmailAddress\n",
            "0      John      Doe   30   j.doe@mail.com\n",
            "1      Jane    Smith   28    jane@mail.com\n",
            "2      Mike     Ross   35  m.ross@mail.com\n",
            "\n",
            "Original DataFrame (should be unchanged):\n",
            "   First Name Last Name  Age_ yrs    email address\n",
            "0       John       Doe        30   j.doe@mail.com\n",
            "1       Jane     Smith        28    jane@mail.com\n",
            "2       Mike      Ross        35  m.ross@mail.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename all columns by assigning a new list to df.columns\n",
        "df_rename_all = df_rename_raw.copy()  # Work on a copy to avoid modifying original DataFrame\n",
        "new_column_names = ['FName', 'LName', 'CurrentAge', 'Email']"
      ],
      "metadata": {
        "id": "VEB52n16bovM"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure correct column name count\n",
        "if len(new_column_names) == len(df_rename_all.columns):\n",
        "    df_rename_all.columns = new_column_names\n",
        "    print(\"\\nAfter renaming all columns by direct assignment:\\n\", df_rename_all)\n",
        "else:\n",
        "    print(\"\\nError: Length of new column names list does not match number of columns.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eSTh32gbqZc",
        "outputId": "8565fc62-d332-4d9d-f1e8-e8891cb94448"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After renaming all columns by direct assignment:\n",
            "   FName  LName  CurrentAge            Email\n",
            "0  John    Doe          30   j.doe@mail.com\n",
            "1  Jane  Smith          28    jane@mail.com\n",
            "2  Mike   Ross          35  m.ross@mail.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean column names systematically\n",
        "df_clean_cols = df_rename_raw.copy()\n",
        "df_clean_cols.columns = (\n",
        "    df_clean_cols.columns\n",
        "    .str.lower()\n",
        "    .str.replace(' ', '_')\n",
        "    .str.replace('[^A-Za-z0-9_]+', '', regex=True)  # Explicit regex=True\n",
        ")\n",
        "\n",
        "print(\"\\nAfter cleaning all column names systematically:\\n\", df_clean_cols)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7Au_C0sb91-",
        "outputId": "9eb9127c-07dd-476e-dc4d-a4a606e8229e"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After cleaning all column names systematically:\n",
            "   first_name last_name  age__yrs    email_address\n",
            "0       John       Doe        30   j.doe@mail.com\n",
            "1       Jane     Smith        28    jane@mail.com\n",
            "2       Mike      Ross        35  m.ross@mail.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Choosing a Method:**\n",
        "    * `rename()`: More flexible for renaming a subset of columns, or renaming based on a function. Safer as it returns a new DataFrame by default. *[49]*\n",
        "    * Direct assignment to `df.columns`: Quicker for a complete overhaul of all column names if you have the full list of new names. Modifies in-place.\n"
      ],
      "metadata": {
        "id": "xspAUqjBmPWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Module 3: Practice Questions**\n"
      ],
      "metadata": {
        "id": "cnz9m4l3cDoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. **True/False:** In Pandas, `NaN` stands for \"Not a Number\" and is used to represent missing data.\n"
      ],
      "metadata": {
        "id": "21ukuvm3cFNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. **Pandas Method:** Which method would you use to get a count of missing values for each column in a DataFrame `df_data`?"
      ],
      "metadata": {
        "id": "XkiOTlcJcGiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. **MCQ:** If you want to drop rows from `df_data` only if *all* values in that row are missing, what parameters would you use with `dropna()`?\n",
        "    * A) `axis=0, how='any'`\n",
        "    * B) `axis=1, how='all'`\n",
        "    * C) `axis=0, how='all'`\n",
        "    * D) `axis=1, how='any'`"
      ],
      "metadata": {
        "id": "6XimNgV5cJFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. **Short Answer:** What is \"imputation\" in the context of handling missing data? Name two common imputation techniques for numerical data."
      ],
      "metadata": {
        "id": "3TQVsNGKcKKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. **Pandas Method:** How can you fill all `NaN` values in a numerical column 'Age' with the median age using `fillna()`? Write the code. (Assume `df_data` is your DataFrame)."
      ],
      "metadata": {
        "id": "wnd1F8NecMBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "46. **Explain:** What is the difference between `method='ffill'` and `method='bfill'` in `fillna()`? When might you use them?"
      ],
      "metadata": {
        "id": "8d71E0fKcNH6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Id6JzhvhcO4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "47. **Coding:** You have a DataFrame `df_sales` with columns 'Product_ID' and 'Region'. Some 'Region' values are missing. You want to impute these missing 'Region' values with the mode of the 'Region' column. Write the code."
      ],
      "metadata": {
        "id": "t6OayBLecPNY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7mySH_tLcRV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "48. **Pandas Method:** Which method is used to identify duplicate rows in a DataFrame?"
      ],
      "metadata": {
        "id": "HnGId7JkcRhY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "49. **Parameter:** In `df.drop_duplicates()`, what does the `keep` parameter control? What are its possible values?"
      ],
      "metadata": {
        "id": "97OHrBfmcSv0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LYgrxFancUF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "50. **Coding:** Given a DataFrame `df_orders`, remove duplicate rows based on all columns, keeping the *last* occurrence. Modify the DataFrame in-place."
      ],
      "metadata": {
        "id": "W-ZIAUSQcUR_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jNr7p0mRcWjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "51. **Pandas Method:** Which method is primarily used to convert the data type of a Pandas Series or DataFrame column?"
      ],
      "metadata": {
        "id": "GrNf4rGBcWwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "52. **Scenario:** You have a column 'Price_Text' with string values like \"25.99\", \"$10.50\". You want to convert this to a numeric (float) column named 'Price_Numeric'. Outline the steps you would take, including handling potential errors."
      ],
      "metadata": {
        "id": "w6-JyU9ScYKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "53. **Function:** Which Pandas function is more robust for converting a column to numeric types and allows you to coerce errors into `NaN`s?"
      ],
      "metadata": {
        "id": "7uXngkJscaKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "54. **Function:** Which Pandas function is used to convert a column of string dates into datetime objects? What parameter helps if the dates are in non-standard formats?"
      ],
      "metadata": {
        "id": "TZvn7SpVccDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "55. **Coding:** Rename a column ' old name with spaces ' to 'new_concise_name' in `df_data`. Do this without modifying `df_data` in-place."
      ],
      "metadata": {
        "id": "xISNjxigcdZE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dvI8-zq7cf4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "56. **Coding:** You have a DataFrame `df_raw` with columns `['User ID', 'Transaction Date', 'Amount (USD)']`. Provide the code to change these column names to `['user_id', 'transaction_date', 'amount_usd']`.\n"
      ],
      "metadata": {
        "id": "iuitvbXKcgHg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UFWrygvAciEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "57. **Critical Thinking:** Why is it generally better to use `errors='coerce'` with `pd.to_numeric` or `pd.to_datetime` during initial data cleaning rather than letting the operation fail?"
      ],
      "metadata": {
        "id": "IAyqMZLeciSD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RBLUFC7Qcj0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "58. **Pandas Method:** How can you check if *any* value in an entire DataFrame `df` is `NaN` with a single boolean output?"
      ],
      "metadata": {
        "id": "_9HT6MU9ckKT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-UulAJJjclcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "59. **Imputation Strategy:** If a numerical column has many outliers, would mean imputation or median imputation be a more robust choice? Why?"
      ],
      "metadata": {
        "id": "v2pNvQBhcltK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ejdJZRlCcmwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "60. **Code:** Given `df_missing` from the module notes, drop any column that has more than 1 missing value. (Hint: You might need to iterate or use a comprehension with `df.isnull().sum()`)."
      ],
      "metadata": {
        "id": "f-xXJ4Dhcm9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"md-recitation\">\n",
        "  Sources\n",
        "  <ol>\n",
        "  <li><a href=\"https://www.scribd.com/document/681281393/210130107085-PDS-Practical\">https://www.scribd.com/document/681281393/210130107085-PDS-Practical</a></li>\n",
        "  <li><a href=\"https://medium.com/@tim.po.developer/must-know-python-libraries-for-dominating-finance-your-essential-toolkit-f4c25a0f6d30\">https://medium.com/@tim.po.developer/must-know-python-libraries-for-dominating-finance-your-essential-toolkit-f4c25a0f6d30</a></li>\n",
        "  <li><a href=\"https://medium.com/coders-mojo/implemented-scikit-learn-projects-c0e65f70e54e\">https://medium.com/coders-mojo/implemented-scikit-learn-projects-c0e65f70e54e</a></li>\n",
        "  <li><a href=\"https://github.com/BladimirBL/Talento-Tech\">https://github.com/BladimirBL/Talento-Tech</a></li>\n",
        "  </ol>\n",
        "</div>"
      ],
      "metadata": {
        "id": "UxE4MtoTmPWM"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}